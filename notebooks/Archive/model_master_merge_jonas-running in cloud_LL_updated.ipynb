{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c91c641-7676-421d-aef8-79ef55aa0315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import timedelta \n",
    "import datetime\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pmdarima\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models.forecasting.tft_model import TFTModel\n",
    "from darts.metrics import mse\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import smape, mae\n",
    "from torchmetrics.regression import MeanAbsoluteError\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ab5d5-4860-4ec1-854b-9a5fbcf69082",
   "metadata": {},
   "source": [
    "# -1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c23922c-8d99-4757-aa9b-2a4684ab8a30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#csv_file_path = \n",
    "train_df = pd.read_csv('../raw_data/sales_train_validation.csv')\n",
    "prices_df = pd.read_csv('../raw_data/sell_prices.csv')\n",
    "calendar_df = pd.read_csv('../raw_data/calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195acbf7-4954-4faa-9805-9e95a2cfe84f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_filtered = train_df.iloc[0:5000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed29bb3-435c-4c7e-93cf-ca208ab5ee68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_sample = train_df_filtered.melt(id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9357d41-9baa-49b9-8cc9-e5f46c8fa05a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_sample.rename(columns={'variable': 'd', 'value':'sales'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdabd0b-3596-4d78-b3cd-e1216f64dd60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_df = train_df_sample.merge(calendar_df,on='d',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc759ed2-cffe-4513-9a95-82f06bdfffae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_df = merge_df.merge(prices_df,on=['store_id', 'item_id','wm_yr_wk'],how='left')\n",
    "merge_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c5842e-7d08-4165-90ec-b7edfc62dcc7",
   "metadata": {},
   "source": [
    "# 0 Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023fbbd9-6c97-41d6-bf76-920c4aee83ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "merge_df['date'] = pd.to_datetime(merge_df['date'])\n",
    "merge_df.set_index('date', inplace=True)\n",
    "\n",
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ccbb8-ef5f-4cd7-aa78-aa216738475d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#FILLING THE EMPTY PLACES\n",
    "merge_df['sell_price'].fillna(0, inplace=True)\n",
    "merge_df['event_name_1'].fillna('missing', inplace=True)\n",
    "merge_df['event_type_1'].fillna('missing', inplace=True)\n",
    "merge_df['event_name_2'].fillna('missing', inplace=True)\n",
    "merge_df['event_type_2'].fillna('missing', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c394d6c-7e1e-4570-bdd5-b03c0828fbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame with datetime index and \"id\" column\n",
    "start_date = merge_df.index.min()\n",
    "end_date = merge_df.index.max()\n",
    "\n",
    "# Generate the complete date range\n",
    "complete_date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "# Get unique values of \"id\" column\n",
    "unique_ids = merge_df['id'].unique()\n",
    "\n",
    "merge_df = merge_df.reset_index()\n",
    "\n",
    "# Create a MultiIndex with Cartesian product of date range and unique ids\n",
    "multi_index = pd.MultiIndex.from_product([complete_date_range, unique_ids], names=['date', 'id'])\n",
    "\n",
    "# Reindex the DataFrame using the MultiIndex\n",
    "merge_df = merge_df.set_index(['date', 'id']).reindex(multi_index)\n",
    "\n",
    "# Reset the index to make \"date\" and \"id\" columns again\n",
    "merge_df = merge_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4156b-8860-413e-9e54-1178d2d03650",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.set_index(\"date\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd28632-fbba-4622-bf9b-e7cd07cb3e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale 'sell_price' and 'year' by using MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "merge_df[['sell_price']] = minmax_scaler.fit_transform(merge_df[['sell_price']])\n",
    "merge_df[['year']] = minmax_scaler.fit_transform(merge_df[['year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d6e1682-e512-4ea7-aefc-97d9fee2a919",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check unique values for 'cat_id'\n",
    "print(f\"The unique values for 'cat_id' are {merge_df['cat_id'].unique()}\")\n",
    "\n",
    "# Check unique values for 'store_id'\n",
    "print(f\"The unique values for 'store_id' are {merge_df['store_id'].unique()}\")\n",
    "\n",
    "# Instantiate the OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit encoder for both 'cat_id' and 'store_id'\n",
    "ohe.fit(merge_df[['cat_id', 'store_id']])\n",
    "\n",
    "# Display the detected categories for both columns\n",
    "print(f\"The categories detected by the OneHotEncoder are {ohe.categories_}\")\n",
    "\n",
    "# Display the generated names for both columns\n",
    "print(f\"The column names for the encoded values are {ohe.get_feature_names_out()}\")\n",
    "\n",
    "# Transform the 'cat_id' and 'store_id' columns\n",
    "encoded_columns = ohe.transform(merge_df[['cat_id', 'store_id']])\n",
    "\n",
    "# Drop the original 'cat_id' and 'store_id' columns\n",
    "merge_df.drop(columns=['cat_id', 'store_id'], inplace=True)\n",
    "\n",
    "# Concatenate the encoded columns to the DataFrame\n",
    "merge_df[ ohe.get_feature_names_out()] = encoded_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ad5349-0e57-4e5b-a1a1-60e799c25eda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check unique values\n",
    "print(f\"The unique values for 'event_type_1' are {merge_df['event_type_1'].unique()}\")\n",
    "\n",
    "# Fit encoder\n",
    "ohe.fit(merge_df[['event_type_1']])\n",
    "\n",
    "# Display the detected categories\n",
    "print(f\"The categories detected by the OneHotEncoder are {ohe.categories_}\")\n",
    "\n",
    "# Display the generated names\n",
    "print(f\"The column names for the encoded values are {ohe.get_feature_names_out()}\")\n",
    "\n",
    "# Transform the current \"cat_id\" column\n",
    "merge_df[ohe.get_feature_names_out()] = ohe.transform(merge_df[['event_type_1']])\n",
    "\n",
    "# Drop the column \"cat_id\" which has been encoded\n",
    "merge_df.drop(columns = ['event_type_1'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adefde7a-880f-49e2-ac3d-05570dce8685",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check unique values\n",
    "print(f\"The unique values for 'event_type_2' are {merge_df['event_type_2'].unique()}\")\n",
    "\n",
    "# Fit encoder\n",
    "ohe.fit(merge_df[['event_type_2']])\n",
    "\n",
    "# Display the detected categories\n",
    "print(f\"The categories detected by the OneHotEncoder are {ohe.categories_}\")\n",
    "\n",
    "# Display the generated names\n",
    "print(f\"The column names for the encoded values are {ohe.get_feature_names_out()}\")\n",
    "\n",
    "# Transform the current \"cat_id\" column\n",
    "merge_df[ohe.get_feature_names_out()] = ohe.transform(merge_df[['event_type_2']])\n",
    "\n",
    "# Drop the column \"cat_id\" which has been encoded\n",
    "merge_df.drop(columns = ['event_type_2'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24abdb8-3be4-4608-91b0-ef0331e80133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check unique values\n",
    "print(f\"The unique values for 'event_name_1' are {merge_df['event_name_1'].unique()}\")\n",
    "\n",
    "# Fit encoder\n",
    "ohe.fit(merge_df[['event_name_1']])\n",
    "\n",
    "# Display the detected categories\n",
    "print(f\"The categories detected by the OneHotEncoder are {ohe.categories_}\")\n",
    "\n",
    "# Display the generated names\n",
    "print(f\"The column names for the encoded values are {ohe.get_feature_names_out()}\")\n",
    "\n",
    "# Transform the current \"cat_id\" column\n",
    "merge_df[ohe.get_feature_names_out()] = ohe.transform(merge_df[['event_name_1']])\n",
    "\n",
    "# Drop the column \"cat_id\" which has been encoded\n",
    "merge_df.drop(columns = ['event_name_1'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c1b1c5-3631-4720-bfd9-92f4f381c73a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check unique values\n",
    "print(f\"The unique values for 'event_name_2' are {merge_df['event_name_2'].unique()}\")\n",
    "\n",
    "# Fit encoder\n",
    "ohe.fit(merge_df[['event_name_2']])\n",
    "\n",
    "# Display the detected categories\n",
    "print(f\"The categories detected by the OneHotEncoder are {ohe.categories_}\")\n",
    "\n",
    "# Display the generated names\n",
    "print(f\"The column names for the encoded values are {ohe.get_feature_names_out()}\")\n",
    "\n",
    "# Transform the current \"cat_id\" column\n",
    "merge_df[ohe.get_feature_names_out()] = ohe.transform(merge_df[['event_name_2']])\n",
    "\n",
    "# Drop the column \"cat_id\" which has been encoded\n",
    "merge_df.drop(columns = ['event_name_2'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e7ea5ba-4d85-483b-a890-f5d148cddf08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Encoding Cyclical Features for weekdays\n",
    "# Notice that Sat starts as 1 till Fri as 7 for 'wday'\n",
    "merge_df['wday_sin'] = np.sin(2 * np.pi * merge_df['wday'] /7.0)\n",
    "merge_df['wday_cos'] = np.cos(2 * np.pi * merge_df['wday'] /7.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922ddcea-9ad4-47ea-8157-2498fde75a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Encoding Cyclical Features for month\n",
    "\n",
    "merge_df['month_sin'] = np.sin(2 * np.pi * merge_df['month'] /12.0)\n",
    "merge_df['month_cos'] = np.cos(2 * np.pi * merge_df['month'] /12.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91872056-89ef-4a5b-bc70-bc2f1ebf0526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_df_scaled = merge_df.drop(columns=['d', 'wm_yr_wk','weekday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250e4c27-5980-4e8f-9714-66905ab51ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Downcast numeric columns\n",
    "numeric_columns = merge_df_scaled.select_dtypes(include=['int64', 'float64']).columns\n",
    "merge_df_scaled[numeric_columns] = merge_df_scaled[numeric_columns].apply(lambda x: pd.to_numeric(x, downcast='integer' if np.issubdtype(x.dtype, np.integer) else 'float'))\n",
    "\n",
    "# Confirm the new datatypes\n",
    "print(merge_df_scaled.dtypes.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93811f0b-32c4-4beb-b498-127548d1ddf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_extraction(merge_df_scaled):\n",
    "    # Ignore all warnings within this function\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "        #lagged features\n",
    "        for i in range(1, 8):\n",
    "            merge_df_scaled[f'sales_lag_{i}'] = merge_df_scaled['sales'].shift(i)\n",
    "    \n",
    "        #lagged features per years\n",
    "        for i in range(1, 4):\n",
    "            merge_df_scaled[f'sales_lag_{i}years'] = merge_df_scaled['sales'].shift(i * 365)\n",
    "    \n",
    "            #rolling sum\n",
    "        merge_df_scaled['rolling_sum_7'] = merge_df_scaled['sales'].rolling(window=7).sum()\n",
    "        merge_df_scaled['rolling_sum_30'] = merge_df_scaled['sales'].rolling(window=30).sum()\n",
    "        merge_df_scaled['rolling_sum_60'] = merge_df_scaled['sales'].rolling(window=60).sum()\n",
    "        merge_df_scaled['rolling_sum_90'] = merge_df_scaled['sales'].rolling(window=90).sum()\n",
    "        merge_df_scaled['rolling_sum_120'] = merge_df_scaled['sales'].rolling(window=120).sum()\n",
    "    \n",
    "        #rolling average\n",
    "        merge_df_scaled['rolling_mean_7'] = merge_df_scaled['sales'].rolling(window=7).mean()\n",
    "        merge_df_scaled['rolling_mean_30'] = merge_df_scaled['sales'].rolling(window=30).mean()\n",
    "        merge_df_scaled['rolling_mean_60'] = merge_df_scaled['sales'].rolling(window=60).mean()\n",
    "        merge_df_scaled['rolling_mean_90'] = merge_df_scaled['sales'].rolling(window=90).mean()\n",
    "        merge_df_scaled['rolling_mean_120'] = merge_df_scaled['sales'].rolling(window=120).mean()\n",
    "    \n",
    "        #rolling stdv\n",
    "        merge_df_scaled['rolling_stdv_7'] = merge_df_scaled['sales'].rolling(window=7).std()\n",
    "        merge_df_scaled['rolling_stdv_30'] = merge_df_scaled['sales'].rolling(window=30).std()\n",
    "        merge_df_scaled['rolling_stdv_60'] = merge_df_scaled['sales'].rolling(window=60).std()\n",
    "        merge_df_scaled['rolling_stdv_90'] = merge_df_scaled['sales'].rolling(window=90).std()\n",
    "        merge_df_scaled['rolling_stdv_120'] = merge_df_scaled['sales'].rolling(window=120).std()\n",
    "    \n",
    "        merge_df_scaled.fillna(0,inplace=True)\n",
    "        \n",
    "\n",
    "    return merge_df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02df851-703f-412f-8f52-cf105070d070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_extraction_transfer_test(train_df,test_df):\n",
    "\n",
    "    # Ignore all warnings within this function\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "           \n",
    "    \n",
    "        #lagged features per years\n",
    "        for i in range(1, 4):\n",
    "            test_df[f'sales_lag_{i}years'] = train_df[f'sales_lag_{i}years'].iloc[-1]\n",
    "    \n",
    "            #rolling sum\n",
    "        test_df['rolling_sum_7'] = train_df['rolling_sum_7'].iloc[-1]\n",
    "        test_df['rolling_sum_30'] = train_df['rolling_sum_30'].iloc[-1]\n",
    "        test_df['rolling_sum_60'] = train_df['rolling_sum_60'].iloc[-1]\n",
    "        test_df['rolling_sum_90'] = train_df['rolling_sum_90'].iloc[-1]\n",
    "        test_df['rolling_sum_120'] = train_df['rolling_sum_120'].iloc[-1]\n",
    "        \n",
    "        # Rolling average\n",
    "        test_df['rolling_mean_7'] = train_df['rolling_mean_7'].iloc[-1]\n",
    "        test_df['rolling_mean_30'] = train_df['rolling_mean_30'].iloc[-1]\n",
    "        test_df['rolling_mean_60'] = train_df['rolling_mean_60'].iloc[-1]\n",
    "        test_df['rolling_mean_90'] = train_df['rolling_mean_90'].iloc[-1]\n",
    "        test_df['rolling_mean_120'] = train_df['rolling_mean_120'].iloc[-1]\n",
    "        \n",
    "        # Rolling standard deviation\n",
    "        test_df['rolling_stdv_7'] = train_df['rolling_stdv_7'].iloc[-1]\n",
    "        test_df['rolling_stdv_30'] = train_df['rolling_stdv_30'].iloc[-1]\n",
    "        test_df['rolling_stdv_60'] = train_df['rolling_stdv_60'].iloc[-1]\n",
    "        test_df['rolling_stdv_90'] = train_df['rolling_stdv_90'].iloc[-1]\n",
    "        test_df['rolling_stdv_120'] = train_df['rolling_stdv_120'].iloc[-1]\n",
    "    \n",
    "        # Identify the last available date in the training data\n",
    "        last_date_train = train_df.index[-1]\n",
    "    \n",
    "        # Fill in lagged features for the first few rows where future knowledge is available\n",
    "        #for i in range(1, 8):\n",
    "        #    # Identify the lagged date for the current lag\n",
    "        #    lagged_date = last_date_train - pd.Timedelta(days=i)\n",
    "            \n",
    "            # Fill in the lagged sales values for corresponding lagged days from the training data\n",
    "        #    test_df[f'sales_lag_{i}'] = test_df.index.map(lambda x: train_df.loc[x - pd.Timedelta(days=i), 'sales'] if x <= last_date_train else train_df[f'sales_lag_{i}'].iloc[-1])\n",
    "\n",
    "\n",
    "        # Fill in lagged features for the first few rows where future knowledge is available\n",
    "        for i in range(1, 8):\n",
    "            test_df[f'sales_lag_{i}'] = np.nan  # Initialize with NaN\n",
    "            \n",
    "            # Iterate over each row in the test DataFrame\n",
    "            for idx, row in test_df.iterrows():\n",
    "                lagged_date = idx - pd.Timedelta(days=i)  # Calculate the lagged date\n",
    "                \n",
    "                # Check if the lagged date is within the training data range\n",
    "                if lagged_date in train_df.index:\n",
    "                    test_df.at[idx, f'sales_lag_{i}'] = train_df.loc[lagged_date, 'sales']\n",
    "                else:\n",
    "                    test_df.at[idx, f'sales_lag_{i}'] = train_df['sales'].iloc[-1]\n",
    "    \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013dda06-1b86-4216-a9f8-366e5e27f7fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_rmsse(train, test, predictions):\n",
    "    forecast_mse = mean_squared_error(test, predictions)\n",
    "    train_mse = ((train - train.shift(1)) ** 2).mean()\n",
    "    return np.sqrt(forecast_mse / train_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e0fca26-43dc-4b21-b08e-65bd9d58aa40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_last_28_days_weights(df):\n",
    "    most_recent_date = df.index.max()\n",
    "    cutoff_date = most_recent_date - timedelta(days=27)\n",
    "    last_28_days_df = df.loc[cutoff_date:most_recent_date]\n",
    "    last_28_days_df['revenues'] = last_28_days_df['sales'] * last_28_days_df['sell_price']\n",
    "    \n",
    "    weights_df = last_28_days_df.groupby('id')['revenues'].sum()\n",
    "    weights_df = pd.DataFrame(weights_df)\n",
    "    \n",
    "    total_revenues = last_28_days_df['revenues'].sum()\n",
    "    weights_df['weights'] = weights_df['revenues'] / total_revenues\n",
    "    \n",
    "    return weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e8448-ecde-4944-99b4-30f74e5f489a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_df_scaled.drop(columns=[\"item_id\",\"dept_id\",\"state_id\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9391eb6-da51-4b69-a4aa-5fd1ccba9db6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_df_scaled.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a35134-1f8a-436c-b168-111341c17cdf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_df_scaled[\"id\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c90d7-181f-4c34-b9db-73d5632292e5",
   "metadata": {},
   "source": [
    "# 1. Defining Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c2e333-1676-4450-a8ac-b1d64ce1fee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_prophet(product_data):\n",
    "\n",
    "    product_data.reset_index(inplace=True,names=\"date\")\n",
    "    \n",
    "    prophet_product_df = product_data[[\"id\",\"date\",\"sales\"]]\n",
    "    prophet_product_df.columns = [\"id\",\"ds\",\"y\"]\n",
    "    prophet_product_df['ds'] = pd.to_datetime(prophet_product_df['ds'])\n",
    "    \n",
    "    data_train = prophet_product_df.iloc[:-28]\n",
    "    data_test = prophet_product_df.iloc[-28:]\n",
    "    X_train = data_train[\"ds\"]\n",
    "    y_train = data_train[\"y\"]\n",
    "    X_test = data_test[\"ds\"]\n",
    "    y_test = data_test[\"y\"]\n",
    "    \n",
    "    fbp = Prophet()\n",
    "\n",
    "    model = fbp.fit(data_train)\n",
    "    \n",
    "    predict_placeholder = fbp.make_future_dataframe(28,freq=\"D\")\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = fbp.predict(predict_placeholder[-28:])\n",
    "    \n",
    "\n",
    "    # Calculate and return the error metric for the current fold\n",
    "    rmsse = calc_rmsse(y_train, y_test, y_pred[\"yhat\"])\n",
    "    \n",
    "    return model, rmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151bd3d5-e70c-41ef-8a3e-2da51e4fe900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_auto_arima(product_data):\n",
    "    data_train = product_data.iloc[:-28]\n",
    "    data_test = product_data.iloc[-28:]\n",
    "    y_train = data_train[\"sales\"]\n",
    "    y_test = data_test[\"sales\"]\n",
    "\n",
    "    # Fit ARIMA model on the training data using auto_arima to find the best (p, d, q)\n",
    "    model = auto_arima(y_train, start_p=0, start_q=0, max_p=5, max_q=5, d=1,\n",
    "                       seasonal=True, trace=False, error_action='ignore', \n",
    "                       suppress_warnings=True, stepwise=True)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    predictions = model.predict(n_periods=len(y_test))\n",
    "\n",
    "    # Calculate and return the error metric for the current fold\n",
    "    rmsse = calc_rmsse(y_train,y_test,predictions)\n",
    "    \n",
    "    return model, rmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e1224-e66f-4e66-bad4-dd867e626f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective_optuna(trial, y_train, y_test):\n",
    "    \n",
    "    trend = trial.suggest_categorical('trend', ['add'])\n",
    "    seasonal = trial.suggest_categorical('seasonal', [None, 'add'])\n",
    "    seasonal_periods = trial.suggest_categorical('seasonal_periods', [None, 4, 7, 12])\n",
    "    \n",
    "    product_results = []\n",
    "\n",
    "    # Fit Holt-Winters model on the training data\n",
    "    model = ExponentialSmoothing(y_train, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods,freq='D')\n",
    "    fitted_model = model.fit(optimized=True)\n",
    "\n",
    "    # Predict on the test data\n",
    "    predictions = fitted_model.forecast(steps=len(y_test))\n",
    "\n",
    "    # Calculate and store the error metric\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    \n",
    "\n",
    "    # Calculate and return the error metric for the current fold\n",
    "    rmsse = calc_rmsse(y_train,y_test, predictions)\n",
    "    product_results.append(rmsse)\n",
    "\n",
    "    # Average MAE for this product\n",
    "    average_rmsse = np.mean(product_results)\n",
    "    return average_rmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102b594c-39b7-4817-bb4b-f3d5706c0d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_exp_smoothing(product_data):\n",
    "    data_train = product_data.iloc[:-28]\n",
    "    data_test = product_data.iloc[-28:]\n",
    "    y_train = data_train[\"sales\"]\n",
    "    y_test = data_test[\"sales\"]\n",
    "    # Create a study object\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    \n",
    "    print(f\"Optimizing hyperparameters for product: {id}\")\n",
    "    \n",
    "    \n",
    "    # Run the optimization process for the current product\n",
    "    study.optimize(lambda trial: objective_optuna(trial, y_train, y_test), n_trials=10, n_jobs=-1)\n",
    "\n",
    "    # Get the best hyperparameters and the corresponding best MAE\n",
    "    best_params = study.best_params\n",
    "    best_rmsse = study.best_value\n",
    "\n",
    "    # Create the best model with the obtained hyperparameters\n",
    "    best_model = ExponentialSmoothing(y_train, **best_params).fit()\n",
    "    \n",
    "    return best_model, best_rmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e96217a-a286-4ecd-b8d3-9d23a0fc3811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_lightgbm(product_data):\n",
    "    \n",
    "    data_train_val = product_data.iloc[:-28]\n",
    "    data_train_val = feature_extraction(data_train_val)\n",
    "    data_test = product_data.iloc[-28:]\n",
    "    data_test = feature_extraction_transfer_test(data_train_val,data_test)\n",
    "\n",
    "    data_train = data_train_val.iloc[:-112]\n",
    "    data_val = data_train_val.iloc[-112:]\n",
    "    \n",
    "    X_train = data_train.drop(columns=\"sales\")\n",
    "    y_train = data_train[\"sales\"]\n",
    "    X_val = data_val.drop(columns=\"sales\")\n",
    "    y_val = data_val[\"sales\"]\n",
    "    X_test = data_test.drop(columns=\"sales\")\n",
    "    y_test = data_test[\"sales\"]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define LightGBM parameters\n",
    "    params = {\n",
    "        \"n_estimators\": 1000,\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"max_depth\": -1,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.4,\n",
    "        \"lambda_l1\": 1,\n",
    "        \"lambda_l2\": 1,\n",
    "        \"seed\": 46,\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    \n",
    "    # Create dataset for LightGBM\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "    \n",
    "    # Train the model\n",
    "    num_round = 1000\n",
    "\n",
    "    bst = lgb.train(params, lgb_train, num_round, valid_sets=lgb_eval, callbacks=[lgb.early_stopping(stopping_rounds=50)])\n",
    "     \n",
    "    # Make predictions for the next 28 days\n",
    "    predictions = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "    # Calculate and return the error metric for the current fold\n",
    "    rmsse = calc_rmsse(y_train,y_test, predictions)\n",
    "    \n",
    "    return bst, rmsse\n",
    "\n",
    "# Example usage:\n",
    "# sales_forecast = forecast_sales(product_data)\n",
    "# print(sales_forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeebf22-5873-4a34-a105-e9332cce181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(product_data):\n",
    "    target = product_data[['sales']]\n",
    "    past_cov = product_data.drop(columns=['sales', 'id','item_id','dept_id','state_id','event_name_2'])\n",
    "    future_cov = product_data.drop(columns=['sales','id','item_id','dept_id','state_id','event_name_2'])\n",
    "\n",
    "    y_train = target.loc[:'2016-01-01']\n",
    "    past_cov_train = past_cov.loc[:'2016-01-01']\n",
    "    future_cov_train = future_cov.loc[:'2016-01-29']\n",
    "\n",
    "    y_val = target.loc['2016-01-02':'2016-04-24']\n",
    "    past_cov_val = past_cov.loc['2016-01-02':'2016-04-24']\n",
    "    future_cov_val = future_cov.loc['2016-01-02':'2016-05-22']\n",
    "\n",
    "    return (y_train, past_cov_train, future_cov_train,\n",
    "            y_val, past_cov_val, future_cov_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b88055f-815b-4258-91f7-b5e7a65f0589",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tft_model(y_train_series, past_cov_train_series, future_cov_train_series,\n",
    "                    y_val_series, past_cov_val_series, future_cov_val_series):\n",
    "    input_chunk_length = 28*2\n",
    "    output_chunk_length = 28\n",
    "\n",
    "    from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "    my_stopper = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=30,\n",
    "        min_delta=0.001,\n",
    "        mode='min',\n",
    "    )\n",
    "\n",
    "    pl_trainer_kwargs={\"callbacks\": [my_stopper],\n",
    "                       \"accelerator\": \"cpu\"}\n",
    "\n",
    "    tft = TFTModel(input_chunk_length=input_chunk_length,\n",
    "                   output_chunk_length=output_chunk_length,\n",
    "                   pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "                   lstm_layers=2,\n",
    "                   num_attention_heads=4,\n",
    "                   dropout=0.2,\n",
    "                   batch_size=16,\n",
    "                   hidden_size=16,\n",
    "                   torch_metrics=MeanAbsoluteError(),\n",
    "                   n_epochs=100,\n",
    "                   )\n",
    "\n",
    "    tft.fit(series=y_train_series,\n",
    "            past_covariates=past_cov_train_series,\n",
    "            future_covariates=future_cov_train_series,\n",
    "            val_series=y_val_series,\n",
    "            val_past_covariates=past_cov_val_series,\n",
    "            val_future_covariates=future_cov_val_series)\n",
    "\n",
    "    return tft\n",
    "\n",
    "#from darts.models import RNNModel\n",
    "\n",
    "# model = TFTModel(input_chunk_length=28*2)\n",
    "\n",
    "# model.save(\"my_model.pt\")\n",
    "# model_loaded = TFTModel.load(\"my_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc54d2d-19a1-443b-b653-7131bbfaa48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_and_create_sequences(data, input_length, output_length, scaler=None):\n",
    "    if scaler is None:\n",
    "        scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    data['scaled_sales'] = scaler.fit_transform(data[['sales']])\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - input_length - output_length + 1):\n",
    "        X.append(data['scaled_sales'].values[i:(i + input_length)])\n",
    "        y.append(data['scaled_sales'].values[(i + input_length):(i + input_length + output_length)])\n",
    "    X = np.array(X).reshape((-1, input_length, 1))\n",
    "    y = np.array(y)\n",
    "    return X, y, scaler\n",
    "\n",
    "def train_and_evaluate_lstm_model(X_train, y_train, X_test, y_test, input_length, output_length):\n",
    "    n_features = X_train.shape[2]\n",
    "    model = Sequential([LSTM(50, activation='relu'), Dense(output_length)])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mean_absolute_error'])\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=16, verbose=0)\n",
    "    predictions = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    return model, mae\n",
    "\n",
    "def run_lstm_pipeline(data, input_length, output_length, n_splits):\n",
    "    fold_results = []\n",
    "    tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "    for train_index, test_index in tscv.split(data):\n",
    "        train, test = data.iloc[train_index], data.iloc[test_index]\n",
    "\n",
    "        # Prepare data and create sequences\n",
    "        X_train, y_train, scaler = prepare_data_and_create_sequences(train, input_length, output_length)\n",
    "        X_test, y_test, _ = prepare_data_and_create_sequences(test, input_length, output_length, scaler)\n",
    "\n",
    "        # Train and evaluate model\n",
    "        model, mae = train_and_evaluate_lstm_model(X_train, y_train, X_test, y_test, input_length, output_length)\n",
    "        fold_results.append(mae)\n",
    "    return fold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ca7540-6b6f-4f53-99b5-dd1f613b9628",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4313bdf-003b-428f-addb-07ad6b7058b8",
   "metadata": {},
   "source": [
    "# 2.Running all models in a loop to find for each product with lowest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09341ed-7407-4421-87d4-b97c92ef1fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_list = [\"ARIMA\",\"Prophet\",\"ExponentialSmoothing\",\"LightGBM\",\"DartsTFT\", \"LSTM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25636b8c-848f-4188-bc73-1546fdeb687b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pmdarima import auto_arima\n",
    "\n",
    "# Dictionary to store MAE results for each unique time-series identified by id\n",
    "product_results = {}\n",
    "average_rmsse = []\n",
    "\n",
    "ids_filtered=merge_df_scaled['id'].unique()\n",
    "filtered_df = merge_df_scaled[merge_df_scaled['id'].isin(ids_filtered)]\n",
    "weights=calculate_last_28_days_weights(filtered_df)   \n",
    "\n",
    "# Iterate over each unique product series identified by id\n",
    "for id in ids_filtered:\n",
    "    print(f\"Analyzing product: {id}\")\n",
    "    product_data = merge_df_scaled[merge_df_scaled['id'] == id].drop(columns=\"id\")\n",
    "    product_data_with_id = merge_df_scaled[merge_df_scaled['id'] == id]\n",
    "\n",
    "    # Results list for the current product time-series\n",
    "    results = {}\n",
    "    best_score = 999.99\n",
    "    best_model_name = \"\"\n",
    "\n",
    "\n",
    "    product_weight=weights.loc[id].weights\n",
    "\n",
    "\n",
    "\n",
    "    #Looping all models\n",
    "    for model_name in models_list:\n",
    "\n",
    "        if model_name == \"ARIMA\":\n",
    "            #TODO: Add 5-fold split here for another loop (or inside the model function?) and then take the average score per model as their mae score\n",
    "            \n",
    "            # Fit ARIMA model on the training data using auto_arima to find the best (p, d, q)\n",
    "            model, rmsse = perform_auto_arima(product_data)\n",
    "            results[model_name] = {\"rmsse\": rmsse, \"model\": model}\n",
    "            if rmsse < best_score:\n",
    "                best_score = rmsse\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "\n",
    "        elif model_name == \"ExponentialSmoothing\":\n",
    "\n",
    "            # To be built\n",
    "            model, rmsse = perform_exp_smoothing(product_data)\n",
    "            results[model_name] = {\"rmsse\": rmsse, \"model\": model}\n",
    "            if rmsse < best_score:\n",
    "                best_score = rmsse\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "\n",
    "        elif model_name == \"Prophet\":\n",
    "\n",
    "            model, rmsse = perform_prophet(product_data_with_id)\n",
    "            results[model_name] = {\"rmsse\": rmsse, \"model\": model}\n",
    "            if rmsse < best_score:\n",
    "                best_score = rmsse\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "\n",
    "\n",
    "        elif model_name == \"LightGBM\":\n",
    "\n",
    "            model, rmsse = perform_lightgbm(product_data)\n",
    "            results[model_name] = {\"rmsse\": rmsse, \"model\": model}\n",
    "            if rmsse < best_score:\n",
    "                best_score = rmsse\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "\n",
    "        \n",
    "        elif model_name == \"DartsTFT\":\n",
    "            # Prepare data for TFT model\n",
    "            (y_train, past_cov_train, future_cov_train,\n",
    "             y_val, past_cov_val, future_cov_val) = prepare_data(product_data_with_id)\n",
    "\n",
    "            # Example code assuming daily frequency ('D')\n",
    "            y_train_series = TimeSeries.from_dataframe(y_train, fill_missing_dates=True, freq='D')\n",
    "            past_cov_train_series = TimeSeries.from_dataframe(past_cov_train, fill_missing_dates=True, freq='D')\n",
    "            future_cov_train_series = TimeSeries.from_dataframe(future_cov_train, fill_missing_dates=True, freq='D')\n",
    "\n",
    "            y_val_series = TimeSeries.from_dataframe(y_val, fill_missing_dates=True, freq='D')\n",
    "            past_cov_val_series = TimeSeries.from_dataframe(past_cov_val, fill_missing_dates=True, freq='D')\n",
    "            future_cov_val_series = TimeSeries.from_dataframe(future_cov_val, fill_missing_dates=True, freq='D')\n",
    "\n",
    "            trained_model = train_tft_model(y_train_series, past_cov_train_series, future_cov_train_series,\n",
    "                                            y_val_series, past_cov_val_series, future_cov_val_series)\n",
    "\n",
    "            # Store the trained TFT model in the results dictionary\n",
    "            results[model_name] = {\"rmsse\": trained_model.validation_loss(), \"model\": trained_model}\n",
    "            if trained_model.validation_loss() < best_score:\n",
    "                best_score = trained_model.validation_loss()\n",
    "                best_model = trained_model\n",
    "                best_model_name = model_name\n",
    "\n",
    "        elif model_name == \"LSTM\":\n",
    "            input_length = 200\n",
    "            output_length = 28\n",
    "            n_splits = 10\n",
    "            \n",
    "            fold_results = run_lstm_pipeline(product_data, input_length, output_length, n_splits)\n",
    "            mae = np.mean(fold_results)\n",
    "            results[model_name] = {\"mae\": mae}\n",
    "            if mae < best_score:\n",
    "                best_score = mae\n",
    "                best_model = None\n",
    "                best_model_name = model_name\n",
    "\n",
    "    \n",
    "    #Printing results for this product\n",
    "    print(results)\n",
    "    print(f\"Model results for {id}\")\n",
    "    print(f\"Best model: {best_model_name}\")\n",
    "    print(f\"Best score: {best_score}\")\n",
    "\n",
    "    average_rmsse.append(best_score*product_weight)\n",
    "\n",
    "    # Store the average MAE for the current product time-series\n",
    "    product_results[id] = {\"best_score\": best_score, \"best_model\": best_model_name, \"model\": best_model}\n",
    "\n",
    "    #Store the best model in a pkl file\n",
    "    if best_model_name == \"DartsTFT\":\n",
    "        filename = f'../models/{id}_model.pt'\n",
    "        best_model.save(filename)\n",
    "    elif best_model_name == \"LSTM\":\n",
    "        filename = f'../models/{id}_model.h5'\n",
    "        best_model.save(filename)\n",
    "    else:\n",
    "        filename = f'../models/{id}_model.pkl'\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(best_model, f)\n",
    "            \n",
    "# Create a DataFrame to store the results\n",
    "results_df_arima = pd.DataFrame(product_results.items(), columns=['id', 'RMSSE'])\n",
    "\n",
    "# Set the 'id' column as the index\n",
    "results_df_arima.set_index('id', inplace=True)\n",
    "\n",
    "average_rmsse_score = np.sum(average_rmsse)\n",
    "\n",
    "print(f\"Total average wRMSSE: {average_rmsse_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6752189-3c59-43db-bdca-1538095d58d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c2e1d-77c1-45b2-b5e1-97ded9501914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d4c26-7684-462d-bcff-9686788292e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
