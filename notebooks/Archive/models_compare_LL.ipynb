{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c91c641-7676-421d-aef8-79ef55aa0315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "023fbbd9-6c97-41d6-bf76-920c4aee83ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>...</th>\n",
       "      <th>event_name_1_StPatricksDay</th>\n",
       "      <th>event_name_1_SuperBowl</th>\n",
       "      <th>event_name_1_Thanksgiving</th>\n",
       "      <th>event_name_1_ValentinesDay</th>\n",
       "      <th>event_name_1_VeteransDay</th>\n",
       "      <th>event_name_1_missing</th>\n",
       "      <th>wday_sin</th>\n",
       "      <th>wday_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_2_197_CA_1_validation</td>\n",
       "      <td>FOODS_2_197</td>\n",
       "      <td>FOODS_2</td>\n",
       "      <td>CA</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_3_080_CA_1_validation</td>\n",
       "      <td>FOODS_3_080</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_3_090_CA_1_validation</td>\n",
       "      <td>FOODS_3_090</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_3_120_CA_1_validation</td>\n",
       "      <td>FOODS_3_120</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_3_252_CA_1_validation</td>\n",
       "      <td>FOODS_3_252</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id      item_id  dept_id state_id  sales  \\\n",
       "date                                                                            \n",
       "2011-01-29  FOODS_2_197_CA_1_validation  FOODS_2_197  FOODS_2       CA     38   \n",
       "2011-01-29  FOODS_3_080_CA_1_validation  FOODS_3_080  FOODS_3       CA     33   \n",
       "2011-01-29  FOODS_3_090_CA_1_validation  FOODS_3_090  FOODS_3       CA    107   \n",
       "2011-01-29  FOODS_3_120_CA_1_validation  FOODS_3_120  FOODS_3       CA      0   \n",
       "2011-01-29  FOODS_3_252_CA_1_validation  FOODS_3_252  FOODS_3       CA     19   \n",
       "\n",
       "            wday  month  year event_name_2  snap_CA  ...  \\\n",
       "date                                                 ...   \n",
       "2011-01-29     1      1   0.0      missing        0  ...   \n",
       "2011-01-29     1      1   0.0      missing        0  ...   \n",
       "2011-01-29     1      1   0.0      missing        0  ...   \n",
       "2011-01-29     1      1   0.0      missing        0  ...   \n",
       "2011-01-29     1      1   0.0      missing        0  ...   \n",
       "\n",
       "            event_name_1_StPatricksDay  event_name_1_SuperBowl  \\\n",
       "date                                                             \n",
       "2011-01-29                         0.0                     0.0   \n",
       "2011-01-29                         0.0                     0.0   \n",
       "2011-01-29                         0.0                     0.0   \n",
       "2011-01-29                         0.0                     0.0   \n",
       "2011-01-29                         0.0                     0.0   \n",
       "\n",
       "            event_name_1_Thanksgiving  event_name_1_ValentinesDay  \\\n",
       "date                                                                \n",
       "2011-01-29                        0.0                         0.0   \n",
       "2011-01-29                        0.0                         0.0   \n",
       "2011-01-29                        0.0                         0.0   \n",
       "2011-01-29                        0.0                         0.0   \n",
       "2011-01-29                        0.0                         0.0   \n",
       "\n",
       "            event_name_1_VeteransDay  event_name_1_missing  wday_sin  \\\n",
       "date                                                                   \n",
       "2011-01-29                       0.0                   1.0  0.781832   \n",
       "2011-01-29                       0.0                   1.0  0.781832   \n",
       "2011-01-29                       0.0                   1.0  0.781832   \n",
       "2011-01-29                       0.0                   1.0  0.781832   \n",
       "2011-01-29                       0.0                   1.0  0.781832   \n",
       "\n",
       "            wday_cos  month_sin  month_cos  \n",
       "date                                        \n",
       "2011-01-29   0.62349        0.5   0.866025  \n",
       "2011-01-29   0.62349        0.5   0.866025  \n",
       "2011-01-29   0.62349        0.5   0.866025  \n",
       "2011-01-29   0.62349        0.5   0.866025  \n",
       "2011-01-29   0.62349        0.5   0.866025  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "df = pd.read_csv('../raw_data/merge_df_resize.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "\n",
    "df.head() # 382600 rows Ã— 73 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2be79d-f94d-40f6-8c08-9538a0063704",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160e906f-15fa-4eb9-a0cf-7950364a51ba",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d652e98-679e-42fb-bf8f-dea3a947f17d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4313bdf-003b-428f-addb-07ad6b7058b8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1.Auto_ARIMA with mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25636b8c-848f-4188-bc73-1546fdeb687b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing product: FOODS_2_197_CA_1_validation\n",
      "Mean Absolute Error for FOODS_2_197_CA_1_validation: 8.726633436944372\n",
      "Analyzing product: FOODS_3_080_CA_1_validation\n",
      "Mean Absolute Error for FOODS_3_080_CA_1_validation: 6.034984090618189\n",
      "Analyzing product: FOODS_3_090_CA_1_validation\n",
      "Mean Absolute Error for FOODS_3_090_CA_1_validation: 19.479598099076547\n",
      "Analyzing product: FOODS_3_120_CA_1_validation\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m product_data \u001b[38;5;241m=\u001b[39m df[df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mid\u001b[39m]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Call the function to train and evaluate the ARIMA model\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m mae \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_arima_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproduct_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Store the result in the dictionary\u001b[39;00m\n\u001b[1;32m     34\u001b[0m product_results[\u001b[38;5;28mid\u001b[39m] \u001b[38;5;241m=\u001b[39m mae\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mtrain_arima_model\u001b[0;34m(product_data)\u001b[0m\n\u001b[1;32m      8\u001b[0m y_test \u001b[38;5;241m=\u001b[39m data_test[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Fit ARIMA model on the training data using auto_arima to find the best (p, d, q)\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mauto_arima\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_q\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mseasonal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mignore\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                   \u001b[49m\u001b[43msuppress_warnings\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstepwise\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Predict on the test data\u001b[39;00m\n\u001b[1;32m     16\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(n_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(y_test))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pmdarima/arima/auto.py:701\u001b[0m, in \u001b[0;36mauto_arima\u001b[0;34m(y, X, start_p, d, start_q, max_p, max_d, max_q, start_P, D, start_Q, max_P, max_D, max_Q, max_order, m, seasonal, stationary, information_criterion, alpha, test, seasonal_test, stepwise, n_jobs, start_params, trend, method, maxiter, offset_test_args, seasonal_test_args, suppress_warnings, error_action, trace, random, random_state, n_fits, return_valid_fits, out_of_sample_size, scoring, scoring_args, with_intercept, sarimax_kwargs, **fit_args)\u001b[0m\n\u001b[1;32m    669\u001b[0m     \u001b[38;5;66;03m# init the stepwise model wrapper\u001b[39;00m\n\u001b[1;32m    670\u001b[0m     search \u001b[38;5;241m=\u001b[39m solvers\u001b[38;5;241m.\u001b[39m_StepwiseFitWrapper(\n\u001b[1;32m    671\u001b[0m         y,\n\u001b[1;32m    672\u001b[0m         X\u001b[38;5;241m=\u001b[39mX,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    698\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39msarimax_kwargs,\n\u001b[1;32m    699\u001b[0m     )\n\u001b[0;32m--> 701\u001b[0m sorted_res \u001b[38;5;241m=\u001b[39m \u001b[43msearch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msolve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _return_wrapper(sorted_res, return_valid_fits, start, trace)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pmdarima/arima/_auto_solvers.py:405\u001b[0m, in \u001b[0;36m_StepwiseFitWrapper.solve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    400\u001b[0m     p \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    401\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m    403\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m q \u001b[38;5;241m<\u001b[39m max_q \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    404\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_k \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m--> 405\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mQ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    406\u001b[0m     q \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    407\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pmdarima/arima/_auto_solvers.py:235\u001b[0m, in \u001b[0;36m_StepwiseFitWrapper._do_fit\u001b[0;34m(self, order, seasonal_order, constant)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (order, seasonal_order, constant) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_dict:\n\u001b[1;32m    231\u001b[0m \n\u001b[1;32m    232\u001b[0m     \u001b[38;5;66;03m# increment the number of fits\u001b[39;00m\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 235\u001b[0m     fit, fit_time, new_ic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_arima\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mseasonal_order\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseasonal_order\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwith_intercept\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m# use the orders as a key to be hashed for\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;66;03m# the dictionary (pointing to fit)\u001b[39;00m\n\u001b[1;32m    242\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mresults_dict[(order, seasonal_order, constant)] \u001b[38;5;241m=\u001b[39m fit\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pmdarima/arima/_auto_solvers.py:508\u001b[0m, in \u001b[0;36m_fit_candidate_model\u001b[0;34m(y, X, order, seasonal_order, start_params, trend, method, maxiter, fit_params, suppress_warnings, trace, error_action, out_of_sample_size, scoring, scoring_args, with_intercept, information_criterion, **kwargs)\u001b[0m\n\u001b[1;32m    500\u001b[0m fit \u001b[38;5;241m=\u001b[39m ARIMA(order\u001b[38;5;241m=\u001b[39morder, seasonal_order\u001b[38;5;241m=\u001b[39mseasonal_order,\n\u001b[1;32m    501\u001b[0m             start_params\u001b[38;5;241m=\u001b[39mstart_params, trend\u001b[38;5;241m=\u001b[39mtrend, method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    502\u001b[0m             maxiter\u001b[38;5;241m=\u001b[39mmaxiter, suppress_warnings\u001b[38;5;241m=\u001b[39msuppress_warnings,\n\u001b[1;32m    503\u001b[0m             out_of_sample_size\u001b[38;5;241m=\u001b[39mout_of_sample_size, scoring\u001b[38;5;241m=\u001b[39mscoring,\n\u001b[1;32m    504\u001b[0m             scoring_args\u001b[38;5;241m=\u001b[39mscoring_args,\n\u001b[1;32m    505\u001b[0m             with_intercept\u001b[38;5;241m=\u001b[39mwith_intercept, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 508\u001b[0m     \u001b[43mfit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# for non-stationarity errors or singular matrices, return None\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (LinAlgError, \u001b[38;5;167;01mValueError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m v:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pmdarima/arima/arima.py:603\u001b[0m, in \u001b[0;36mARIMA.fit\u001b[0;34m(self, y, X, **fit_args)\u001b[0m\n\u001b[1;32m    600\u001b[0m         X \u001b[38;5;241m=\u001b[39m safe_indexing(X, \u001b[38;5;28mslice\u001b[39m(\u001b[38;5;241m0\u001b[39m, n_exog \u001b[38;5;241m-\u001b[39m cv))\n\u001b[1;32m    602\u001b[0m \u001b[38;5;66;03m# Internal call\u001b[39;00m\n\u001b[0;32m--> 603\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[38;5;66;03m# now make a forecast if we're validating to compute the\u001b[39;00m\n\u001b[1;32m    606\u001b[0m \u001b[38;5;66;03m# out-of-sample score\u001b[39;00m\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cv_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    608\u001b[0m     \u001b[38;5;66;03m# get the predictions (use self.predict, which calls forecast\u001b[39;00m\n\u001b[1;32m    609\u001b[0m     \u001b[38;5;66;03m# from statsmodels internally)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pmdarima/arima/arima.py:524\u001b[0m, in \u001b[0;36mARIMA._fit\u001b[0;34m(self, y, X, **fit_args)\u001b[0m\n\u001b[1;32m    522\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    523\u001b[0m         warnings\u001b[38;5;241m.\u001b[39msimplefilter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 524\u001b[0m         fit, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marima_res_ \u001b[38;5;241m=\u001b[39m \u001b[43m_fit_wrapper\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    525\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    526\u001b[0m     fit, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marima_res_ \u001b[38;5;241m=\u001b[39m _fit_wrapper()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pmdarima/arima/arima.py:510\u001b[0m, in \u001b[0;36mARIMA._fit.<locals>._fit_wrapper\u001b[0;34m()\u001b[0m\n\u001b[1;32m    507\u001b[0m _maxiter \u001b[38;5;241m=\u001b[39m fit_args\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m\"\u001b[39m, _maxiter)\n\u001b[1;32m    509\u001b[0m disp \u001b[38;5;241m=\u001b[39m fit_args\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 510\u001b[0m fitted \u001b[38;5;241m=\u001b[39m \u001b[43marima\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    511\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    512\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    513\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_maxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    514\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    518\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arima, fitted\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/statsmodels/tsa/statespace/mlemodel.py:703\u001b[0m, in \u001b[0;36mMLEModel.fit\u001b[0;34m(self, start_params, transformed, includes_fixed, cov_type, cov_kwds, method, maxiter, full_output, disp, callback, return_params, optim_score, optim_complex_step, optim_hessian, flags, low_memory, **kwargs)\u001b[0m\n\u001b[1;32m    701\u001b[0m         flags[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhessian_method\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m optim_hessian\n\u001b[1;32m    702\u001b[0m     fargs \u001b[38;5;241m=\u001b[39m (flags,)\n\u001b[0;32m--> 703\u001b[0m     mlefit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    704\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    705\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    706\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    707\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    708\u001b[0m \u001b[43m                         \u001b[49m\u001b[43mskip_hessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;66;03m# Just return the fitted parameters if requested\u001b[39;00m\n\u001b[1;32m    711\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_params:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/statsmodels/base/model.py:566\u001b[0m, in \u001b[0;36mLikelihoodModel.fit\u001b[0;34m(self, start_params, method, maxiter, full_output, disp, fargs, callback, retall, skip_hessian, **kwargs)\u001b[0m\n\u001b[1;32m    563\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_t\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    565\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m Optimizer()\n\u001b[0;32m--> 566\u001b[0m xopt, retvals, optim_settings \u001b[38;5;241m=\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    568\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mhessian\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhess\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    569\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    570\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    571\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    572\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    573\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mretall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    574\u001b[0m \u001b[43m                                               \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    575\u001b[0m \u001b[38;5;66;03m# Restore cov_type, cov_kwds and use_t\u001b[39;00m\n\u001b[1;32m    576\u001b[0m optim_settings\u001b[38;5;241m.\u001b[39mupdate(kwds)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/statsmodels/base/optimizer.py:243\u001b[0m, in \u001b[0;36mOptimizer._fit\u001b[0;34m(self, objective, gradient, start_params, fargs, kwargs, hessian, method, maxiter, full_output, disp, callback, retall)\u001b[0m\n\u001b[1;32m    240\u001b[0m     fit_funcs\u001b[38;5;241m.\u001b[39mupdate(extra_fit_funcs)\n\u001b[1;32m    242\u001b[0m func \u001b[38;5;241m=\u001b[39m fit_funcs[method]\n\u001b[0;32m--> 243\u001b[0m xopt, retvals \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mretall\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretall\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfull_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mhess\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhessian\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m optim_settings \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m: method, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstart_params\u001b[39m\u001b[38;5;124m'\u001b[39m: start_params,\n\u001b[1;32m    249\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m'\u001b[39m: maxiter, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfull_output\u001b[39m\u001b[38;5;124m'\u001b[39m: full_output,\n\u001b[1;32m    250\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfargs\u001b[39m\u001b[38;5;124m'\u001b[39m: fargs, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m: callback,\n\u001b[1;32m    251\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mretall\u001b[39m\u001b[38;5;124m'\u001b[39m: retall, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mextra_fit_funcs\u001b[39m\u001b[38;5;124m\"\u001b[39m: extra_fit_funcs}\n\u001b[1;32m    252\u001b[0m optim_settings\u001b[38;5;241m.\u001b[39mupdate(kwargs)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/statsmodels/base/optimizer.py:660\u001b[0m, in \u001b[0;36m_fit_lbfgs\u001b[0;34m(f, score, start_params, fargs, kwargs, disp, maxiter, callback, retall, full_output, hess)\u001b[0m\n\u001b[1;32m    657\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m approx_grad:\n\u001b[1;32m    658\u001b[0m     func \u001b[38;5;241m=\u001b[39m f\n\u001b[0;32m--> 660\u001b[0m retvals \u001b[38;5;241m=\u001b[39m \u001b[43moptimize\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfmin_l_bfgs_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxiter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxiter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    661\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    662\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdisp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[43m                                 \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mextra_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m full_output:\n\u001b[1;32m    666\u001b[0m     xopt, fopt, d \u001b[38;5;241m=\u001b[39m retvals\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py:199\u001b[0m, in \u001b[0;36mfmin_l_bfgs_b\u001b[0;34m(func, x0, fprime, args, approx_grad, bounds, m, factr, pgtol, epsilon, iprint, maxfun, maxiter, disp, callback, maxls)\u001b[0m\n\u001b[1;32m    187\u001b[0m callback \u001b[38;5;241m=\u001b[39m _wrap_callback(callback)\n\u001b[1;32m    188\u001b[0m opts \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdisp\u001b[39m\u001b[38;5;124m'\u001b[39m: disp,\n\u001b[1;32m    189\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124miprint\u001b[39m\u001b[38;5;124m'\u001b[39m: iprint,\n\u001b[1;32m    190\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxcor\u001b[39m\u001b[38;5;124m'\u001b[39m: m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcallback\u001b[39m\u001b[38;5;124m'\u001b[39m: callback,\n\u001b[1;32m    197\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaxls\u001b[39m\u001b[38;5;124m'\u001b[39m: maxls}\n\u001b[0;32m--> 199\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43m_minimize_lbfgsb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjac\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m                       \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mopts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m d \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgrad\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjac\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    202\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtask\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    203\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuncalls\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnfev\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    204\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnit\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    205\u001b[0m      \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwarnflag\u001b[39m\u001b[38;5;124m'\u001b[39m: res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m]}\n\u001b[1;32m    206\u001b[0m f \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfun\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/scipy/optimize/_lbfgsb_py.py:365\u001b[0m, in \u001b[0;36m_minimize_lbfgsb\u001b[0;34m(fun, x0, args, jac, bounds, disp, maxcor, ftol, gtol, eps, maxfun, maxiter, iprint, callback, maxls, finite_diff_rel_step, **unknown_options)\u001b[0m\n\u001b[1;32m    359\u001b[0m task_str \u001b[38;5;241m=\u001b[39m task\u001b[38;5;241m.\u001b[39mtobytes()\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFG\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;66;03m# The minimization routine wants f and g at the current x.\u001b[39;00m\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;66;03m# Note that interruptions due to maxfun are postponed\u001b[39;00m\n\u001b[1;32m    363\u001b[0m     \u001b[38;5;66;03m# until the completion of the current minimization iteration.\u001b[39;00m\n\u001b[1;32m    364\u001b[0m     \u001b[38;5;66;03m# Overwrite f and g:\u001b[39;00m\n\u001b[0;32m--> 365\u001b[0m     f, g \u001b[38;5;241m=\u001b[39m \u001b[43mfunc_and_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m task_str\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW_X\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    367\u001b[0m     \u001b[38;5;66;03m# new iteration\u001b[39;00m\n\u001b[1;32m    368\u001b[0m     n_iterations \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:286\u001b[0m, in \u001b[0;36mScalarFunction.fun_and_grad\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_x_impl(x)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[0;32m--> 286\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:256\u001b[0m, in \u001b[0;36mScalarFunction._update_grad\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_update_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated:\n\u001b[0;32m--> 256\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_grad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg_updated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:173\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.update_grad\u001b[0;34m()\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_fun()\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mngev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 173\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mg \u001b[38;5;241m=\u001b[39m \u001b[43mapprox_derivative\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    174\u001b[0m \u001b[43m                           \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfinite_diff_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/scipy/optimize/_numdiff.py:505\u001b[0m, in \u001b[0;36mapprox_derivative\u001b[0;34m(fun, x0, method, rel_step, abs_step, f0, bounds, sparsity, as_linear_operator, args, kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     use_one_sided \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sparsity \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 505\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_dense_difference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfun_wrapped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf0\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m                             \u001b[49m\u001b[43muse_one_sided\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    508\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(sparsity) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(sparsity) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/scipy/optimize/_numdiff.py:576\u001b[0m, in \u001b[0;36m_dense_difference\u001b[0;34m(fun, x0, f0, h, use_one_sided, method)\u001b[0m\n\u001b[1;32m    574\u001b[0m     x \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n\u001b[1;32m    575\u001b[0m     dx \u001b[38;5;241m=\u001b[39m x[i] \u001b[38;5;241m-\u001b[39m x0[i]  \u001b[38;5;66;03m# Recompute dx as exactly representable number.\u001b[39;00m\n\u001b[0;32m--> 576\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m f0\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m3-point\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m use_one_sided[i]:\n\u001b[1;32m    578\u001b[0m     x1 \u001b[38;5;241m=\u001b[39m x0 \u001b[38;5;241m+\u001b[39m h_vecs[i]\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/scipy/optimize/_numdiff.py:456\u001b[0m, in \u001b[0;36mapprox_derivative.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfun_wrapped\u001b[39m(x):\n\u001b[0;32m--> 456\u001b[0m     f \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39matleast_1d(\u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m f\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    458\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`fun` return value has \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    459\u001b[0m                            \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmore than 1 dimension.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/scipy/optimize/_differentiable_functions.py:137\u001b[0m, in \u001b[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnfev \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Send a copy because the user may overwrite it.\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;66;03m# Overwriting results in undefined behaviour because\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m fx \u001b[38;5;241m=\u001b[39m \u001b[43mfun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;66;03m# Make sure the function returns a true scalar\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misscalar(fx):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/statsmodels/base/model.py:534\u001b[0m, in \u001b[0;36mLikelihoodModel.fit.<locals>.f\u001b[0;34m(params, *args)\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(params, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 534\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m nobs\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/statsmodels/tsa/statespace/mlemodel.py:938\u001b[0m, in \u001b[0;36mMLEModel.loglike\u001b[0;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m complex_step:\n\u001b[1;32m    936\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minversion_method\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m INVERT_UNIVARIATE \u001b[38;5;241m|\u001b[39m SOLVE_LU\n\u001b[0;32m--> 938\u001b[0m loglike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mssm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloglike\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_step\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    940\u001b[0m \u001b[38;5;66;03m# Koopman, Shephard, and Doornik recommend maximizing the average\u001b[39;00m\n\u001b[1;32m    941\u001b[0m \u001b[38;5;66;03m# likelihood to avoid scale issues, but the averaging is done\u001b[39;00m\n\u001b[1;32m    942\u001b[0m \u001b[38;5;66;03m# automatically in the base model `fit` method\u001b[39;00m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loglike\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/statsmodels/tsa/statespace/kalman_filter.py:1001\u001b[0m, in \u001b[0;36mKalmanFilter.loglike\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;124;03mCalculate the loglikelihood associated with the statespace model.\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    997\u001b[0m \u001b[38;5;124;03m    The joint loglikelihood.\u001b[39;00m\n\u001b[1;32m    998\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    999\u001b[0m kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconserve_memory\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1000\u001b[0m                   MEMORY_CONSERVE \u001b[38;5;241m^\u001b[39m MEMORY_NO_LIKELIHOOD)\n\u001b[0;32m-> 1001\u001b[0m kfilter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1002\u001b[0m loglikelihood_burn \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloglikelihood_burn\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1003\u001b[0m                                 \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloglikelihood_burn)\n\u001b[1;32m   1004\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mconserve_memory\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m&\u001b[39m MEMORY_NO_LIKELIHOOD):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/statsmodels/tsa/statespace/kalman_filter.py:921\u001b[0m, in \u001b[0;36mKalmanFilter._filter\u001b[0;34m(self, filter_method, inversion_method, stability_method, conserve_memory, filter_timing, tolerance, loglikelihood_burn, complex_step)\u001b[0m\n\u001b[1;32m    918\u001b[0m kfilter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kalman_filters[prefix]\n\u001b[1;32m    920\u001b[0m \u001b[38;5;66;03m# Initialize the state\u001b[39;00m\n\u001b[0;32m--> 921\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_initialize_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Run the filter\u001b[39;00m\n\u001b[1;32m    924\u001b[0m kfilter()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/statsmodels/tsa/statespace/representation.py:1058\u001b[0m, in \u001b[0;36mRepresentation._initialize_state\u001b[0;34m(self, prefix, complex_step)\u001b[0m\n\u001b[1;32m   1056\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minitialization\u001b[38;5;241m.\u001b[39minitialized:\n\u001b[1;32m   1057\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitialization is incomplete.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m-> 1058\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_statespaces\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialization\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1059\u001b[0m \u001b[43m                                         \u001b[49m\u001b[43mcomplex_step\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcomplex_step\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStatespace model not initialized.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from pmdarima import auto_arima\n",
    "\n",
    "def train_arima_model(product_data):\n",
    "    # Split data into training and test sets\n",
    "    data_train = product_data.iloc[:-28]\n",
    "    data_test = product_data.iloc[-28:]\n",
    "    y_train = data_train[\"sales\"]\n",
    "    y_test = data_test[\"sales\"]\n",
    "\n",
    "    # Fit ARIMA model on the training data using auto_arima to find the best (p, d, q)\n",
    "    model = auto_arima(y_train, start_p=0, start_q=0, max_p=5, max_q=5, d=1,\n",
    "                       seasonal=True, trace=False, error_action='ignore', \n",
    "                       suppress_warnings=True, stepwise=True)\n",
    "\n",
    "    # Predict on the test data\n",
    "    predictions = model.predict(n_periods=len(y_test))\n",
    "\n",
    "    # Calculate and return the error metric\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    return mae\n",
    "\n",
    "# Dictionary to store MAE results for each unique time-series identified by id\n",
    "product_results = {}\n",
    "\n",
    "# Iterate over each unique product series identified by id \n",
    "for id in df['id'].unique()[:10]:\n",
    "    print(f\"Analyzing product: {id}\")\n",
    "    product_data = df[df['id'] == id]\n",
    "\n",
    "    # Call the function to train and evaluate the ARIMA model\n",
    "    mae = train_arima_model(product_data)\n",
    "    \n",
    "    # Store the result in the dictionary\n",
    "    product_results[id] = mae\n",
    "    print(f'Mean Absolute Error for {id}: {mae}')\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df_arima = pd.DataFrame(product_results.items(), columns=['id', 'ARIMA_MAE'])\n",
    "\n",
    "# Set the 'id' column as the index\n",
    "results_df_arima.set_index('id', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57dd76a-6bf4-4221-a99d-a50b6b010664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results_df_arima "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe7c8b8-1154-4f5b-b3a6-06d2ec0069fe",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2.Holt-Winters Exponential Smoothing model with mae "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cf118d-0c1e-4418-a5dd-40d154aee999",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "\n",
    "def train_exponential_smoothing_model(y_train, data_test):\n",
    "    # Define the objective function for hyperparameter optimization\n",
    "    def objective(trial):\n",
    "        trend = trial.suggest_categorical('trend', ['add'])\n",
    "        seasonal = trial.suggest_categorical('seasonal', [None, 'add'])\n",
    "        seasonal_periods = trial.suggest_categorical('seasonal_periods', [None, 4, 7, 12])\n",
    "\n",
    "        # Fit Exponential Smoothing model on the training data\n",
    "        model = ExponentialSmoothing(y_train, trend=trend, seasonal=seasonal, \n",
    "                                      seasonal_periods=seasonal_periods, freq='D')\n",
    "        fitted_model = model.fit(optimized=True)\n",
    "\n",
    "        # Predict on the test data\n",
    "        predictions = fitted_model.forecast(steps=len(data_test))\n",
    "\n",
    "        # Calculate and return the error metric\n",
    "        mae = mean_absolute_error(data_test, predictions)\n",
    "        return mae\n",
    "\n",
    "    # Create a study object\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "\n",
    "    # Run the optimization process for the current product\n",
    "    study.optimize(objective, n_trials=10)\n",
    "\n",
    "    # Get the best hyperparameters and the corresponding best MAE\n",
    "    best_params = study.best_params\n",
    "    best_mae = study.best_value\n",
    "\n",
    "    return best_params, best_mae\n",
    "\n",
    "# Dictionary to store results for each unique time-series identified by id\n",
    "results_dict = {}\n",
    "\n",
    "# Iterate over each unique product series identified by id \n",
    "for id in df['id'].unique()[:10]:\n",
    "    print(f\"Optimizing hyperparameters for product: {id}\")\n",
    "    product_data = df[df['id'] == id]\n",
    "    \n",
    "    # Split data into training and test sets\n",
    "    data_train = product_data.iloc[:-28]\n",
    "    data_test = product_data.iloc[-28:]\n",
    "    y_train = data_train[\"sales\"]\n",
    "    \n",
    "    # Call the function to train and evaluate the Exponential Smoothing model\n",
    "    best_params, best_mae = train_exponential_smoothing_model(y_train, data_test[\"sales\"])\n",
    "    \n",
    "    # Store the result in the dictionary\n",
    "    results_dict[id] = {'ExpSmoothing_params': best_params, 'ExpSmoothing_MAE': best_mae}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4d1187-fb23-450a-ac87-0ad96d294124",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert the results dictionary to a DataFrame\n",
    "results_df_exp = pd.DataFrame(results_dict).T.reset_index()\n",
    "results_df_exp.columns = ['id', 'ExpSmoothing_params', 'ExpSmoothing_MAE']\n",
    "\n",
    "# Set the 'Product ID' column as the index\n",
    "results_df_exp.set_index('id', inplace=True)\n",
    "results_df_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22b314d-6a14-476e-a6da-e020b326229b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13ac0a-fa34-4333-9abf-2dc1e7592607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import lightgbm as lgb\n",
    "\n",
    "# def train_lightgbm_model(product_data):\n",
    "#     # Split data into training and test sets\n",
    "#     data_train = product_data.iloc[:-28]\n",
    "#     data_test = product_data.iloc[-28:]\n",
    "#     y_train = data_train[\"sales\"]\n",
    "#     y_test = data_test[\"sales\"]\n",
    "\n",
    "#     # Convert data into LightGBM dataset format\n",
    "#     lgb_train = lgb.Dataset(np.array(y_train).reshape(-1, 1), label=y_train)\n",
    "#     lgb_test = lgb.Dataset(np.array(y_test).reshape(-1, 1), label=y_test)\n",
    "\n",
    "#     # Define parameters for the LightGBM model\n",
    "#     params = {\n",
    "#         'objective': 'regression',  # Regression task\n",
    "#         'metric': 'mae',             # Mean Absolute Error\n",
    "#         'verbosity': -1              # No output during training\n",
    "#     }\n",
    "\n",
    "#     # Train the LightGBM model with early stopping\n",
    "#     gbm = lgb.train(params,\n",
    "#                     lgb_train,\n",
    "#                     num_boost_round=1000,      # Large number of boosting rounds\n",
    "#                     valid_sets=[lgb_train, lgb_test],  # Validation data\n",
    "#                     callbacks=[lgb.early_stopping(stopping_rounds=10)])   # Early stopping criterion\n",
    "\n",
    "#     # Predict on the test set\n",
    "#     y_pred = gbm.predict(np.array(y_test).reshape(-1, 1), num_iteration=gbm.best_iteration)\n",
    "#     print(y_pred)\n",
    "\n",
    "#     # Calculate and return the mean absolute error\n",
    "#     mae = mean_absolute_error(y_test, y_pred)\n",
    "#     return mae\n",
    "\n",
    "# # Dictionary to store MAE results for each unique time-series identified by id\n",
    "# product_results = {}\n",
    "\n",
    "# # Iterate over each unique product series identified by id \n",
    "# for id in df['id'].unique()[:10]:\n",
    "#     print(f\"Analyzing product: {id}\")\n",
    "#     product_data = df[df['id'] == id]\n",
    "#     mae = train_lightgbm_model(product_data)\n",
    "#     product_results[id] = mae\n",
    "#     print(f'Mean Absolute Error for {id}: {mae}')\n",
    "\n",
    "# # Create a DataFrame to store the results\n",
    "# results_df_lgbm = pd.DataFrame(product_results.items(), columns=['id', 'LightGBM_MAE'])\n",
    "\n",
    "# # Set the 'id' column as the index\n",
    "# results_df_lgbm.set_index('id', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d180eb73-ee0d-4fd9-a379-c84f4eb40db6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2971e02f-8450-4b8c-8225-fb0953beb1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_df_lgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be88762-bdc8-43c9-b997-53ca9f6ba47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the two DataFrames based on the product ID\n",
    "comparison_df = pd.merge(results_df_arima, results_df_exp, left_index=True, right_index=True)\n",
    "#comparison_df = pd.merge(comparison_df, results_df_lgbm, left_index=True, right_index=True)\n",
    "comparison_df['Best MAE'] = comparison_df[['ARIMA_MAE', 'ExpSmoothing_MAE']].min(axis=1)\n",
    "comparison_df['Best Method'] = comparison_df.apply(lambda row: \n",
    "                                                   'ARIMA' if row['Best MAE'] == row['ARIMA_MAE'] \n",
    "                                                   else 'ExpSmoothing_MAE', axis=1)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "#comparison_df.drop(['ARIMA_MAE', 'ExpSmoothing_params', 'ExpSmoothing_MAE', 'LightGBM_MAE'], axis=1, inplace=True)\n",
    "#comparison_df.drop(['ExpSmoothing_params'], axis=1, inplace=True)\n",
    "comparison_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e017ebd-1184-4f8d-8a68-6a7f7267a4e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c6fa4b46-e8da-4478-99ec-f37e6a6ccd90",
   "metadata": {},
   "source": [
    "# 3. DARTS TFT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75ab003b-3b42-4fc4-9993-457416af94a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>...</th>\n",
       "      <th>event_name_1_StPatricksDay</th>\n",
       "      <th>event_name_1_SuperBowl</th>\n",
       "      <th>event_name_1_Thanksgiving</th>\n",
       "      <th>event_name_1_ValentinesDay</th>\n",
       "      <th>event_name_1_VeteransDay</th>\n",
       "      <th>event_name_1_missing</th>\n",
       "      <th>wday_sin</th>\n",
       "      <th>wday_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_2_197_CA_1_validation</td>\n",
       "      <td>FOODS_2_197</td>\n",
       "      <td>FOODS_2</td>\n",
       "      <td>CA</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_3_080_CA_1_validation</td>\n",
       "      <td>FOODS_3_080</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_3_090_CA_1_validation</td>\n",
       "      <td>FOODS_3_090</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_3_120_CA_1_validation</td>\n",
       "      <td>FOODS_3_120</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_3_252_CA_1_validation</td>\n",
       "      <td>FOODS_3_252</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.62349</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id      item_id  dept_id state_id  sales  \\\n",
       "date                                                                            \n",
       "2011-01-29  FOODS_2_197_CA_1_validation  FOODS_2_197  FOODS_2       CA     38   \n",
       "2011-01-29  FOODS_3_080_CA_1_validation  FOODS_3_080  FOODS_3       CA     33   \n",
       "2011-01-29  FOODS_3_090_CA_1_validation  FOODS_3_090  FOODS_3       CA    107   \n",
       "2011-01-29  FOODS_3_120_CA_1_validation  FOODS_3_120  FOODS_3       CA      0   \n",
       "2011-01-29  FOODS_3_252_CA_1_validation  FOODS_3_252  FOODS_3       CA     19   \n",
       "\n",
       "            wday  month  year event_name_2  snap_CA  ...  \\\n",
       "date                                                 ...   \n",
       "2011-01-29     1      1   0.0      missing        0  ...   \n",
       "2011-01-29     1      1   0.0      missing        0  ...   \n",
       "2011-01-29     1      1   0.0      missing        0  ...   \n",
       "2011-01-29     1      1   0.0      missing        0  ...   \n",
       "2011-01-29     1      1   0.0      missing        0  ...   \n",
       "\n",
       "            event_name_1_StPatricksDay  event_name_1_SuperBowl  \\\n",
       "date                                                             \n",
       "2011-01-29                         0.0                     0.0   \n",
       "2011-01-29                         0.0                     0.0   \n",
       "2011-01-29                         0.0                     0.0   \n",
       "2011-01-29                         0.0                     0.0   \n",
       "2011-01-29                         0.0                     0.0   \n",
       "\n",
       "            event_name_1_Thanksgiving  event_name_1_ValentinesDay  \\\n",
       "date                                                                \n",
       "2011-01-29                        0.0                         0.0   \n",
       "2011-01-29                        0.0                         0.0   \n",
       "2011-01-29                        0.0                         0.0   \n",
       "2011-01-29                        0.0                         0.0   \n",
       "2011-01-29                        0.0                         0.0   \n",
       "\n",
       "            event_name_1_VeteransDay  event_name_1_missing  wday_sin  \\\n",
       "date                                                                   \n",
       "2011-01-29                       0.0                   1.0  0.781832   \n",
       "2011-01-29                       0.0                   1.0  0.781832   \n",
       "2011-01-29                       0.0                   1.0  0.781832   \n",
       "2011-01-29                       0.0                   1.0  0.781832   \n",
       "2011-01-29                       0.0                   1.0  0.781832   \n",
       "\n",
       "            wday_cos  month_sin  month_cos  \n",
       "date                                        \n",
       "2011-01-29   0.62349        0.5   0.866025  \n",
       "2011-01-29   0.62349        0.5   0.866025  \n",
       "2011-01-29   0.62349        0.5   0.866025  \n",
       "2011-01-29   0.62349        0.5   0.866025  \n",
       "2011-01-29   0.62349        0.5   0.866025  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f51406ca-f0f9-44ef-b417-9e15d1e9b110",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "from darts.models.forecasting.tft_model import TFTModel\n",
    "from darts.metrics import mse\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import smape, mae\n",
    "from torchmetrics.regression import MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c18e66-feb6-4e27-be41-65b1ec666ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "product1 = df[df['id'] == 'FOODS_3_180_CA_1_validation']\n",
    "product1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16fd3e24-29f7-4128-8419-46337ea6626b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = pd.to_datetime(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5970240-11ce-40c0-8cff-e057e63888c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.index.max(), df.index.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7f1f82f-e1d5-444f-8863-cb2b4468f9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the date 28 days before the maximum date\n",
    "# last_28_start = df.index.max() - pd.Timedelta(days=28)\n",
    "\n",
    "# # Print the last 28 days range\n",
    "# print(last_28_start, \"-\", df.index.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9f78b1-187a-4c89-b2f0-709f65e7c217",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate the date 28 days before the maximum date\n",
    "# second_last_28 = last_28_start - pd.Timedelta(days=28)\n",
    "\n",
    "# # Print the last 28 days range\n",
    "# print(second_last_28, \"-\", last_28_start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77142c3-df80-4014-be38-eb82fc263837",
   "metadata": {},
   "outputs": [],
   "source": [
    "originalindex = df.index\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d471fb-d03e-49f8-8e73-4a95b1d65bfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['DayOfWeek'] = df.index.to_series().dt.dayofweek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4fbc0dc-b34e-4550-a03c-a2e2aa44f377",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from darts.dataprocessing.transformers.scaler import Scaler\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "# scaler = MinMaxScaler()\n",
    "# target_scaler = MinMaxScaler()\n",
    "# target_scaler.fit(df[['sales']])\n",
    "# df = pd.DataFrame(scaler.fit_transform(df), columns=scaler.get_feature_names_out())\n",
    "# df.index = originalindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4489f4d-589f-40dc-8026-46ebaba0faba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = df[['sales']]\n",
    "# past_cov = df.drop(columns=['sales'])\n",
    "# future_cov = df.drop(columns=['sales'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ee5152-fc3b-4ef2-a10b-254d6212c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = product1[['sales']]\n",
    "past_cov = product1.drop(columns=['sales', 'id','item_id','dept_id','state_id'])\n",
    "future_cov = product1.drop(columns=['sales','id','item_id','dept_id','state_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d5a9318-a072-4eae-b497-01f8ac43e97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q darts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5bcae-dd8b-4026-9dca-8f804cc151cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "target.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b94ef70-0234-427d-b1d2-91e6792b5688",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_cov.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e5eb32-e004-4177-8cec-9aa8d4ef303e",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_cov.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e3313b8-07e1-466e-a4b4-6e773a5ebe03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432f71ed-e9e4-469b-80ae-506f1ec5d094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train = target.loc['2011-01-29':'2016-01-29']\n",
    "# past_cov_train = past_cov.loc['2011-01-29':'2016-01-29']\n",
    "# future_cov_train = future_cov.loc['2011-01-29':'2016-03-26']\n",
    "\n",
    "# y_val = target.loc['2016-01-30':'2016-03-26']\n",
    "# past_cov_val = past_cov.loc['2016-01-30':'2016-03-26']\n",
    "# future_cov_val = future_cov.loc['2016-01-30':'2016-04-24']\n",
    "\n",
    "# y_test = target.loc['2016-03-27':'2016-04-24']\n",
    "\n",
    "# y_train_backtest = target.loc['2011-01-29':'2016-03-26']\n",
    "# past_cov_train_backtest = past_cov.loc['2011-01-29':'2016-03-26']\n",
    "# future_cov_train_backtest = future_cov.loc['2011-01-29':'2016-04-24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e05e6db-0097-451e-99bb-41309ebca996",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = target.loc[:'2016-01-01']\n",
    "past_cov_train = past_cov.loc[:'2016-01-01']\n",
    "future_cov_train = future_cov.loc[:'2016-01-29']\n",
    "\n",
    "y_val = target.loc['2016-01-02':'2016-04-24']\n",
    "past_cov_val = past_cov.loc['2016-01-02':'2016-04-24']\n",
    "future_cov_val = future_cov.loc['2016-01-02':'2016-05-22']\n",
    "\n",
    "y_test = target.loc['2016-04-25':'2016-05-22']\n",
    "\n",
    "y_train_backtest = target.loc['2011-01-29':'2016-05-22']\n",
    "past_cov_train_backtest = past_cov.loc['2011-01-29':'2016-05-22']\n",
    "future_cov_train_backtest = past_cov.loc['2011-01-29':'2016-06-19']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdefdd9f-87b4-4076-9150-e9d0a6e3e95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from IPython.display import Image\n",
    "\n",
    "# image_path = '../raw_data/image.png'\n",
    "\n",
    "# # Display the image\n",
    "# Image(filename=image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f4ee8-fd37-48b7-9edc-8ea7f34efc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# product_data = df[df['id'] == id]\n",
    "\n",
    "# y_train = target.loc['2011-01-29':-56]\n",
    "# past_cov_train = past_cov.loc['2011-01-29':'2016-02-27']\n",
    "# future_cov_train = future_cov.loc['2011-01-29':'2016-03-26']\n",
    "\n",
    "# y_val = target.loc['2016-02-28':'2016-03-26']\n",
    "# past_cov_val = past_cov.loc['2016-02-28':'2016-03-26']\n",
    "# future_cov_val = future_cov.loc['2016-03-27':'2016-04-24']\n",
    "\n",
    "# y_test = target.loc['2016-03-27':'2016-04-24']\n",
    "\n",
    "# y_train_backtest = target.loc['2011-01-29':'2016-03-26']\n",
    "# past_cov_train_backtest = past_cov.loc['2011-01-29':'2016-03-26']\n",
    "# future_cov_train_backtest = future_cov.loc['2011-01-29':'2016-04-24']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4f4232-63da-443f-a7df-5c9c5d765710",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train.max())\n",
    "print(y_train.idxmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "534058d8-a129-423c-b75f-25cd787c0635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Check for duplicate timestamps in y_train DataFrame\n",
    "# duplicate_indices = y_train.index.duplicated()\n",
    "# print(\"Duplicate timestamps in y_train:\", duplicate_indices.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b298f7f0-8658-4678-b21e-e490ee57ec25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Aggregate duplicate timestamps by taking the mean\n",
    "# y_train_agg = y_train.groupby(y_train.index).mean()\n",
    "\n",
    "# # Create TimeSeries objects from the aggregated DataFrame\n",
    "# y_train_series = TimeSeries.from_dataframe(y_train_agg, freq='D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cc661c-8577-48d5-87ca-b667cfb26e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example code assuming daily frequency ('D')\n",
    "y_train_series = TimeSeries.from_dataframe(y_train, freq='D')\n",
    "past_cov_train_series = TimeSeries.from_dataframe(past_cov_train, freq='D')\n",
    "future_cov_train_series = TimeSeries.from_dataframe(future_cov_train, freq='D')\n",
    "\n",
    "y_val_series = TimeSeries.from_dataframe(y_val, freq='D')\n",
    "past_cov_val_series = TimeSeries.from_dataframe(past_cov_val, freq='D')\n",
    "future_cov_val_series = TimeSeries.from_dataframe(future_cov_val, freq='D')\n",
    "\n",
    "y_test_series = TimeSeries.from_dataframe(y_test, freq='D')\n",
    "\n",
    "y_train_backtest_series = TimeSeries.from_dataframe(y_train_backtest, freq='D')\n",
    "past_cov_train_backtest_series = TimeSeries.from_dataframe(past_cov_train_backtest, freq='D')\n",
    "future_cov_train_backtest_series = TimeSeries.from_dataframe(future_cov_train_backtest, freq='D')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "463c7de0-c2aa-42bf-bca7-807038cad8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val_series.duration, y_val_series.start_time(), y_val_series.end_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39154265-ef2d-4df8-a06d-201b52425fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_cov_val_series.duration, past_cov_val_series.start_time(), past_cov_val_series.end_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77e8542-26b8-4225-9778-597458c29749",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_cov_val_series.duration, future_cov_val_series.start_time(), future_cov_val_series.end_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b770a0-4d85-4c13-a562-290199c84289",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_series.duration, y_train_series.start_time(), y_train_series.end_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "406bb10e-918a-4604-b1e5-63df0ea3ac02",
   "metadata": {},
   "outputs": [],
   "source": [
    "past_cov_train_series.duration, past_cov_train_series.start_time(), past_cov_train_series.end_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8cd00-c032-4c67-9594-814487d7f879",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_cov_train_series.duration, future_cov_train_series.start_time(), future_cov_train_series.end_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390ca1c9-972a-4bd9-b12e-fb88a586a5dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73b479-3bbf-4e27-a557-54779e324aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TFTModel:\n",
    "input_chunk_length = 28*2\n",
    "output_chunk_length = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65bda305-bbfb-4edc-934f-c4b4674b4592",
   "metadata": {},
   "outputs": [],
   "source": [
    "# series = TimeSeries.from_dataframe(df, 'date', 'sales')\n",
    "\n",
    "# # Split the data into training, validation, and test sets\n",
    "# train_val_test_split = int(len(series) * 0.7)\n",
    "# train, val_test = series[:train_val_test_split], series[train_val_test_split:]\n",
    "# val, test = val_test[:int(len(val_test) * 0.5)], val_test[int(len(val_test) * 0.5):]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5b7c91-63ab-4277-a312-7ebb6e73cb85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "# stop training when validation loss does not decrease more than 0.05 (`min_delta`) over\n",
    "# a period of 5 epochs (`patience`)\n",
    "my_stopper = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    patience=30,\n",
    "    min_delta=0.001,\n",
    "    mode='min',\n",
    ")\n",
    "\n",
    "# use GPU\n",
    "# pl_trainer_kwargs={\"callbacks\": [my_stopper],\n",
    "#                    \"accelerator\": \"gpu\",\n",
    "#                    \"devices\": [0]}\n",
    "\n",
    "# use CPU\n",
    "pl_trainer_kwargs={\"callbacks\": [my_stopper],\n",
    "                   \"accelerator\": \"cpu\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ce0379-b622-40e4-9a3a-d73bc9cb94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Without tuning\n",
    "# tft = TFTModel(input_chunk_length =input_chunk_length ,\n",
    "#                output_chunk_length = output_chunk_length,\n",
    "#                pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "#                torch_metrics=MeanAbsoluteError(),\n",
    "#                n_epochs=50\n",
    "#                )\n",
    "\n",
    "# Advanced tuning\n",
    "tft = TFTModel(input_chunk_length =input_chunk_length ,\n",
    "               output_chunk_length = output_chunk_length,\n",
    "               pl_trainer_kwargs = pl_trainer_kwargs,\n",
    "               lstm_layers=2,\n",
    "               num_attention_heads=4,\n",
    "               dropout=0.2,\n",
    "               batch_size=16,\n",
    "               hidden_size=16,\n",
    "               torch_metrics=MeanAbsoluteError(),\n",
    "               n_epochs=1000,\n",
    "               # add_encoders=add_encoders\n",
    "               )\n",
    "\n",
    "tft.fit(series=y_train_series,\n",
    "        past_covariates = past_cov_train_series,\n",
    "        future_covariates = future_cov_train_series,\n",
    "        val_series=y_val_series,\n",
    "        val_past_covariates=past_cov_val_series,\n",
    "        val_future_covariates=future_cov_val_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4147a131-6e9b-4916-8f65-1fb564edaec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_series.duration, y_train_series.start_time(), y_train_series.end_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2bfb5d-25e0-4bf1-868a-333300fe2ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_val.to_numpy().reshape(168, 7, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d583ccb-8862-4f5c-9d61-da936a74754a",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = tft.predict(n=output_chunk_length,\n",
    "                   series=y_val_series,\n",
    "                   past_covariates = past_cov_val_series,\n",
    "                   future_covariates = future_cov_val_series)\n",
    "# score = mse(y_val_series, preds)\n",
    "# preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e9064-c756-4a3d-8419-15939d78a212",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds.plot(label='prediction_sales')\n",
    "y_test_series[:output_chunk_length].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a9acca-ce34-4dc5-80d1-1bb9464b0e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mae(preds, y_test_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602233c1-c818-4869-b691-b11588f9489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "future_cov_train_backtest_series.duration, future_cov_train_backtest_series.start_time(), future_cov_train_backtest_series.end_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb3022-8179-4c37-9d37-5d214e0e34ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "historical_fcast_tft = tft.historical_forecasts(\n",
    "        series=y_train_backtest_series,\n",
    "        past_covariates=past_cov_train_backtest_series,\n",
    "        future_covariates=future_cov_train_backtest_series,\n",
    "        start=0.3,\n",
    "        forecast_horizon=7,\n",
    "        verbose=False,\n",
    "        retrain=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0d8eec-f3e8-4f15-9fe8-92e9278ff7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_backtest_series.plot(label=\"data\")\n",
    "historical_fcast_tft.plot(low_quantile=0.01, high_quantile=0.99,label=\"backtest ahead forecast (TFTModel)\")\n",
    "print(\"SMAPE = {:.2f}%\".format(smape(historical_fcast_tft, y_train_backtest_series)))\n",
    "print(\"MAE = {:.2f}\".format(mae(historical_fcast_tft, y_train_backtest_series)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2ec7c8-692e-45b3-a20b-722e03554a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_values = target_scaler.inverse_transform(preds.pd_dataframe())\n",
    "real_values = target_scaler.inverse_transform(y_test.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae259b00-8cde-433f-a44e-1b749cc5aa39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smape_function(actual, predicted):\n",
    "    \"\"\"\n",
    "    Calculate Symmetric Mean Absolute Percentage Error (SMAPE) between two arrays.\n",
    "\n",
    "    Parameters:\n",
    "    - actual: array containing actual values\n",
    "    - predicted: array containing predicted values\n",
    "\n",
    "    Returns:\n",
    "    - SMAPE value\n",
    "    \"\"\"\n",
    "    denominator = (np.abs(actual) + np.abs(predicted)) / 2.0\n",
    "    diff = np.abs(actual - predicted) / denominator\n",
    "    diff[denominator == 0] = 0.0  # Handle division by zero\n",
    "    smape_value = np.mean(diff) * 100.0\n",
    "\n",
    "    return smape_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9522b7-ec2e-4bf5-8834-c3b31b8330ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "smape_function(pred_values, real_values[:output_chunk_length])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dbceaf8-a265-420f-ab56-9852b14651ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.explainability.tft_explainer import TFTExplainer\n",
    "\n",
    "explainer = TFTExplainer(tft)\n",
    "results = explainer.explain()\n",
    "# plot the results\n",
    "# explainer.plot_attention(results, plot_type=\"heatmap\")\n",
    "explainer.plot_variable_selection(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6640662c-8c89-4b20-ba37-f1ead426d32f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47221870-f41a-434d-9d41-45d2c6340b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts import TimeSeries\n",
    "from darts.models.forecasting.tft_model import TFTModel\n",
    "from darts.metrics import mse\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import smape, mae\n",
    "from torchmetrics.regression import MeanAbsoluteError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5c7bc651-4c82-44c1-8322-34f597c58d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(product_data):\n",
    "    target = product_data[['sales']]\n",
    "    past_cov = product_data.drop(columns=['sales', 'id','item_id','dept_id','state_id','event_name_2'])\n",
    "    future_cov = product_data.drop(columns=['sales','id','item_id','dept_id','state_id','event_name_2'])\n",
    "\n",
    "    y_train = target.loc[:'2016-01-01']\n",
    "    past_cov_train = past_cov.loc[:'2016-01-01']\n",
    "    future_cov_train = future_cov.loc[:'2016-01-29']\n",
    "\n",
    "    y_val = target.loc['2016-01-02':'2016-04-24']\n",
    "    past_cov_val = past_cov.loc['2016-01-02':'2016-04-24']\n",
    "    future_cov_val = future_cov.loc['2016-01-02':'2016-05-22']\n",
    "\n",
    "    return (y_train, past_cov_train, future_cov_train,\n",
    "            y_val, past_cov_val, future_cov_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed0a5ee1-a27b-48d1-8acd-63da60f14557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tft_model(y_train_series, past_cov_train_series, future_cov_train_series,\n",
    "                    y_val_series, past_cov_val_series, future_cov_val_series):\n",
    "    input_chunk_length = 28*2\n",
    "    output_chunk_length = 28\n",
    "\n",
    "    from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "\n",
    "    my_stopper = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=30,\n",
    "        min_delta=0.001,\n",
    "        mode='min',\n",
    "    )\n",
    "\n",
    "    pl_trainer_kwargs={\"callbacks\": [my_stopper],\n",
    "                       \"accelerator\": \"cpu\"}\n",
    "\n",
    "    tft = TFTModel(input_chunk_length=input_chunk_length,\n",
    "                   output_chunk_length=output_chunk_length,\n",
    "                   pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "                   lstm_layers=2,\n",
    "                   num_attention_heads=4,\n",
    "                   dropout=0.2,\n",
    "                   batch_size=16,\n",
    "                   hidden_size=16,\n",
    "                   torch_metrics=MeanAbsoluteError(),\n",
    "                   n_epochs=500,\n",
    "                   )\n",
    "\n",
    "    tft.fit(series=y_train_series,\n",
    "            past_covariates=past_cov_train_series,\n",
    "            future_covariates=future_cov_train_series,\n",
    "            val_series=y_val_series,\n",
    "            val_past_covariates=past_cov_val_series,\n",
    "            val_future_covariates=future_cov_val_series)\n",
    "\n",
    "    return tft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8a0aafcd-8dbd-4cbc-a2f1-1f770375bf46",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'product_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mproduct_data\u001b[49m\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'product_data' is not defined"
     ]
    }
   ],
   "source": [
    "print(product_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62861e29-417e-4f95-be0f-9b2438846646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for product: FOODS_2_197_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/sljaoudli/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "   | Name                              | Type                             | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | train_metrics                     | MetricCollection                 | 0     \n",
      "1  | val_metrics                       | MetricCollection                 | 0     \n",
      "2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
      "3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
      "4  | encoder_vsn                       | _VariableSelectionNetwork        | 73.5 K\n",
      "5  | decoder_vsn                       | _VariableSelectionNetwork        | 36.7 K\n",
      "6  | static_context_grn                | _GatedResidualNetwork            | 1.1 K \n",
      "7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_enrichment         | _GatedResidualNetwork            | 1.1 K \n",
      "10 | lstm_encoder                      | LSTM                             | 4.4 K \n",
      "11 | lstm_decoder                      | LSTM                             | 4.4 K \n",
      "12 | post_lstm_gan                     | _GateAddNorm                     | 576   \n",
      "13 | static_enrichment_grn             | _GatedResidualNetwork            | 1.4 K \n",
      "14 | multihead_attn                    | _InterpretableMultiHeadAttention | 676   \n",
      "15 | post_attn_gan                     | _GateAddNorm                     | 576   \n",
      "16 | feed_forward_block                | _GatedResidualNetwork            | 1.1 K \n",
      "17 | pre_output_gan                    | _GateAddNorm                     | 576   \n",
      "18 | output_layer                      | Linear                           | 289   \n",
      "----------------------------------------------------------------------------------------\n",
      "127 K     Trainable params\n",
      "0         Non-trainable params\n",
      "127 K     Total params\n",
      "0.511     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sljaoudli/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pytorch_lightning/core/module.py:507: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676c459e83f14c528eda1f7cf63132c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sljaoudli/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pytorch_lightning/core/module.py:507: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
      "/home/sljaoudli/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/sljaoudli/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57503baaf221478caf8b251061f8a7b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                     | 0/? [00:00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for product: FOODS_3_080_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/sljaoudli/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "   | Name                              | Type                             | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | train_metrics                     | MetricCollection                 | 0     \n",
      "1  | val_metrics                       | MetricCollection                 | 0     \n",
      "2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
      "3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
      "4  | encoder_vsn                       | _VariableSelectionNetwork        | 73.5 K\n",
      "5  | decoder_vsn                       | _VariableSelectionNetwork        | 36.7 K\n",
      "6  | static_context_grn                | _GatedResidualNetwork            | 1.1 K \n",
      "7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_enrichment         | _GatedResidualNetwork            | 1.1 K \n",
      "10 | lstm_encoder                      | LSTM                             | 4.4 K \n",
      "11 | lstm_decoder                      | LSTM                             | 4.4 K \n",
      "12 | post_lstm_gan                     | _GateAddNorm                     | 576   \n",
      "13 | static_enrichment_grn             | _GatedResidualNetwork            | 1.4 K \n",
      "14 | multihead_attn                    | _InterpretableMultiHeadAttention | 676   \n",
      "15 | post_attn_gan                     | _GateAddNorm                     | 576   \n",
      "16 | feed_forward_block                | _GatedResidualNetwork            | 1.1 K \n",
      "17 | pre_output_gan                    | _GateAddNorm                     | 576   \n",
      "18 | output_layer                      | Linear                           | 289   \n",
      "----------------------------------------------------------------------------------------\n",
      "127 K     Trainable params\n",
      "0         Non-trainable params\n",
      "127 K     Total params\n",
      "0.511     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sljaoudli/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pytorch_lightning/core/module.py:507: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab2cb96f8d384473831ddb6a9db59c9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sljaoudli/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pytorch_lightning/core/module.py:507: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
      "/home/sljaoudli/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcdae7da5993482aa6614176c4b60a08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                     | 0/? [00:00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for product: FOODS_3_090_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/sljaoudli/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "   | Name                              | Type                             | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | train_metrics                     | MetricCollection                 | 0     \n",
      "1  | val_metrics                       | MetricCollection                 | 0     \n",
      "2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
      "3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
      "4  | encoder_vsn                       | _VariableSelectionNetwork        | 73.5 K\n",
      "5  | decoder_vsn                       | _VariableSelectionNetwork        | 36.7 K\n",
      "6  | static_context_grn                | _GatedResidualNetwork            | 1.1 K \n",
      "7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_enrichment         | _GatedResidualNetwork            | 1.1 K \n",
      "10 | lstm_encoder                      | LSTM                             | 4.4 K \n",
      "11 | lstm_decoder                      | LSTM                             | 4.4 K \n",
      "12 | post_lstm_gan                     | _GateAddNorm                     | 576   \n",
      "13 | static_enrichment_grn             | _GatedResidualNetwork            | 1.4 K \n",
      "14 | multihead_attn                    | _InterpretableMultiHeadAttention | 676   \n",
      "15 | post_attn_gan                     | _GateAddNorm                     | 576   \n",
      "16 | feed_forward_block                | _GatedResidualNetwork            | 1.1 K \n",
      "17 | pre_output_gan                    | _GateAddNorm                     | 576   \n",
      "18 | output_layer                      | Linear                           | 289   \n",
      "----------------------------------------------------------------------------------------\n",
      "127 K     Trainable params\n",
      "0         Non-trainable params\n",
      "127 K     Total params\n",
      "0.511     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sljaoudli/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pytorch_lightning/core/module.py:507: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab5d103e90cc4678ae299c698e691bcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sljaoudli/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pytorch_lightning/core/module.py:507: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n",
      "/home/sljaoudli/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c17dd062a892499db1c7635fe324dedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting: |                                                                                     | 0/? [00:00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for product: FOODS_3_120_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/sljaoudli/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pytorch_lightning/trainer/setup.py:187: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
      "\n",
      "   | Name                              | Type                             | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | train_metrics                     | MetricCollection                 | 0     \n",
      "1  | val_metrics                       | MetricCollection                 | 0     \n",
      "2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
      "3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
      "4  | encoder_vsn                       | _VariableSelectionNetwork        | 73.5 K\n",
      "5  | decoder_vsn                       | _VariableSelectionNetwork        | 36.7 K\n",
      "6  | static_context_grn                | _GatedResidualNetwork            | 1.1 K \n",
      "7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_enrichment         | _GatedResidualNetwork            | 1.1 K \n",
      "10 | lstm_encoder                      | LSTM                             | 4.4 K \n",
      "11 | lstm_decoder                      | LSTM                             | 4.4 K \n",
      "12 | post_lstm_gan                     | _GateAddNorm                     | 576   \n",
      "13 | static_enrichment_grn             | _GatedResidualNetwork            | 1.4 K \n",
      "14 | multihead_attn                    | _InterpretableMultiHeadAttention | 676   \n",
      "15 | post_attn_gan                     | _GateAddNorm                     | 576   \n",
      "16 | feed_forward_block                | _GatedResidualNetwork            | 1.1 K \n",
      "17 | pre_output_gan                    | _GateAddNorm                     | 576   \n",
      "18 | output_layer                      | Linear                           | 289   \n",
      "----------------------------------------------------------------------------------------\n",
      "127 K     Trainable params\n",
      "0         Non-trainable params\n",
      "127 K     Total params\n",
      "0.511     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |                                                                                | 0/? [00:00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sljaoudli/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pytorch_lightning/core/module.py:507: You called `self.log('val_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21c5922d399d44a691f4bb635d0be4f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                       | 0/? [00:00â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sljaoudli/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/pytorch_lightning/core/module.py:507: You called `self.log('train_MeanAbsoluteError', ..., logger=True)` but have no logger configured. You can enable one by doing `Trainer(logger=ALogger(...))`\n"
     ]
    }
   ],
   "source": [
    "results_dict = {}\n",
    "\n",
    "# Iterate over each unique product series identified by id \n",
    "for product_id in df['id'].unique()[:10]:\n",
    "    print(f\"Training model for product: {product_id}\")\n",
    "    \n",
    "    # Extract data for the current product\n",
    "    product_data = df[df['id'] == product_id]\n",
    "    \n",
    "    # Prepare data\n",
    "    (y_train, past_cov_train, future_cov_train,\n",
    "     y_val, past_cov_val, future_cov_val) = prepare_data(product_data)\n",
    "\n",
    "    # Example code assuming daily frequency ('D')\n",
    "    y_train_series = TimeSeries.from_dataframe(y_train, fill_missing_dates=True, freq='D')\n",
    "    past_cov_train_series = TimeSeries.from_dataframe(past_cov_train, fill_missing_dates=True, freq='D')\n",
    "    future_cov_train_series = TimeSeries.from_dataframe(future_cov_train, fill_missing_dates=True, freq='D')\n",
    "\n",
    "    y_val_series = TimeSeries.from_dataframe(y_val, fill_missing_dates=True, freq='D')\n",
    "    past_cov_val_series = TimeSeries.from_dataframe(past_cov_val, fill_missing_dates=True, freq='D')\n",
    "    future_cov_val_series = TimeSeries.from_dataframe(future_cov_val, fill_missing_dates=True, freq='D')\n",
    "\n",
    "    # Train model\n",
    "    trained_model = train_tft_model(y_train_series, past_cov_train_series, future_cov_train_series,\n",
    "                                    y_val_series, past_cov_val_series, future_cov_val_series)\n",
    "\n",
    "    # Store the trained model in the dictionary\n",
    "    results_dict[product_id] = {'model': trained_model, 'forecast': trained_model.predict(n=28)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b294bfb7-68bd-45f2-a174-0640e0634208",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
