{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed87cd7b-df5f-4791-a0e6-ca2d62e7a19f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c053399-9f33-4dc8-9c1c-df89eeb2a010",
   "metadata": {},
   "source": [
    "1️⃣ [FOLDS] Cross-Validation in Time Series\n",
    "\n",
    "Starting from this single Time Series:\n",
    "- We will create FOLDS\n",
    "-Train/Evaluate our LSTM  on each of these different FOLDS to conclude about the robustness of the model.\n",
    "(It is very common to create hundreds of folds in Time Series forecasting, in order to cover all types of external conditions: crash market periods, bull markets, atone markets, etc...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ca2752d-cec7-4a1e-83ae-55d1fc56fbed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned and merged dataset\n",
    "data_path = '../raw_data/cleaned_merge_df_top10.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "data['date'] = pd.to_datetime(data['date'])  # Ensure 'date' is a datetime object\n",
    "data.set_index('date', inplace=True)        # Set 'date' as index if not already\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d68fd3e0-d59c-4da0-a6fc-bf7873c2f99e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (1740, 17), Test shape: (1739, 17)\n",
      "Train shape: (3479, 17), Test shape: (1739, 17)\n",
      "Train shape: (5218, 17), Test shape: (1739, 17)\n",
      "Train shape: (6957, 17), Test shape: (1739, 17)\n",
      "Train shape: (8696, 17), Test shape: (1739, 17)\n",
      "Train shape: (10435, 17), Test shape: (1739, 17)\n",
      "Train shape: (12174, 17), Test shape: (1739, 17)\n",
      "Train shape: (13913, 17), Test shape: (1739, 17)\n",
      "Train shape: (15652, 17), Test shape: (1739, 17)\n",
      "Train shape: (17391, 17), Test shape: (1739, 17)\n"
     ]
    }
   ],
   "source": [
    "# Define the number of splits\n",
    "n_splits = 10  # You can increase this depending on the length and granularity of your data\n",
    "\n",
    "# Create time series cross-validator\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Generate the indices to split data into training and test set\n",
    "for train_index, test_index in tscv.split(data):\n",
    "    train, test = data.iloc[train_index], data.iloc[test_index]\n",
    "    print(f\"Train shape: {train.shape}, Test shape: {test.shape}\")\n",
    "    # Here, you could fit your LSTM model on 'train' and evaluate it on 'test'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e681d41f-3b8c-49b8-9b88-bf6d978f1990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and train the LSTM model\n",
    "def train_lstm(train_data, test_data):\n",
    "    # Assuming 'sales' is the column to predict\n",
    "    X_train, y_train = train_data.drop('sales', axis=1), train_data['sales']\n",
    "    X_test, y_test = test_data.drop('sales', axis=1), test_data['sales']\n",
    "    \n",
    "    # Reshape input to be [samples, time steps, features] which is required for LSTM\n",
    "    X_train = X_train.values.reshape((X_train.shape[0], 1, X_train.shape[1]))\n",
    "    X_test = X_test.values.reshape((X_test.shape[0], 1, X_test.shape[1]))\n",
    "    \n",
    "    # Build LSTM Model\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=72, validation_data=(X_test, y_test), verbose=2, shuffle=False)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    test_predict = model.predict(X_test)\n",
    "    # Compute error metrics, e.g., RMSE\n",
    "    return model, test_predict\n",
    "\n",
    "# Apply this function in your cross-validation loop\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ee9b5e-07bd-4aa8-99df-d8ced9d111cf",
   "metadata": {},
   "source": [
    "2️⃣ [TRAIN-TEST SPLIT] Holdout method\n",
    "\n",
    "For each FOLD, we will do a TRAIN-TEST SPLIT to:\n",
    "fit the model on the train set\n",
    "evaluate it on the test set\n",
    "(Always split the train set **chronologically** before the test set!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22dacd91-0dd7-45a3-9358-7d551c214554",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/julietta/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Build and fit the LSTM model\u001b[39;00m\n\u001b[1;32m     38\u001b[0m model \u001b[38;5;241m=\u001b[39m build_lstm((X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], X_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m]))\n\u001b[0;32m---> 39\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m72\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_validate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_validate\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Evaluate the model on the test set\u001b[39;00m\n\u001b[1;32m     42\u001b[0m test_loss \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "# Define the number of splits for cross-validation\n",
    "n_splits = 5  # Adjust based on your dataset size and granularity\n",
    "\n",
    "# Initialize the TimeSeriesSplit object\n",
    "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "\n",
    "# Placeholder function to prepare data for LSTM\n",
    "def prepare_data(data):\n",
    "    # Assuming 'sales' is the column to predict and others are features\n",
    "    X = data.drop('sales', axis=1).values\n",
    "    y = data['sales'].values\n",
    "    X = X.reshape((X.shape[0], 1, X.shape[1]))\n",
    "    return X, y\n",
    "\n",
    "# Function to create the LSTM model\n",
    "def build_lstm(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(50, input_shape=input_shape))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model\n",
    "\n",
    "# Loop through each fold\n",
    "for train_index, test_index in tscv.split(data):\n",
    "    # Split data into train and test sets\n",
    "    train, test = data.iloc[train_index], data.iloc[test_index]\n",
    "    \n",
    "    # Train-test split inside the train data for validation\n",
    "    train_size = int(len(train) * 0.85)  # Using 85% for training, 15% for validation\n",
    "    train_data, validate_data = train[:train_size], train[train_size:]\n",
    "    \n",
    "    # Prepare data for LSTM\n",
    "    X_train, y_train = prepare_data(train_data)\n",
    "    X_validate, y_validate = prepare_data(validate_data)\n",
    "    X_test, y_test = prepare_data(test)\n",
    "    \n",
    "    # Build and fit the LSTM model\n",
    "    model = build_lstm((X_train.shape[1], X_train.shape[2]))\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=72, validation_data=(X_validate, y_validate), verbose=2, shuffle=False)\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f'Test Loss for the fold: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc703c4-e163-4735-ba29-d44c300572fe",
   "metadata": {},
   "source": [
    "3️⃣ [SEQUENCES] Sampling/Extracting sequences\n",
    "After splitting each fold into a train set and a test set, it is time to:\n",
    "- 🏋 sample lots of sequences on which the model will be trained\n",
    "- 👩🏻‍🏫 sample lots of sequences on which the model will be evaluated\n",
    "\n",
    "👉 All these sequences in the train set and the test set will have a common shape `(input_length, n_features)`\n",
    "👉 Each sequence has a target, the shape of which will be `(output_length, n_targets)` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3af63196-4ccb-49bf-bb5a-55037c139e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_sequences(data, input_length, output_length):\n",
    "    \"\"\"\n",
    "    Samples sequences for LSTM training and evaluation.\n",
    "\n",
    "    Args:\n",
    "    data (DataFrame): The dataset containing features and target.\n",
    "    input_length (int): The number of timesteps in each input sequence.\n",
    "    output_length (int): The number of timesteps in each output sequence.\n",
    "\n",
    "    Returns:\n",
    "    X, y (np.array): Arrays of input sequences and corresponding target sequences.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - input_length - output_length + 1):\n",
    "        # Extract input sequence and corresponding targets\n",
    "        X.append(data.iloc[i:(i + input_length)].values)\n",
    "        y.append(data.iloc[(i + input_length):(i + input_length + output_length)]['sales'].values)\n",
    "    return np.array(X), np.array(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a61928ca-839e-4e39-ae64-974514d888a7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m\n\u001b[1;32m     10\u001b[0m train, test \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39miloc[train_index], data\u001b[38;5;241m.\u001b[39miloc[test_index]\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Sample sequences from train and test data\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43msample_sequences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_length\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m sample_sequences(test, input_length, output_length)\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Assuming the function to build and train the model is already defined\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Build the LSTM model and fit it\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m, in \u001b[0;36msample_sequences\u001b[0;34m(data, input_length, output_length)\u001b[0m\n\u001b[1;32m     16\u001b[0m     X\u001b[38;5;241m.\u001b[39mappend(data\u001b[38;5;241m.\u001b[39miloc[i:(i \u001b[38;5;241m+\u001b[39m input_length)]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[1;32m     17\u001b[0m     y\u001b[38;5;241m.\u001b[39mappend(data\u001b[38;5;241m.\u001b[39miloc[(i \u001b[38;5;241m+\u001b[39m input_length):(i \u001b[38;5;241m+\u001b[39m input_length \u001b[38;5;241m+\u001b[39m output_length)][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msales\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues)\n\u001b[0;32m---> 18\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39marray(X), np\u001b[38;5;241m.\u001b[39marray(y)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# Define parameters\n",
    "input_length = 112  # Number of days in each input sequence\n",
    "output_length = 28  # Number of days to predict\n",
    "\n",
    "# Initialize the TimeSeriesSplit object\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for train_index, test_index in tscv.split(data):\n",
    "    # Split data into train and test sets\n",
    "    train, test = data.iloc[train_index], data.iloc[test_index]\n",
    "    \n",
    "    # Sample sequences from train and test data\n",
    "    X_train, y_train = sample_sequences(train, input_length, output_length)\n",
    "    X_test, y_test = sample_sequences(test, input_length, output_length)\n",
    "    \n",
    "    # Assuming the function to build and train the model is already defined\n",
    "    # Build the LSTM model and fit it\n",
    "    model = build_lstm((X_train.shape[1], X_train.shape[2]))  # Ensure this matches the model definition\n",
    "    model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=2)\n",
    "    \n",
    "    # Evaluate the model on the test set\n",
    "    test_loss = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f'Test Loss for the fold: {test_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f34bf6b8-6aa9-410b-a951-0a483ff94e5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
