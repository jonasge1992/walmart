{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c91c641-7676-421d-aef8-79ef55aa0315",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jonas/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Importing plotly failed. Interactive plots will not work.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import timedelta \n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c5842e-7d08-4165-90ec-b7edfc62dcc7",
   "metadata": {},
   "source": [
    "# 0 Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "023fbbd9-6c97-41d6-bf76-920c4aee83ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>sales</th>\n",
       "      <th>wday</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>...</th>\n",
       "      <th>event_name_1_StPatricksDay</th>\n",
       "      <th>event_name_1_SuperBowl</th>\n",
       "      <th>event_name_1_Thanksgiving</th>\n",
       "      <th>event_name_1_ValentinesDay</th>\n",
       "      <th>event_name_1_VeteransDay</th>\n",
       "      <th>event_name_1_missing</th>\n",
       "      <th>wday_sin</th>\n",
       "      <th>wday_cos</th>\n",
       "      <th>month_sin</th>\n",
       "      <th>month_cos</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_2_197_CA_1_validation</td>\n",
       "      <td>FOODS_2_197</td>\n",
       "      <td>FOODS_2</td>\n",
       "      <td>CA</td>\n",
       "      <td>38</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_3_080_CA_1_validation</td>\n",
       "      <td>FOODS_3_080</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_3_090_CA_1_validation</td>\n",
       "      <td>FOODS_3_090</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>107</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_3_120_CA_1_validation</td>\n",
       "      <td>FOODS_3_120</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>FOODS_3_252_CA_1_validation</td>\n",
       "      <td>FOODS_3_252</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.781832</td>\n",
       "      <td>0.623490</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.866025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-24</th>\n",
       "      <td>FOODS_3_555_CA_1_validation</td>\n",
       "      <td>FOODS_3_555</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>24</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-24</th>\n",
       "      <td>FOODS_3_586_CA_1_validation</td>\n",
       "      <td>FOODS_3_586</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>54</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-24</th>\n",
       "      <td>FOODS_3_587_CA_1_validation</td>\n",
       "      <td>FOODS_3_587</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>26</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-24</th>\n",
       "      <td>FOODS_3_714_CA_1_validation</td>\n",
       "      <td>FOODS_3_714</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>27</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-24</th>\n",
       "      <td>FOODS_3_808_CA_1_validation</td>\n",
       "      <td>FOODS_3_808</td>\n",
       "      <td>FOODS_3</td>\n",
       "      <td>CA</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>missing</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.974928</td>\n",
       "      <td>-0.222521</td>\n",
       "      <td>0.866025</td>\n",
       "      <td>-0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19130 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     id      item_id  dept_id state_id  sales  \\\n",
       "date                                                                            \n",
       "2011-01-29  FOODS_2_197_CA_1_validation  FOODS_2_197  FOODS_2       CA     38   \n",
       "2011-01-29  FOODS_3_080_CA_1_validation  FOODS_3_080  FOODS_3       CA     33   \n",
       "2011-01-29  FOODS_3_090_CA_1_validation  FOODS_3_090  FOODS_3       CA    107   \n",
       "2011-01-29  FOODS_3_120_CA_1_validation  FOODS_3_120  FOODS_3       CA      0   \n",
       "2011-01-29  FOODS_3_252_CA_1_validation  FOODS_3_252  FOODS_3       CA     19   \n",
       "...                                 ...          ...      ...      ...    ...   \n",
       "2016-04-24  FOODS_3_555_CA_1_validation  FOODS_3_555  FOODS_3       CA     24   \n",
       "2016-04-24  FOODS_3_586_CA_1_validation  FOODS_3_586  FOODS_3       CA     54   \n",
       "2016-04-24  FOODS_3_587_CA_1_validation  FOODS_3_587  FOODS_3       CA     26   \n",
       "2016-04-24  FOODS_3_714_CA_1_validation  FOODS_3_714  FOODS_3       CA     27   \n",
       "2016-04-24  FOODS_3_808_CA_1_validation  FOODS_3_808  FOODS_3       CA      0   \n",
       "\n",
       "            wday  month  year event_name_2  snap_CA  ...  \\\n",
       "date                                                 ...   \n",
       "2011-01-29     1      1   0.0      missing        0  ...   \n",
       "2011-01-29     1      1   0.0      missing        0  ...   \n",
       "2011-01-29     1      1   0.0      missing        0  ...   \n",
       "2011-01-29     1      1   0.0      missing        0  ...   \n",
       "2011-01-29     1      1   0.0      missing        0  ...   \n",
       "...          ...    ...   ...          ...      ...  ...   \n",
       "2016-04-24     2      4   1.0      missing        0  ...   \n",
       "2016-04-24     2      4   1.0      missing        0  ...   \n",
       "2016-04-24     2      4   1.0      missing        0  ...   \n",
       "2016-04-24     2      4   1.0      missing        0  ...   \n",
       "2016-04-24     2      4   1.0      missing        0  ...   \n",
       "\n",
       "            event_name_1_StPatricksDay  event_name_1_SuperBowl  \\\n",
       "date                                                             \n",
       "2011-01-29                         0.0                     0.0   \n",
       "2011-01-29                         0.0                     0.0   \n",
       "2011-01-29                         0.0                     0.0   \n",
       "2011-01-29                         0.0                     0.0   \n",
       "2011-01-29                         0.0                     0.0   \n",
       "...                                ...                     ...   \n",
       "2016-04-24                         0.0                     0.0   \n",
       "2016-04-24                         0.0                     0.0   \n",
       "2016-04-24                         0.0                     0.0   \n",
       "2016-04-24                         0.0                     0.0   \n",
       "2016-04-24                         0.0                     0.0   \n",
       "\n",
       "            event_name_1_Thanksgiving  event_name_1_ValentinesDay  \\\n",
       "date                                                                \n",
       "2011-01-29                        0.0                         0.0   \n",
       "2011-01-29                        0.0                         0.0   \n",
       "2011-01-29                        0.0                         0.0   \n",
       "2011-01-29                        0.0                         0.0   \n",
       "2011-01-29                        0.0                         0.0   \n",
       "...                               ...                         ...   \n",
       "2016-04-24                        0.0                         0.0   \n",
       "2016-04-24                        0.0                         0.0   \n",
       "2016-04-24                        0.0                         0.0   \n",
       "2016-04-24                        0.0                         0.0   \n",
       "2016-04-24                        0.0                         0.0   \n",
       "\n",
       "            event_name_1_VeteransDay  event_name_1_missing  wday_sin  \\\n",
       "date                                                                   \n",
       "2011-01-29                       0.0                   1.0  0.781832   \n",
       "2011-01-29                       0.0                   1.0  0.781832   \n",
       "2011-01-29                       0.0                   1.0  0.781832   \n",
       "2011-01-29                       0.0                   1.0  0.781832   \n",
       "2011-01-29                       0.0                   1.0  0.781832   \n",
       "...                              ...                   ...       ...   \n",
       "2016-04-24                       0.0                   1.0  0.974928   \n",
       "2016-04-24                       0.0                   1.0  0.974928   \n",
       "2016-04-24                       0.0                   1.0  0.974928   \n",
       "2016-04-24                       0.0                   1.0  0.974928   \n",
       "2016-04-24                       0.0                   1.0  0.974928   \n",
       "\n",
       "            wday_cos  month_sin  month_cos  \n",
       "date                                        \n",
       "2011-01-29  0.623490   0.500000   0.866025  \n",
       "2011-01-29  0.623490   0.500000   0.866025  \n",
       "2011-01-29  0.623490   0.500000   0.866025  \n",
       "2011-01-29  0.623490   0.500000   0.866025  \n",
       "2011-01-29  0.623490   0.500000   0.866025  \n",
       "...              ...        ...        ...  \n",
       "2016-04-24 -0.222521   0.866025  -0.500000  \n",
       "2016-04-24 -0.222521   0.866025  -0.500000  \n",
       "2016-04-24 -0.222521   0.866025  -0.500000  \n",
       "2016-04-24 -0.222521   0.866025  -0.500000  \n",
       "2016-04-24 -0.222521   0.866025  -0.500000  \n",
       "\n",
       "[19130 rows x 58 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "merge_df_scaled = pd.read_csv('../raw_data/merge_df_resize.csv')\n",
    "merge_df_scaled['date'] = pd.to_datetime(merge_df_scaled['date'])\n",
    "merge_df_scaled.set_index('date', inplace=True)\n",
    "\n",
    "merge_df_scaled\n",
    "# 382600 rows × 64 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93811f0b-32c4-4beb-b498-127548d1ddf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(merge_df_scaled):\n",
    "    # Ignore all warnings within this function\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "        #lagged features\n",
    "        for i in range(1, 8):\n",
    "            merge_df_scaled[f'sales_lag_{i}'] = merge_df_scaled['sales'].shift(i)\n",
    "    \n",
    "        #lagged features per years\n",
    "        for i in range(1, 4):\n",
    "            merge_df_scaled[f'sales_lag_{i}years'] = merge_df_scaled['sales'].shift(i * 365)\n",
    "    \n",
    "            #rolling sum\n",
    "        merge_df_scaled['rolling_sum_7'] = merge_df_scaled['sales'].rolling(window=7).sum()\n",
    "        merge_df_scaled['rolling_sum_30'] = merge_df_scaled['sales'].rolling(window=30).sum()\n",
    "        merge_df_scaled['rolling_sum_60'] = merge_df_scaled['sales'].rolling(window=60).sum()\n",
    "        merge_df_scaled['rolling_sum_90'] = merge_df_scaled['sales'].rolling(window=90).sum()\n",
    "        merge_df_scaled['rolling_sum_120'] = merge_df_scaled['sales'].rolling(window=120).sum()\n",
    "    \n",
    "        #rolling average\n",
    "        merge_df_scaled['rolling_mean_7'] = merge_df_scaled['sales'].rolling(window=7).mean()\n",
    "        merge_df_scaled['rolling_mean_30'] = merge_df_scaled['sales'].rolling(window=30).mean()\n",
    "        merge_df_scaled['rolling_mean_60'] = merge_df_scaled['sales'].rolling(window=60).mean()\n",
    "        merge_df_scaled['rolling_mean_90'] = merge_df_scaled['sales'].rolling(window=90).mean()\n",
    "        merge_df_scaled['rolling_mean_120'] = merge_df_scaled['sales'].rolling(window=120).mean()\n",
    "    \n",
    "        #rolling stdv\n",
    "        merge_df_scaled['rolling_stdv_7'] = merge_df_scaled['sales'].rolling(window=7).std()\n",
    "        merge_df_scaled['rolling_stdv_30'] = merge_df_scaled['sales'].rolling(window=30).std()\n",
    "        merge_df_scaled['rolling_stdv_60'] = merge_df_scaled['sales'].rolling(window=60).std()\n",
    "        merge_df_scaled['rolling_stdv_90'] = merge_df_scaled['sales'].rolling(window=90).std()\n",
    "        merge_df_scaled['rolling_stdv_120'] = merge_df_scaled['sales'].rolling(window=120).std()\n",
    "    \n",
    "        merge_df_scaled.fillna(0,inplace=True)\n",
    "        \n",
    "\n",
    "    return merge_df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b02df851-703f-412f-8f52-cf105070d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction_transfer_test(train_df,test_df):\n",
    "\n",
    "    # Ignore all warnings within this function\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "           \n",
    "    \n",
    "        #lagged features per years\n",
    "        for i in range(1, 4):\n",
    "            test_df[f'sales_lag_{i}years'] = train_df[f'sales_lag_{i}years'].iloc[-1]\n",
    "    \n",
    "            #rolling sum\n",
    "        test_df['rolling_sum_7'] = train_df['rolling_sum_7'].iloc[-1]\n",
    "        test_df['rolling_sum_30'] = train_df['rolling_sum_30'].iloc[-1]\n",
    "        test_df['rolling_sum_60'] = train_df['rolling_sum_60'].iloc[-1]\n",
    "        test_df['rolling_sum_90'] = train_df['rolling_sum_90'].iloc[-1]\n",
    "        test_df['rolling_sum_120'] = train_df['rolling_sum_120'].iloc[-1]\n",
    "        \n",
    "        # Rolling average\n",
    "        test_df['rolling_mean_7'] = train_df['rolling_mean_7'].iloc[-1]\n",
    "        test_df['rolling_mean_30'] = train_df['rolling_mean_30'].iloc[-1]\n",
    "        test_df['rolling_mean_60'] = train_df['rolling_mean_60'].iloc[-1]\n",
    "        test_df['rolling_mean_90'] = train_df['rolling_mean_90'].iloc[-1]\n",
    "        test_df['rolling_mean_120'] = train_df['rolling_mean_120'].iloc[-1]\n",
    "        \n",
    "        # Rolling standard deviation\n",
    "        test_df['rolling_stdv_7'] = train_df['rolling_stdv_7'].iloc[-1]\n",
    "        test_df['rolling_stdv_30'] = train_df['rolling_stdv_30'].iloc[-1]\n",
    "        test_df['rolling_stdv_60'] = train_df['rolling_stdv_60'].iloc[-1]\n",
    "        test_df['rolling_stdv_90'] = train_df['rolling_stdv_90'].iloc[-1]\n",
    "        test_df['rolling_stdv_120'] = train_df['rolling_stdv_120'].iloc[-1]\n",
    "    \n",
    "        # Identify the last available date in the training data\n",
    "        last_date_train = train_df.index[-1]\n",
    "    \n",
    "        # Fill in lagged features for the first few rows where future knowledge is available\n",
    "        #for i in range(1, 8):\n",
    "        #    # Identify the lagged date for the current lag\n",
    "        #    lagged_date = last_date_train - pd.Timedelta(days=i)\n",
    "            \n",
    "            # Fill in the lagged sales values for corresponding lagged days from the training data\n",
    "        #    test_df[f'sales_lag_{i}'] = test_df.index.map(lambda x: train_df.loc[x - pd.Timedelta(days=i), 'sales'] if x <= last_date_train else train_df[f'sales_lag_{i}'].iloc[-1])\n",
    "\n",
    "\n",
    "        # Fill in lagged features for the first few rows where future knowledge is available\n",
    "        for i in range(1, 8):\n",
    "            test_df[f'sales_lag_{i}'] = np.nan  # Initialize with NaN\n",
    "            \n",
    "            # Iterate over each row in the test DataFrame\n",
    "            for idx, row in test_df.iterrows():\n",
    "                lagged_date = idx - pd.Timedelta(days=i)  # Calculate the lagged date\n",
    "                \n",
    "                # Check if the lagged date is within the training data range\n",
    "                if lagged_date in train_df.index:\n",
    "                    test_df.at[idx, f'sales_lag_{i}'] = train_df.loc[lagged_date, 'sales']\n",
    "                else:\n",
    "                    test_df.at[idx, f'sales_lag_{i}'] = train_df['sales'].iloc[-1]\n",
    "    \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "013dda06-1b86-4216-a9f8-366e5e27f7fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rmsse(train, test, predictions):\n",
    "    forecast_mse = mean_squared_error(test, predictions)\n",
    "    train_mse = ((train - train.shift(1)) ** 2).mean()\n",
    "    return np.sqrt(forecast_mse / train_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6e0fca26-43dc-4b21-b08e-65bd9d58aa40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_last_28_days_weights(df):\n",
    "    most_recent_date = df.index.max()\n",
    "    cutoff_date = most_recent_date - timedelta(days=27)\n",
    "    last_28_days_df = df.loc[cutoff_date:most_recent_date]\n",
    "    last_28_days_df['revenues'] = last_28_days_df['sales'] * last_28_days_df['sell_price']\n",
    "    \n",
    "    weights_df = last_28_days_df.groupby('id')['revenues'].sum()\n",
    "    weights_df = pd.DataFrame(weights_df)\n",
    "    \n",
    "    total_revenues = last_28_days_df['revenues'].sum()\n",
    "    weights_df['weights'] = weights_df['revenues'] / total_revenues\n",
    "    \n",
    "    return weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d1e8448-ecde-4944-99b4-30f74e5f489a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df_scaled.drop(columns=[\"item_id\",\"dept_id\",\"state_id\",\"event_name_2\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d9391eb6-da51-4b69-a4aa-5fd1ccba9db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df_scaled.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13a35134-1f8a-436c-b168-111341c17cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FOODS_2_197_CA_1_validation', 'FOODS_3_080_CA_1_validation',\n",
       "       'FOODS_3_090_CA_1_validation', 'FOODS_3_120_CA_1_validation',\n",
       "       'FOODS_3_252_CA_1_validation', 'FOODS_3_555_CA_1_validation',\n",
       "       'FOODS_3_586_CA_1_validation', 'FOODS_3_587_CA_1_validation',\n",
       "       'FOODS_3_714_CA_1_validation', 'FOODS_3_808_CA_1_validation'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df_scaled[\"id\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c90d7-181f-4c34-b9db-73d5632292e5",
   "metadata": {},
   "source": [
    "# 1. Defining Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "44c2e333-1676-4450-a8ac-b1d64ce1fee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_prophet(product_data):\n",
    "\n",
    "    product_data.reset_index(inplace=True,names=\"date\")\n",
    "    \n",
    "    prophet_product_df = product_data[[\"id\",\"date\",\"sales\"]]\n",
    "    prophet_product_df.columns = [\"id\",\"ds\",\"y\"]\n",
    "    prophet_product_df['ds'] = pd.to_datetime(prophet_product_df['ds'])\n",
    "    \n",
    "    data_train = prophet_product_df.iloc[:-28]\n",
    "    data_test = prophet_product_df.iloc[-28:]\n",
    "    X_train = data_train[\"ds\"]\n",
    "    y_train = data_train[\"y\"]\n",
    "    X_test = data_test[\"ds\"]\n",
    "    y_test = data_test[\"y\"]\n",
    "    \n",
    "    fbp = Prophet()\n",
    "\n",
    "    model = fbp.fit(data_train)\n",
    "    \n",
    "    predict_placeholder = fbp.make_future_dataframe(28,freq=\"D\")\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = fbp.predict(predict_placeholder[-28:])\n",
    "    \n",
    "\n",
    "    # Calculate and return the error metric for the current fold\n",
    "    rmsse = calc_rmsse(y_train, y_test, y_pred[\"yhat\"])\n",
    "    \n",
    "    return model, rmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "151bd3d5-e70c-41ef-8a3e-2da51e4fe900",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_auto_arima(product_data):\n",
    "    data_train = product_data.iloc[:-28]\n",
    "    data_test = product_data.iloc[-28:]\n",
    "    y_train = data_train[\"sales\"]\n",
    "    y_test = data_test[\"sales\"]\n",
    "\n",
    "    # Fit ARIMA model on the training data using auto_arima to find the best (p, d, q)\n",
    "    model = auto_arima(y_train, start_p=0, start_q=0, max_p=5, max_q=5, d=1,\n",
    "                       seasonal=True, trace=False, error_action='ignore', \n",
    "                       suppress_warnings=True, stepwise=True)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    predictions = model.predict(n_periods=len(y_test))\n",
    "\n",
    "    # Calculate and return the error metric for the current fold\n",
    "    rmsse = calc_rmsse(y_train,y_test,predictions)\n",
    "    \n",
    "    return model, rmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bc9e1224-e66f-4e66-bad4-dd867e626f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_optuna(trial, y_train, y_test):\n",
    "    \n",
    "    trend = trial.suggest_categorical('trend', ['add'])\n",
    "    seasonal = trial.suggest_categorical('seasonal', [None, 'add'])\n",
    "    seasonal_periods = trial.suggest_categorical('seasonal_periods', [None, 4, 7, 12])\n",
    "    \n",
    "    product_results = []\n",
    "\n",
    "    # Fit Holt-Winters model on the training data\n",
    "    model = ExponentialSmoothing(y_train, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods,freq='D')\n",
    "    fitted_model = model.fit(optimized=True)\n",
    "\n",
    "    # Predict on the test data\n",
    "    predictions = fitted_model.forecast(steps=len(y_test))\n",
    "\n",
    "    # Calculate and store the error metric\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    \n",
    "\n",
    "    # Calculate and return the error metric for the current fold\n",
    "    rmsse = calc_rmsse(y_train,y_test, predictions)\n",
    "    product_results.append(rmsse)\n",
    "\n",
    "    # Average MAE for this product\n",
    "    average_rmsse = np.mean(product_results)\n",
    "    return average_rmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "102b594c-39b7-4817-bb4b-f3d5706c0d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_exp_smoothing(product_data):\n",
    "    data_train = product_data.iloc[:-28]\n",
    "    data_test = product_data.iloc[-28:]\n",
    "    y_train = data_train[\"sales\"]\n",
    "    y_test = data_test[\"sales\"]\n",
    "    # Create a study object\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    \n",
    "    print(f\"Optimizing hyperparameters for product: {id}\")\n",
    "    \n",
    "    \n",
    "    # Run the optimization process for the current product\n",
    "    study.optimize(lambda trial: objective_optuna(trial, y_train, y_test), n_trials=10, n_jobs=-1)\n",
    "\n",
    "    # Get the best hyperparameters and the corresponding best MAE\n",
    "    best_params = study.best_params\n",
    "    best_rmsse = study.best_value\n",
    "\n",
    "    # Create the best model with the obtained hyperparameters\n",
    "    best_model = ExponentialSmoothing(y_train, **best_params).fit()\n",
    "    \n",
    "    return best_model, best_rmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e96217a-a286-4ecd-b8d3-9d23a0fc3811",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "def perform_lightgbm(product_data):\n",
    "    \n",
    "    data_train_val = product_data.iloc[:-28]\n",
    "    data_train_val = feature_extraction(data_train_val)\n",
    "    data_test = product_data.iloc[-28:]\n",
    "    data_test = feature_extraction_transfer_test(data_train_val,data_test)\n",
    "\n",
    "    data_train = data_train_val.iloc[:-112]\n",
    "    data_val = data_train_val.iloc[-112:]\n",
    "    \n",
    "    X_train = data_train.drop(columns=\"sales\")\n",
    "    y_train = data_train[\"sales\"]\n",
    "    X_val = data_val.drop(columns=\"sales\")\n",
    "    y_val = data_val[\"sales\"]\n",
    "    X_test = data_test.drop(columns=\"sales\")\n",
    "    y_test = data_test[\"sales\"]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define LightGBM parameters\n",
    "    params = {\n",
    "        \"n_estimators\": 1000,\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"max_depth\": -1,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.4,\n",
    "        \"lambda_l1\": 1,\n",
    "        \"lambda_l2\": 1,\n",
    "        \"seed\": 46,\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    \n",
    "    # Create dataset for LightGBM\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "    \n",
    "    # Train the model\n",
    "    num_round = 1000\n",
    "\n",
    "    bst = lgb.train(params, lgb_train, num_round, valid_sets=lgb_eval, callbacks=[lgb.early_stopping(stopping_rounds=50)])\n",
    "     \n",
    "    # Make predictions for the next 28 days\n",
    "    predictions = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "    # Calculate and return the error metric for the current fold\n",
    "    rmsse = calc_rmsse(y_train,y_test, predictions)\n",
    "    \n",
    "    return bst, rmsse\n",
    "\n",
    "# Example usage:\n",
    "# sales_forecast = forecast_sales(product_data)\n",
    "# print(sales_forecast)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4313bdf-003b-428f-addb-07ad6b7058b8",
   "metadata": {},
   "source": [
    "# 2.Running all models in a loop to find for each product with lowest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e09341ed-7407-4421-87d4-b97c92ef1fc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_list = [\"ARIMA\",\"Prophet\",\"ExponentialSmoothing\",\"LightGBM\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25636b8c-848f-4188-bc73-1546fdeb687b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79613/714081597.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  last_28_days_df['revenues'] = last_28_days_df['sales'] * last_28_days_df['sell_price']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing product: FOODS_2_197_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79613/3939290341.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_product_df['ds'] = pd.to_datetime(prophet_product_df['ds'])\n",
      "16:35:03 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:35:04 - cmdstanpy - INFO - Chain [1] done processing\n",
      "[I 2024-05-10 16:35:04,145] A new study created in memory with name: no-name-bff07e7d-b007-4ebe-ab3d-301971d914da\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: FOODS_2_197_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-10 16:35:05,747] Trial 0 finished with value: 1.0070735628480108 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 0 with value: 1.0070735628480108.\n",
      "[I 2024-05-10 16:35:05,875] Trial 2 finished with value: 1.0070735628480108 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 0 with value: 1.0070735628480108.\n",
      "[I 2024-05-10 16:35:05,925] Trial 3 finished with value: 1.0070735628480108 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 0 with value: 1.0070735628480108.\n",
      "[I 2024-05-10 16:35:05,940] Trial 9 finished with value: 1.0070735628480108 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 0 with value: 1.0070735628480108.\n",
      "[I 2024-05-10 16:35:05,969] Trial 6 finished with value: 1.0458704311403648 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 0 with value: 1.0070735628480108.\n",
      "[I 2024-05-10 16:35:06,010] Trial 4 finished with value: 1.0070735628480108 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 0 with value: 1.0070735628480108.\n",
      "[I 2024-05-10 16:35:06,038] Trial 8 finished with value: 1.0070735628480108 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 0 with value: 1.0070735628480108.\n",
      "[I 2024-05-10 16:35:06,426] Trial 5 finished with value: 1.1054026386361062 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 0 with value: 1.0070735628480108.\n",
      "[I 2024-05-10 16:35:06,432] Trial 1 finished with value: 1.1054026386361062 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 0 with value: 1.0070735628480108.\n",
      "[I 2024-05-10 16:35:06,481] Trial 7 finished with value: 1.1054026386361062 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 0 with value: 1.0070735628480108.\n",
      "/home/jonas/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/jonas/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000337 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4460\n",
      "[LightGBM] [Info] Number of data points in the train set: 1773, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 21.664975\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[998]\tvalid_0's rmse: 4.79687\n",
      "{'ARIMA': {'rmsse': 0.8689077819635089, 'model': ARIMA(order=(2, 1, 1), scoring_args={}, suppress_warnings=True,\n",
      "      with_intercept=False)}, 'Prophet': {'rmsse': 1.1436628121182213, 'model': <prophet.forecaster.Prophet object at 0x7f6422a3f580>}, 'ExponentialSmoothing': {'rmsse': 1.0070735628480108, 'model': <statsmodels.tsa.holtwinters.results.HoltWintersResultsWrapper object at 0x7f641ce56f20>}, 'LightGBM': {'rmsse': 1.5971657123779857, 'model': <lightgbm.basic.Booster object at 0x7f641ca31240>}}\n",
      "Model results for FOODS_2_197_CA_1_validation\n",
      "Best model: ARIMA\n",
      "Best score: 0.8689077819635089\n",
      "Analyzing product: FOODS_3_080_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_79613/3939290341.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_product_df['ds'] = pd.to_datetime(prophet_product_df['ds'])\n",
      "16:35:24 - cmdstanpy - INFO - Chain [1] start processing\n",
      "16:35:24 - cmdstanpy - INFO - Chain [1] done processing\n",
      "[I 2024-05-10 16:35:24,314] A new study created in memory with name: no-name-738daa50-60a9-4074-bee7-5a9811a8eccb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: FOODS_3_080_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-10 16:35:26,698] Trial 1 finished with value: 0.713635784527659 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 1 with value: 0.713635784527659.\n",
      "[I 2024-05-10 16:35:26,709] Trial 4 finished with value: 0.713635784527659 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 1 with value: 0.713635784527659.\n",
      "[I 2024-05-10 16:35:26,766] Trial 6 finished with value: 0.713635784527659 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 1 with value: 0.713635784527659.\n",
      "[I 2024-05-10 16:35:27,058] Trial 0 finished with value: 0.7166061982441831 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 1 with value: 0.713635784527659.\n",
      "[I 2024-05-10 16:35:27,103] Trial 8 finished with value: 0.7166061982441831 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 1 with value: 0.713635784527659.\n",
      "[I 2024-05-10 16:35:27,219] Trial 2 finished with value: 0.6719310015660904 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 2 with value: 0.6719310015660904.\n",
      "[I 2024-05-10 16:35:27,242] Trial 5 finished with value: 0.7134859108767532 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 2 with value: 0.6719310015660904.\n",
      "[I 2024-05-10 16:35:27,258] Trial 3 finished with value: 0.7134859108767532 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 2 with value: 0.6719310015660904.\n",
      "[I 2024-05-10 16:35:27,272] Trial 7 finished with value: 0.6719310015660904 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 7}. Best is trial 2 with value: 0.6719310015660904.\n",
      "[I 2024-05-10 16:35:27,311] Trial 9 finished with value: 0.6719310015660904 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 7}. Best is trial 2 with value: 0.6719310015660904.\n",
      "/home/jonas/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/home/jonas/.pyenv/versions/3.10.6/envs/walmart/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003942 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4072\n",
      "[LightGBM] [Info] Number of data points in the train set: 1773, number of used features: 41\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 20.687535\n",
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 5.37814\n",
      "{'ARIMA': {'rmsse': 0.7763824286056337, 'model': ARIMA(order=(5, 1, 1), scoring_args={}, suppress_warnings=True,\n",
      "      with_intercept=False)}, 'Prophet': {'rmsse': 0.6727186368135646, 'model': <prophet.forecaster.Prophet object at 0x7f641c7919c0>}, 'ExponentialSmoothing': {'rmsse': 0.6719310015660904, 'model': <statsmodels.tsa.holtwinters.results.HoltWintersResultsWrapper object at 0x7f641c793b50>}, 'LightGBM': {'rmsse': 1.155753062646979, 'model': <lightgbm.basic.Booster object at 0x7f641ca319c0>}}\n",
      "Model results for FOODS_3_080_CA_1_validation\n",
      "Best model: ExponentialSmoothing\n",
      "Best score: 0.6719310015660904\n",
      "Analyzing product: FOODS_3_090_CA_1_validation\n"
     ]
    }
   ],
   "source": [
    "from pmdarima import auto_arima\n",
    "\n",
    "# Dictionary to store MAE results for each unique time-series identified by id\n",
    "product_results = {}\n",
    "average_rmsse = []\n",
    "\n",
    "ids_filtered=merge_df_scaled['id'].unique()[:10]\n",
    "filtered_df = merge_df_scaled[merge_df_scaled['id'].isin(ids_filtered)]\n",
    "weights=calculate_last_28_days_weights(filtered_df)   \n",
    "\n",
    "# Iterate over each unique product series identified by id\n",
    "for id in ids_filtered:\n",
    "    print(f\"Analyzing product: {id}\")\n",
    "    product_data = merge_df_scaled[merge_df_scaled['id'] == id].drop(columns=\"id\")\n",
    "    product_data_with_id = merge_df_scaled[merge_df_scaled['id'] == id]\n",
    "\n",
    "    # Results list for the current product time-series\n",
    "    results = {}\n",
    "    best_score = 999.99\n",
    "    best_model_name = \"\"\n",
    "\n",
    "\n",
    "    product_weight=weights.loc[id].weights\n",
    "\n",
    "\n",
    "\n",
    "    #Looping all models\n",
    "    for model_name in models_list:\n",
    "\n",
    "        if model_name == \"ARIMA\":\n",
    "            #TODO: Add 5-fold split here for another loop (or inside the model function?) and then take the average score per model as their mae score\n",
    "            \n",
    "            # Fit ARIMA model on the training data using auto_arima to find the best (p, d, q)\n",
    "            model, rmsse = perform_auto_arima(product_data)\n",
    "            results[model_name] = {\"rmsse\": rmsse, \"model\": model}\n",
    "            if rmsse < best_score:\n",
    "                best_score = rmsse\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "\n",
    "        elif model_name == \"ExponentialSmoothing\":\n",
    "\n",
    "            # To be built\n",
    "            model, rmsse = perform_exp_smoothing(product_data)\n",
    "            results[model_name] = {\"rmsse\": rmsse, \"model\": model}\n",
    "            if rmsse < best_score:\n",
    "                best_score = rmsse\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "\n",
    "        elif model_name == \"Prophet\":\n",
    "\n",
    "            model, rmsse = perform_prophet(product_data_with_id)\n",
    "            results[model_name] = {\"rmsse\": rmsse, \"model\": model}\n",
    "            if rmsse < best_score:\n",
    "                best_score = rmsse\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "\n",
    "\n",
    "        elif model_name == \"LightGBM\":\n",
    "\n",
    "            model, rmsse = perform_lightgbm(product_data)\n",
    "            results[model_name] = {\"rmsse\": rmsse, \"model\": model}\n",
    "            if rmsse < best_score:\n",
    "                best_score = rmsse\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "\n",
    "\n",
    "    #Printing results for this product\n",
    "    print(results)\n",
    "    print(f\"Model results for {id}\")\n",
    "    print(f\"Best model: {best_model_name}\")\n",
    "    print(f\"Best score: {best_score}\")\n",
    "\n",
    "    average_rmsse.append(best_score*product_weight)\n",
    "\n",
    "    # Store the average MAE for the current product time-series\n",
    "    product_results[id] = {\"best_score\": best_score, \"best_model\": best_model_name, \"model\": best_model}\n",
    "\n",
    "    #Store the best model in a pkl file\n",
    "    filename = f'../models/{id}_model.pkl'\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(best_model, f)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df_arima = pd.DataFrame(product_results.items(), columns=['id', 'RMSSE'])\n",
    "\n",
    "# Set the 'id' column as the index\n",
    "results_df_arima.set_index('id', inplace=True)\n",
    "\n",
    "average_rmsse_score = np.sum(average_rmsse)\n",
    "\n",
    "print(f\"Total average wRMSSE: {average_rmsse_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f13ac0a-fa34-4333-9abf-2dc1e7592607",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
