{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c91c641-7676-421d-aef8-79ef55aa0315",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "import pickle\n",
    "import warnings\n",
    "from datetime import timedelta \n",
    "import datetime\n",
    "\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import warnings\n",
    "from prophet import Prophet\n",
    "from prophet.diagnostics import cross_validation, performance_metrics\n",
    "from prophet.plot import plot_cross_validation_metric\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pmdarima\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import lightgbm as lgb\n",
    "\n",
    "from darts import TimeSeries\n",
    "from darts.models.forecasting.tft_model import TFTModel\n",
    "from darts.metrics import mse\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "from darts.metrics import smape, mae\n",
    "from torchmetrics.regression import MeanAbsoluteError\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ab5d5-4860-4ec1-854b-9a5fbcf69082",
   "metadata": {},
   "source": [
    "# -1 Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c23922c-8d99-4757-aa9b-2a4684ab8a30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#csv_file_path = \n",
    "train_df = pd.read_csv('../raw_data/sales_train_validation.csv')\n",
    "prices_df = pd.read_csv('../raw_data/sell_prices.csv')\n",
    "calendar_df = pd.read_csv('../raw_data/calendar.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "195acbf7-4954-4faa-9805-9e95a2cfe84f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_filtered = train_df.iloc[0:200,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bed29bb3-435c-4c7e-93cf-ca208ab5ee68",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_sample = train_df_filtered.melt(id_vars=['id','item_id','dept_id','cat_id','store_id','state_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9357d41-9baa-49b9-8cc9-e5f46c8fa05a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df_sample.rename(columns={'variable': 'd', 'value':'sales'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2cdabd0b-3596-4d78-b3cd-e1216f64dd60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_df = train_df_sample.merge(calendar_df,on='d',how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc759ed2-cffe-4513-9a95-82f06bdfffae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>date</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>...</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2011-01-29</td>\n",
       "      <td>11101</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382595</th>\n",
       "      <td>HOBBIES_1_202_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_202</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>11613</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382596</th>\n",
       "      <td>HOBBIES_1_203_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_203</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>11613</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382597</th>\n",
       "      <td>HOBBIES_1_204_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_204</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>11613</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382598</th>\n",
       "      <td>HOBBIES_1_205_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_205</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>11613</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382599</th>\n",
       "      <td>HOBBIES_1_206_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_206</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2016-04-24</td>\n",
       "      <td>11613</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382600 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   id        item_id    dept_id   cat_id  \\\n",
       "0       HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "1       HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES   \n",
       "2       HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES   \n",
       "3       HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES   \n",
       "4       HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES   \n",
       "...                               ...            ...        ...      ...   \n",
       "382595  HOBBIES_1_202_CA_1_validation  HOBBIES_1_202  HOBBIES_1  HOBBIES   \n",
       "382596  HOBBIES_1_203_CA_1_validation  HOBBIES_1_203  HOBBIES_1  HOBBIES   \n",
       "382597  HOBBIES_1_204_CA_1_validation  HOBBIES_1_204  HOBBIES_1  HOBBIES   \n",
       "382598  HOBBIES_1_205_CA_1_validation  HOBBIES_1_205  HOBBIES_1  HOBBIES   \n",
       "382599  HOBBIES_1_206_CA_1_validation  HOBBIES_1_206  HOBBIES_1  HOBBIES   \n",
       "\n",
       "       store_id state_id       d  sales        date  wm_yr_wk  ... month  \\\n",
       "0          CA_1       CA     d_1    0.0  2011-01-29     11101  ...     1   \n",
       "1          CA_1       CA     d_1    0.0  2011-01-29     11101  ...     1   \n",
       "2          CA_1       CA     d_1    0.0  2011-01-29     11101  ...     1   \n",
       "3          CA_1       CA     d_1    0.0  2011-01-29     11101  ...     1   \n",
       "4          CA_1       CA     d_1    0.0  2011-01-29     11101  ...     1   \n",
       "...         ...      ...     ...    ...         ...       ...  ...   ...   \n",
       "382595     CA_1       CA  d_1913    0.0  2016-04-24     11613  ...     4   \n",
       "382596     CA_1       CA  d_1913    1.0  2016-04-24     11613  ...     4   \n",
       "382597     CA_1       CA  d_1913    0.0  2016-04-24     11613  ...     4   \n",
       "382598     CA_1       CA  d_1913    0.0  2016-04-24     11613  ...     4   \n",
       "382599     CA_1       CA  d_1913    0.0  2016-04-24     11613  ...     4   \n",
       "\n",
       "        year  event_name_1  event_type_1 event_name_2 event_type_2 snap_CA  \\\n",
       "0       2011           NaN           NaN          NaN          NaN       0   \n",
       "1       2011           NaN           NaN          NaN          NaN       0   \n",
       "2       2011           NaN           NaN          NaN          NaN       0   \n",
       "3       2011           NaN           NaN          NaN          NaN       0   \n",
       "4       2011           NaN           NaN          NaN          NaN       0   \n",
       "...      ...           ...           ...          ...          ...     ...   \n",
       "382595  2016           NaN           NaN          NaN          NaN       0   \n",
       "382596  2016           NaN           NaN          NaN          NaN       0   \n",
       "382597  2016           NaN           NaN          NaN          NaN       0   \n",
       "382598  2016           NaN           NaN          NaN          NaN       0   \n",
       "382599  2016           NaN           NaN          NaN          NaN       0   \n",
       "\n",
       "       snap_TX  snap_WI  sell_price  \n",
       "0            0        0         NaN  \n",
       "1            0        0         NaN  \n",
       "2            0        0         NaN  \n",
       "3            0        0         NaN  \n",
       "4            0        0         NaN  \n",
       "...        ...      ...         ...  \n",
       "382595       0        0        1.77  \n",
       "382596       0        0       12.98  \n",
       "382597       0        0        9.72  \n",
       "382598       0        0        5.88  \n",
       "382599       0        0        2.72  \n",
       "\n",
       "[382600 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df = merge_df.merge(prices_df,on=['store_id', 'item_id','wm_yr_wk'],how='left')\n",
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b428e553-c303-4c1c-b301-7d37068aa852",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "HOBBIES_1_001_CA_1_validation    1913\n",
       "HOBBIES_1_142_CA_1_validation    1913\n",
       "HOBBIES_1_132_CA_1_validation    1913\n",
       "HOBBIES_1_133_CA_1_validation    1913\n",
       "HOBBIES_1_134_CA_1_validation    1913\n",
       "                                 ... \n",
       "HOBBIES_1_072_CA_1_validation    1913\n",
       "HOBBIES_1_073_CA_1_validation    1913\n",
       "HOBBIES_1_074_CA_1_validation    1913\n",
       "HOBBIES_1_075_CA_1_validation    1913\n",
       "HOBBIES_1_206_CA_1_validation    1913\n",
       "Name: count, Length: 200, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df['id'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c5842e-7d08-4165-90ec-b7edfc62dcc7",
   "metadata": {},
   "source": [
    "# 0 Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "023fbbd9-6c97-41d6-bf76-920c4aee83ce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>dept_id</th>\n",
       "      <th>cat_id</th>\n",
       "      <th>store_id</th>\n",
       "      <th>state_id</th>\n",
       "      <th>d</th>\n",
       "      <th>sales</th>\n",
       "      <th>wm_yr_wk</th>\n",
       "      <th>weekday</th>\n",
       "      <th>...</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>event_name_1</th>\n",
       "      <th>event_type_1</th>\n",
       "      <th>event_name_2</th>\n",
       "      <th>event_type_2</th>\n",
       "      <th>snap_CA</th>\n",
       "      <th>snap_TX</th>\n",
       "      <th>snap_WI</th>\n",
       "      <th>sell_price</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>HOBBIES_1_001_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_001</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>HOBBIES_1_002_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_002</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>HOBBIES_1_003_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_003</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>HOBBIES_1_004_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_004</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2011-01-29</th>\n",
       "      <td>HOBBIES_1_005_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_005</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11101</td>\n",
       "      <td>Saturday</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>2011</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-24</th>\n",
       "      <td>HOBBIES_1_202_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_202</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11613</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-24</th>\n",
       "      <td>HOBBIES_1_203_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_203</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1913</td>\n",
       "      <td>1.0</td>\n",
       "      <td>11613</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>12.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-24</th>\n",
       "      <td>HOBBIES_1_204_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_204</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11613</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-24</th>\n",
       "      <td>HOBBIES_1_205_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_205</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11613</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-04-24</th>\n",
       "      <td>HOBBIES_1_206_CA_1_validation</td>\n",
       "      <td>HOBBIES_1_206</td>\n",
       "      <td>HOBBIES_1</td>\n",
       "      <td>HOBBIES</td>\n",
       "      <td>CA_1</td>\n",
       "      <td>CA</td>\n",
       "      <td>d_1913</td>\n",
       "      <td>0.0</td>\n",
       "      <td>11613</td>\n",
       "      <td>Sunday</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2016</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.72</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>382600 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       id        item_id    dept_id   cat_id  \\\n",
       "date                                                                           \n",
       "2011-01-29  HOBBIES_1_001_CA_1_validation  HOBBIES_1_001  HOBBIES_1  HOBBIES   \n",
       "2011-01-29  HOBBIES_1_002_CA_1_validation  HOBBIES_1_002  HOBBIES_1  HOBBIES   \n",
       "2011-01-29  HOBBIES_1_003_CA_1_validation  HOBBIES_1_003  HOBBIES_1  HOBBIES   \n",
       "2011-01-29  HOBBIES_1_004_CA_1_validation  HOBBIES_1_004  HOBBIES_1  HOBBIES   \n",
       "2011-01-29  HOBBIES_1_005_CA_1_validation  HOBBIES_1_005  HOBBIES_1  HOBBIES   \n",
       "...                                   ...            ...        ...      ...   \n",
       "2016-04-24  HOBBIES_1_202_CA_1_validation  HOBBIES_1_202  HOBBIES_1  HOBBIES   \n",
       "2016-04-24  HOBBIES_1_203_CA_1_validation  HOBBIES_1_203  HOBBIES_1  HOBBIES   \n",
       "2016-04-24  HOBBIES_1_204_CA_1_validation  HOBBIES_1_204  HOBBIES_1  HOBBIES   \n",
       "2016-04-24  HOBBIES_1_205_CA_1_validation  HOBBIES_1_205  HOBBIES_1  HOBBIES   \n",
       "2016-04-24  HOBBIES_1_206_CA_1_validation  HOBBIES_1_206  HOBBIES_1  HOBBIES   \n",
       "\n",
       "           store_id state_id       d  sales  wm_yr_wk   weekday  ...  month  \\\n",
       "date                                                             ...          \n",
       "2011-01-29     CA_1       CA     d_1    0.0     11101  Saturday  ...      1   \n",
       "2011-01-29     CA_1       CA     d_1    0.0     11101  Saturday  ...      1   \n",
       "2011-01-29     CA_1       CA     d_1    0.0     11101  Saturday  ...      1   \n",
       "2011-01-29     CA_1       CA     d_1    0.0     11101  Saturday  ...      1   \n",
       "2011-01-29     CA_1       CA     d_1    0.0     11101  Saturday  ...      1   \n",
       "...             ...      ...     ...    ...       ...       ...  ...    ...   \n",
       "2016-04-24     CA_1       CA  d_1913    0.0     11613    Sunday  ...      4   \n",
       "2016-04-24     CA_1       CA  d_1913    1.0     11613    Sunday  ...      4   \n",
       "2016-04-24     CA_1       CA  d_1913    0.0     11613    Sunday  ...      4   \n",
       "2016-04-24     CA_1       CA  d_1913    0.0     11613    Sunday  ...      4   \n",
       "2016-04-24     CA_1       CA  d_1913    0.0     11613    Sunday  ...      4   \n",
       "\n",
       "            year  event_name_1 event_type_1 event_name_2 event_type_2 snap_CA  \\\n",
       "date                                                                            \n",
       "2011-01-29  2011           NaN          NaN          NaN          NaN       0   \n",
       "2011-01-29  2011           NaN          NaN          NaN          NaN       0   \n",
       "2011-01-29  2011           NaN          NaN          NaN          NaN       0   \n",
       "2011-01-29  2011           NaN          NaN          NaN          NaN       0   \n",
       "2011-01-29  2011           NaN          NaN          NaN          NaN       0   \n",
       "...          ...           ...          ...          ...          ...     ...   \n",
       "2016-04-24  2016           NaN          NaN          NaN          NaN       0   \n",
       "2016-04-24  2016           NaN          NaN          NaN          NaN       0   \n",
       "2016-04-24  2016           NaN          NaN          NaN          NaN       0   \n",
       "2016-04-24  2016           NaN          NaN          NaN          NaN       0   \n",
       "2016-04-24  2016           NaN          NaN          NaN          NaN       0   \n",
       "\n",
       "            snap_TX  snap_WI  sell_price  \n",
       "date                                      \n",
       "2011-01-29        0        0         NaN  \n",
       "2011-01-29        0        0         NaN  \n",
       "2011-01-29        0        0         NaN  \n",
       "2011-01-29        0        0         NaN  \n",
       "2011-01-29        0        0         NaN  \n",
       "...             ...      ...         ...  \n",
       "2016-04-24        0        0        1.77  \n",
       "2016-04-24        0        0       12.98  \n",
       "2016-04-24        0        0        9.72  \n",
       "2016-04-24        0        0        5.88  \n",
       "2016-04-24        0        0        2.72  \n",
       "\n",
       "[382600 rows x 21 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your dataset\n",
    "merge_df['date'] = pd.to_datetime(merge_df['date'])\n",
    "merge_df.set_index('date', inplace=True)\n",
    "\n",
    "merge_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7aba503c-a29f-4f0c-8c1a-8279222635bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id', 'd',\n",
       "       'sales', 'wm_yr_wk', 'weekday', 'wday', 'month', 'year', 'event_name_1',\n",
       "       'event_type_1', 'event_name_2', 'event_type_2', 'snap_CA', 'snap_TX',\n",
       "       'snap_WI', 'sell_price'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b21ccbb8-ef5f-4cd7-aa78-aa216738475d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#FILLING THE EMPTY PLACES\n",
    "merge_df['sell_price'].fillna(0, inplace=True)\n",
    "merge_df['event_name_1'].fillna('missing', inplace=True)\n",
    "merge_df['event_type_1'].fillna('missing', inplace=True)\n",
    "merge_df['event_name_2'].fillna('missing', inplace=True)\n",
    "merge_df['event_type_2'].fillna('missing', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c394d6c-7e1e-4570-bdd5-b03c0828fbd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame with datetime index and \"id\" column\n",
    "start_date = merge_df.index.min()\n",
    "end_date = merge_df.index.max()\n",
    "\n",
    "# Generate the complete date range\n",
    "complete_date_range = pd.date_range(start=start_date, end=end_date)\n",
    "\n",
    "# Get unique values of \"id\" column\n",
    "unique_ids = merge_df['id'].unique()\n",
    "\n",
    "merge_df = merge_df.reset_index()\n",
    "\n",
    "# Create a MultiIndex with Cartesian product of date range and unique ids\n",
    "multi_index = pd.MultiIndex.from_product([complete_date_range, unique_ids], names=['date', 'id'])\n",
    "\n",
    "# Reindex the DataFrame using the MultiIndex\n",
    "merge_df = merge_df.set_index(['date', 'id']).reindex(multi_index)\n",
    "\n",
    "# Reset the index to make \"date\" and \"id\" columns again\n",
    "merge_df = merge_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f5c4156b-8860-413e-9e54-1178d2d03650",
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.set_index(\"date\",inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bd28632-fbba-4622-bf9b-e7cd07cb3e8e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Scale 'sell_price' and 'year' by using MinMaxScaler\n",
    "minmax_scaler = MinMaxScaler()\n",
    "\n",
    "merge_df[['sell_price']] = minmax_scaler.fit_transform(merge_df[['sell_price']])\n",
    "merge_df[['year']] = minmax_scaler.fit_transform(merge_df[['year']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d6e1682-e512-4ea7-aefc-97d9fee2a919",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique values for 'cat_id' are ['HOBBIES']\n",
      "The unique values for 'store_id' are ['CA_1']\n",
      "The categories detected by the OneHotEncoder are [array(['HOBBIES'], dtype=object), array(['CA_1'], dtype=object)]\n",
      "The column names for the encoded values are ['cat_id_HOBBIES' 'store_id_CA_1']\n"
     ]
    }
   ],
   "source": [
    "# Check unique values for 'cat_id'\n",
    "print(f\"The unique values for 'cat_id' are {merge_df['cat_id'].unique()}\")\n",
    "\n",
    "# Check unique values for 'store_id'\n",
    "print(f\"The unique values for 'store_id' are {merge_df['store_id'].unique()}\")\n",
    "\n",
    "# Instantiate the OneHotEncoder\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit encoder for both 'cat_id' and 'store_id'\n",
    "ohe.fit(merge_df[['cat_id', 'store_id']])\n",
    "\n",
    "# Display the detected categories for both columns\n",
    "print(f\"The categories detected by the OneHotEncoder are {ohe.categories_}\")\n",
    "\n",
    "# Display the generated names for both columns\n",
    "print(f\"The column names for the encoded values are {ohe.get_feature_names_out()}\")\n",
    "\n",
    "# Transform the 'cat_id' and 'store_id' columns\n",
    "encoded_columns = ohe.transform(merge_df[['cat_id', 'store_id']])\n",
    "\n",
    "# Drop the original 'cat_id' and 'store_id' columns\n",
    "merge_df.drop(columns=['cat_id', 'store_id'], inplace=True)\n",
    "\n",
    "# Concatenate the encoded columns to the DataFrame\n",
    "merge_df[ ohe.get_feature_names_out()] = encoded_columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8ad5349-0e57-4e5b-a1a1-60e799c25eda",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique values for 'event_type_1' are ['missing' 'Sporting' 'Cultural' 'National' 'Religious']\n",
      "The categories detected by the OneHotEncoder are [array(['Cultural', 'National', 'Religious', 'Sporting', 'missing'],\n",
      "      dtype=object)]\n",
      "The column names for the encoded values are ['event_type_1_Cultural' 'event_type_1_National' 'event_type_1_Religious'\n",
      " 'event_type_1_Sporting' 'event_type_1_missing']\n"
     ]
    }
   ],
   "source": [
    "# Check unique values\n",
    "print(f\"The unique values for 'event_type_1' are {merge_df['event_type_1'].unique()}\")\n",
    "\n",
    "# Fit encoder\n",
    "ohe.fit(merge_df[['event_type_1']])\n",
    "\n",
    "# Display the detected categories\n",
    "print(f\"The categories detected by the OneHotEncoder are {ohe.categories_}\")\n",
    "\n",
    "# Display the generated names\n",
    "print(f\"The column names for the encoded values are {ohe.get_feature_names_out()}\")\n",
    "\n",
    "# Transform the current \"cat_id\" column\n",
    "merge_df[ohe.get_feature_names_out()] = ohe.transform(merge_df[['event_type_1']])\n",
    "\n",
    "# Drop the column \"cat_id\" which has been encoded\n",
    "merge_df.drop(columns = ['event_type_1'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adefde7a-880f-49e2-ac3d-05570dce8685",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique values for 'event_type_2' are ['missing' 'Cultural' 'Religious']\n",
      "The categories detected by the OneHotEncoder are [array(['Cultural', 'Religious', 'missing'], dtype=object)]\n",
      "The column names for the encoded values are ['event_type_2_Cultural' 'event_type_2_Religious' 'event_type_2_missing']\n"
     ]
    }
   ],
   "source": [
    "# Check unique values\n",
    "print(f\"The unique values for 'event_type_2' are {merge_df['event_type_2'].unique()}\")\n",
    "\n",
    "# Fit encoder\n",
    "ohe.fit(merge_df[['event_type_2']])\n",
    "\n",
    "# Display the detected categories\n",
    "print(f\"The categories detected by the OneHotEncoder are {ohe.categories_}\")\n",
    "\n",
    "# Display the generated names\n",
    "print(f\"The column names for the encoded values are {ohe.get_feature_names_out()}\")\n",
    "\n",
    "# Transform the current \"cat_id\" column\n",
    "merge_df[ohe.get_feature_names_out()] = ohe.transform(merge_df[['event_type_2']])\n",
    "\n",
    "# Drop the column \"cat_id\" which has been encoded\n",
    "merge_df.drop(columns = ['event_type_2'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e24abdb8-3be4-4608-91b0-ef0331e80133",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique values for 'event_name_1' are ['missing' 'SuperBowl' 'ValentinesDay' 'PresidentsDay' 'LentStart'\n",
      " 'LentWeek2' 'StPatricksDay' 'Purim End' 'OrthodoxEaster' 'Pesach End'\n",
      " 'Cinco De Mayo' \"Mother's day\" 'MemorialDay' 'NBAFinalsStart'\n",
      " 'NBAFinalsEnd' \"Father's day\" 'IndependenceDay' 'Ramadan starts'\n",
      " 'Eid al-Fitr' 'LaborDay' 'ColumbusDay' 'Halloween' 'EidAlAdha'\n",
      " 'VeteransDay' 'Thanksgiving' 'Christmas' 'Chanukah End' 'NewYear'\n",
      " 'OrthodoxChristmas' 'MartinLutherKingDay' 'Easter']\n",
      "The categories detected by the OneHotEncoder are [array(['Chanukah End', 'Christmas', 'Cinco De Mayo', 'ColumbusDay',\n",
      "       'Easter', 'Eid al-Fitr', 'EidAlAdha', \"Father's day\", 'Halloween',\n",
      "       'IndependenceDay', 'LaborDay', 'LentStart', 'LentWeek2',\n",
      "       'MartinLutherKingDay', 'MemorialDay', \"Mother's day\",\n",
      "       'NBAFinalsEnd', 'NBAFinalsStart', 'NewYear', 'OrthodoxChristmas',\n",
      "       'OrthodoxEaster', 'Pesach End', 'PresidentsDay', 'Purim End',\n",
      "       'Ramadan starts', 'StPatricksDay', 'SuperBowl', 'Thanksgiving',\n",
      "       'ValentinesDay', 'VeteransDay', 'missing'], dtype=object)]\n",
      "The column names for the encoded values are ['event_name_1_Chanukah End' 'event_name_1_Christmas'\n",
      " 'event_name_1_Cinco De Mayo' 'event_name_1_ColumbusDay'\n",
      " 'event_name_1_Easter' 'event_name_1_Eid al-Fitr' 'event_name_1_EidAlAdha'\n",
      " \"event_name_1_Father's day\" 'event_name_1_Halloween'\n",
      " 'event_name_1_IndependenceDay' 'event_name_1_LaborDay'\n",
      " 'event_name_1_LentStart' 'event_name_1_LentWeek2'\n",
      " 'event_name_1_MartinLutherKingDay' 'event_name_1_MemorialDay'\n",
      " \"event_name_1_Mother's day\" 'event_name_1_NBAFinalsEnd'\n",
      " 'event_name_1_NBAFinalsStart' 'event_name_1_NewYear'\n",
      " 'event_name_1_OrthodoxChristmas' 'event_name_1_OrthodoxEaster'\n",
      " 'event_name_1_Pesach End' 'event_name_1_PresidentsDay'\n",
      " 'event_name_1_Purim End' 'event_name_1_Ramadan starts'\n",
      " 'event_name_1_StPatricksDay' 'event_name_1_SuperBowl'\n",
      " 'event_name_1_Thanksgiving' 'event_name_1_ValentinesDay'\n",
      " 'event_name_1_VeteransDay' 'event_name_1_missing']\n"
     ]
    }
   ],
   "source": [
    "# Check unique values\n",
    "print(f\"The unique values for 'event_name_1' are {merge_df['event_name_1'].unique()}\")\n",
    "\n",
    "# Fit encoder\n",
    "ohe.fit(merge_df[['event_name_1']])\n",
    "\n",
    "# Display the detected categories\n",
    "print(f\"The categories detected by the OneHotEncoder are {ohe.categories_}\")\n",
    "\n",
    "# Display the generated names\n",
    "print(f\"The column names for the encoded values are {ohe.get_feature_names_out()}\")\n",
    "\n",
    "# Transform the current \"cat_id\" column\n",
    "merge_df[ohe.get_feature_names_out()] = ohe.transform(merge_df[['event_name_1']])\n",
    "\n",
    "# Drop the column \"cat_id\" which has been encoded\n",
    "merge_df.drop(columns = ['event_name_1'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "63c1b1c5-3631-4720-bfd9-92f4f381c73a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique values for 'event_name_2' are ['missing' 'Easter' 'Cinco De Mayo' 'OrthodoxEaster' \"Father's day\"]\n",
      "The categories detected by the OneHotEncoder are [array(['Cinco De Mayo', 'Easter', \"Father's day\", 'OrthodoxEaster',\n",
      "       'missing'], dtype=object)]\n",
      "The column names for the encoded values are ['event_name_2_Cinco De Mayo' 'event_name_2_Easter'\n",
      " \"event_name_2_Father's day\" 'event_name_2_OrthodoxEaster'\n",
      " 'event_name_2_missing']\n"
     ]
    }
   ],
   "source": [
    "# Check unique values\n",
    "print(f\"The unique values for 'event_name_2' are {merge_df['event_name_2'].unique()}\")\n",
    "\n",
    "# Fit encoder\n",
    "ohe.fit(merge_df[['event_name_2']])\n",
    "\n",
    "# Display the detected categories\n",
    "print(f\"The categories detected by the OneHotEncoder are {ohe.categories_}\")\n",
    "\n",
    "# Display the generated names\n",
    "print(f\"The column names for the encoded values are {ohe.get_feature_names_out()}\")\n",
    "\n",
    "# Transform the current \"cat_id\" column\n",
    "merge_df[ohe.get_feature_names_out()] = ohe.transform(merge_df[['event_name_2']])\n",
    "\n",
    "# Drop the column \"cat_id\" which has been encoded\n",
    "merge_df.drop(columns = ['event_name_2'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9e7ea5ba-4d85-483b-a890-f5d148cddf08",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Encoding Cyclical Features for weekdays\n",
    "# Notice that Sat starts as 1 till Fri as 7 for 'wday'\n",
    "merge_df['wday_sin'] = np.sin(2 * np.pi * merge_df['wday'] /7.0)\n",
    "merge_df['wday_cos'] = np.cos(2 * np.pi * merge_df['wday'] /7.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "922ddcea-9ad4-47ea-8157-2498fde75a0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Encoding Cyclical Features for month\n",
    "\n",
    "merge_df['month_sin'] = np.sin(2 * np.pi * merge_df['month'] /12.0)\n",
    "merge_df['month_cos'] = np.cos(2 * np.pi * merge_df['month'] /12.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "91872056-89ef-4a5b-bc70-bc2f1ebf0526",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_df_scaled = merge_df.drop(columns=['d', 'wm_yr_wk','weekday'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "250e4c27-5980-4e8f-9714-66905ab51ad7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dtype('O') dtype('float32') dtype('int8')]\n"
     ]
    }
   ],
   "source": [
    "# Downcast numeric columns\n",
    "numeric_columns = merge_df_scaled.select_dtypes(include=['int64', 'float64']).columns\n",
    "merge_df_scaled[numeric_columns] = merge_df_scaled[numeric_columns].apply(lambda x: pd.to_numeric(x, downcast='integer' if np.issubdtype(x.dtype, np.integer) else 'float'))\n",
    "\n",
    "# Confirm the new datatypes\n",
    "print(merge_df_scaled.dtypes.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "93811f0b-32c4-4beb-b498-127548d1ddf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_extraction(merge_df_scaled):\n",
    "    # Ignore all warnings within this function\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "        #lagged features\n",
    "        for i in range(1, 8):\n",
    "            merge_df_scaled[f'sales_lag_{i}'] = merge_df_scaled['sales'].shift(i)\n",
    "    \n",
    "        #lagged features per years\n",
    "        for i in range(1, 4):\n",
    "            merge_df_scaled[f'sales_lag_{i}years'] = merge_df_scaled['sales'].shift(i * 365)\n",
    "    \n",
    "            #rolling sum\n",
    "        merge_df_scaled['rolling_sum_7'] = merge_df_scaled['sales'].rolling(window=7).sum()\n",
    "        merge_df_scaled['rolling_sum_30'] = merge_df_scaled['sales'].rolling(window=30).sum()\n",
    "        merge_df_scaled['rolling_sum_60'] = merge_df_scaled['sales'].rolling(window=60).sum()\n",
    "        merge_df_scaled['rolling_sum_90'] = merge_df_scaled['sales'].rolling(window=90).sum()\n",
    "        merge_df_scaled['rolling_sum_120'] = merge_df_scaled['sales'].rolling(window=120).sum()\n",
    "    \n",
    "        #rolling average\n",
    "        merge_df_scaled['rolling_mean_7'] = merge_df_scaled['sales'].rolling(window=7).mean()\n",
    "        merge_df_scaled['rolling_mean_30'] = merge_df_scaled['sales'].rolling(window=30).mean()\n",
    "        merge_df_scaled['rolling_mean_60'] = merge_df_scaled['sales'].rolling(window=60).mean()\n",
    "        merge_df_scaled['rolling_mean_90'] = merge_df_scaled['sales'].rolling(window=90).mean()\n",
    "        merge_df_scaled['rolling_mean_120'] = merge_df_scaled['sales'].rolling(window=120).mean()\n",
    "    \n",
    "        #rolling stdv\n",
    "        merge_df_scaled['rolling_stdv_7'] = merge_df_scaled['sales'].rolling(window=7).std()\n",
    "        merge_df_scaled['rolling_stdv_30'] = merge_df_scaled['sales'].rolling(window=30).std()\n",
    "        merge_df_scaled['rolling_stdv_60'] = merge_df_scaled['sales'].rolling(window=60).std()\n",
    "        merge_df_scaled['rolling_stdv_90'] = merge_df_scaled['sales'].rolling(window=90).std()\n",
    "        merge_df_scaled['rolling_stdv_120'] = merge_df_scaled['sales'].rolling(window=120).std()\n",
    "    \n",
    "        merge_df_scaled.fillna(0,inplace=True)\n",
    "        \n",
    "\n",
    "    return merge_df_scaled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b02df851-703f-412f-8f52-cf105070d070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def feature_extraction_transfer_test(train_df,test_df):\n",
    "\n",
    "    # Ignore all warnings within this function\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "           \n",
    "    \n",
    "        #lagged features per years\n",
    "        for i in range(1, 4):\n",
    "            test_df[f'sales_lag_{i}years'] = train_df[f'sales_lag_{i}years'].iloc[-1]\n",
    "    \n",
    "            #rolling sum\n",
    "        test_df['rolling_sum_7'] = train_df['rolling_sum_7'].iloc[-1]\n",
    "        test_df['rolling_sum_30'] = train_df['rolling_sum_30'].iloc[-1]\n",
    "        test_df['rolling_sum_60'] = train_df['rolling_sum_60'].iloc[-1]\n",
    "        test_df['rolling_sum_90'] = train_df['rolling_sum_90'].iloc[-1]\n",
    "        test_df['rolling_sum_120'] = train_df['rolling_sum_120'].iloc[-1]\n",
    "        \n",
    "        # Rolling average\n",
    "        test_df['rolling_mean_7'] = train_df['rolling_mean_7'].iloc[-1]\n",
    "        test_df['rolling_mean_30'] = train_df['rolling_mean_30'].iloc[-1]\n",
    "        test_df['rolling_mean_60'] = train_df['rolling_mean_60'].iloc[-1]\n",
    "        test_df['rolling_mean_90'] = train_df['rolling_mean_90'].iloc[-1]\n",
    "        test_df['rolling_mean_120'] = train_df['rolling_mean_120'].iloc[-1]\n",
    "        \n",
    "        # Rolling standard deviation\n",
    "        test_df['rolling_stdv_7'] = train_df['rolling_stdv_7'].iloc[-1]\n",
    "        test_df['rolling_stdv_30'] = train_df['rolling_stdv_30'].iloc[-1]\n",
    "        test_df['rolling_stdv_60'] = train_df['rolling_stdv_60'].iloc[-1]\n",
    "        test_df['rolling_stdv_90'] = train_df['rolling_stdv_90'].iloc[-1]\n",
    "        test_df['rolling_stdv_120'] = train_df['rolling_stdv_120'].iloc[-1]\n",
    "    \n",
    "        # Identify the last available date in the training data\n",
    "        last_date_train = train_df.index[-1]\n",
    "    \n",
    "        # Fill in lagged features for the first few rows where future knowledge is available\n",
    "        #for i in range(1, 8):\n",
    "        #    # Identify the lagged date for the current lag\n",
    "        #    lagged_date = last_date_train - pd.Timedelta(days=i)\n",
    "            \n",
    "            # Fill in the lagged sales values for corresponding lagged days from the training data\n",
    "        #    test_df[f'sales_lag_{i}'] = test_df.index.map(lambda x: train_df.loc[x - pd.Timedelta(days=i), 'sales'] if x <= last_date_train else train_df[f'sales_lag_{i}'].iloc[-1])\n",
    "\n",
    "\n",
    "        # Fill in lagged features for the first few rows where future knowledge is available\n",
    "        for i in range(1, 8):\n",
    "            test_df[f'sales_lag_{i}'] = np.nan  # Initialize with NaN\n",
    "            \n",
    "            # Iterate over each row in the test DataFrame\n",
    "            for idx, row in test_df.iterrows():\n",
    "                lagged_date = idx - pd.Timedelta(days=i)  # Calculate the lagged date\n",
    "                \n",
    "                # Check if the lagged date is within the training data range\n",
    "                if lagged_date in train_df.index:\n",
    "                    test_df.at[idx, f'sales_lag_{i}'] = train_df.loc[lagged_date, 'sales']\n",
    "                else:\n",
    "                    test_df.at[idx, f'sales_lag_{i}'] = train_df['sales'].iloc[-1]\n",
    "    \n",
    "    return test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "013dda06-1b86-4216-a9f8-366e5e27f7fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calc_rmsse(train, test, predictions):\n",
    "    forecast_mse = mean_squared_error(test, predictions)\n",
    "    train_mse = ((train - train.shift(1)) ** 2).mean()\n",
    "    return np.sqrt(forecast_mse / train_mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e0fca26-43dc-4b21-b08e-65bd9d58aa40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculate_last_28_days_weights(df):\n",
    "    most_recent_date = df.index.max()\n",
    "    cutoff_date = most_recent_date - timedelta(days=27)\n",
    "    last_28_days_df = df.loc[cutoff_date:most_recent_date]\n",
    "    last_28_days_df['revenues'] = last_28_days_df['sales'] * last_28_days_df['sell_price']\n",
    "    \n",
    "    weights_df = last_28_days_df.groupby('id')['revenues'].sum()\n",
    "    weights_df = pd.DataFrame(weights_df)\n",
    "    \n",
    "    total_revenues = last_28_days_df['revenues'].sum()\n",
    "    weights_df['weights'] = weights_df['revenues'] / total_revenues\n",
    "    \n",
    "    return weights_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3bb6cf5-a720-428d-9f87-b60534928ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_ttsplit(product_data,num_splits=5, days_per_split=28):\n",
    "\n",
    "    # Check if enough data is available\n",
    "    if len(product_data) < days_per_split * num_splits:\n",
    "        print(f\"Not enough data for product {id} to perform {num_splits} splits with {days_per_split} days each.\")\n",
    "        \n",
    "    \n",
    "    # Initialize time_splits as a list of empty lists\n",
    "    time_splits = [[] for _ in range(num_splits)]\n",
    "    \n",
    "    # Create data slices for 5-fold validation with each test slice being exactly 28 days\n",
    "    for i in range(1, num_splits + 1):\n",
    "        data_train = product_data.iloc[:-days_per_split * i]\n",
    "        data_test = product_data.iloc[-days_per_split * i: (-days_per_split * (i - 1) if i > 1 else None)]   \n",
    "\n",
    "        time_splits[i-1].append(data_train)\n",
    "        time_splits[i-1].append(data_test)\n",
    "        \n",
    "        \n",
    "    return time_splits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "88094689-eeab-4710-8000-4f701df35081",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_ttsplit_onlyfeatures(product_data,num_splits=5, days_per_split=28):\n",
    "\n",
    "    # Check if enough data is available\n",
    "    if len(product_data) < days_per_split * 2 * num_splits:\n",
    "        print(f\"Not enough data for product {id} to perform {num_splits} splits with {days_per_split} days each.\")\n",
    "        \n",
    "    \n",
    "    # Initialize time_splits as a list of empty lists\n",
    "    time_splits = [[] for _ in range(num_splits)]\n",
    "    \n",
    "    # Create data slices for 5-fold validation with each test slice being exactly 28 days\n",
    "    for i in range(1, num_splits + 1):\n",
    "        data_train = product_data.iloc[:-days_per_split * i]\n",
    "        data_test = product_data.iloc[-days_per_split * i: (-days_per_split * (i - 1) if i > 1 else None)]   \n",
    "\n",
    "        data_train = feature_extraction(data_train)\n",
    "        data_test = feature_extraction_transfer_test(data_train,data_test)\n",
    "\n",
    "        data_test = data_test[data_train.columns]\n",
    "\n",
    "        time_splits[i-1].append(data_train)\n",
    "        time_splits[i-1].append(data_test)\n",
    "        \n",
    "        \n",
    "    return time_splits\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e1b50d66-f364-4758-a13c-8c770e34777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_ttsplit_inclval(product_data,num_splits=5, days_per_split=28):\n",
    "\n",
    "    # Check if enough data is available\n",
    "    if len(product_data) < days_per_split * 2 * num_splits:\n",
    "        print(f\"Not enough data for product {id} to perform {num_splits} splits with {days_per_split} days each.\")\n",
    "        \n",
    "    \n",
    "    # Initialize time_splits as a list of empty lists\n",
    "    time_splits = [[] for _ in range(num_splits)]\n",
    "    \n",
    "    # Create data slices for 5-fold validation with each test slice being exactly 28 days\n",
    "    for i in range(1, num_splits + 1):\n",
    "        data_train_incl_val = product_data.iloc[:-days_per_split * i]\n",
    "        data_test = product_data.iloc[-days_per_split * i: (-days_per_split * (i - 1) if i > 1 else None)]   \n",
    "\n",
    "        data_train_incl_val = feature_extraction(data_train_incl_val)\n",
    "        data_test = feature_extraction_transfer_test(data_train_incl_val,data_test)\n",
    "        data_train = data_train_incl_val[:-days_per_split]\n",
    "        data_val = data_train_incl_val[-days_per_split:]\n",
    "\n",
    "        time_splits[i-1].append(data_train)\n",
    "        time_splits[i-1].append(data_val)\n",
    "        time_splits[i-1].append(data_test)\n",
    "        \n",
    "        \n",
    "    return time_splits\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5d1e8448-ecde-4944-99b4-30f74e5f489a",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_df_scaled.drop(columns=[\"item_id\",\"dept_id\",\"state_id\"],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9391eb6-da51-4b69-a4aa-5fd1ccba9db6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "merge_df_scaled.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "13a35134-1f8a-436c-b168-111341c17cdf",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['HOBBIES_1_001_CA_1_validation', 'HOBBIES_1_002_CA_1_validation',\n",
       "       'HOBBIES_1_003_CA_1_validation', 'HOBBIES_1_004_CA_1_validation',\n",
       "       'HOBBIES_1_005_CA_1_validation', 'HOBBIES_1_006_CA_1_validation',\n",
       "       'HOBBIES_1_007_CA_1_validation', 'HOBBIES_1_008_CA_1_validation',\n",
       "       'HOBBIES_1_009_CA_1_validation', 'HOBBIES_1_010_CA_1_validation',\n",
       "       'HOBBIES_1_011_CA_1_validation', 'HOBBIES_1_012_CA_1_validation',\n",
       "       'HOBBIES_1_013_CA_1_validation', 'HOBBIES_1_014_CA_1_validation',\n",
       "       'HOBBIES_1_015_CA_1_validation', 'HOBBIES_1_016_CA_1_validation',\n",
       "       'HOBBIES_1_017_CA_1_validation', 'HOBBIES_1_018_CA_1_validation',\n",
       "       'HOBBIES_1_019_CA_1_validation', 'HOBBIES_1_020_CA_1_validation',\n",
       "       'HOBBIES_1_021_CA_1_validation', 'HOBBIES_1_022_CA_1_validation',\n",
       "       'HOBBIES_1_023_CA_1_validation', 'HOBBIES_1_024_CA_1_validation',\n",
       "       'HOBBIES_1_025_CA_1_validation', 'HOBBIES_1_026_CA_1_validation',\n",
       "       'HOBBIES_1_027_CA_1_validation', 'HOBBIES_1_028_CA_1_validation',\n",
       "       'HOBBIES_1_029_CA_1_validation', 'HOBBIES_1_030_CA_1_validation',\n",
       "       'HOBBIES_1_031_CA_1_validation', 'HOBBIES_1_032_CA_1_validation',\n",
       "       'HOBBIES_1_033_CA_1_validation', 'HOBBIES_1_034_CA_1_validation',\n",
       "       'HOBBIES_1_035_CA_1_validation', 'HOBBIES_1_036_CA_1_validation',\n",
       "       'HOBBIES_1_037_CA_1_validation', 'HOBBIES_1_038_CA_1_validation',\n",
       "       'HOBBIES_1_039_CA_1_validation', 'HOBBIES_1_040_CA_1_validation',\n",
       "       'HOBBIES_1_041_CA_1_validation', 'HOBBIES_1_042_CA_1_validation',\n",
       "       'HOBBIES_1_043_CA_1_validation', 'HOBBIES_1_044_CA_1_validation',\n",
       "       'HOBBIES_1_045_CA_1_validation', 'HOBBIES_1_046_CA_1_validation',\n",
       "       'HOBBIES_1_047_CA_1_validation', 'HOBBIES_1_048_CA_1_validation',\n",
       "       'HOBBIES_1_049_CA_1_validation', 'HOBBIES_1_050_CA_1_validation',\n",
       "       'HOBBIES_1_051_CA_1_validation', 'HOBBIES_1_052_CA_1_validation',\n",
       "       'HOBBIES_1_053_CA_1_validation', 'HOBBIES_1_054_CA_1_validation',\n",
       "       'HOBBIES_1_055_CA_1_validation', 'HOBBIES_1_056_CA_1_validation',\n",
       "       'HOBBIES_1_057_CA_1_validation', 'HOBBIES_1_058_CA_1_validation',\n",
       "       'HOBBIES_1_060_CA_1_validation', 'HOBBIES_1_061_CA_1_validation',\n",
       "       'HOBBIES_1_062_CA_1_validation', 'HOBBIES_1_063_CA_1_validation',\n",
       "       'HOBBIES_1_064_CA_1_validation', 'HOBBIES_1_065_CA_1_validation',\n",
       "       'HOBBIES_1_066_CA_1_validation', 'HOBBIES_1_067_CA_1_validation',\n",
       "       'HOBBIES_1_068_CA_1_validation', 'HOBBIES_1_069_CA_1_validation',\n",
       "       'HOBBIES_1_070_CA_1_validation', 'HOBBIES_1_072_CA_1_validation',\n",
       "       'HOBBIES_1_073_CA_1_validation', 'HOBBIES_1_074_CA_1_validation',\n",
       "       'HOBBIES_1_075_CA_1_validation', 'HOBBIES_1_076_CA_1_validation',\n",
       "       'HOBBIES_1_077_CA_1_validation', 'HOBBIES_1_078_CA_1_validation',\n",
       "       'HOBBIES_1_079_CA_1_validation', 'HOBBIES_1_080_CA_1_validation',\n",
       "       'HOBBIES_1_081_CA_1_validation', 'HOBBIES_1_082_CA_1_validation',\n",
       "       'HOBBIES_1_083_CA_1_validation', 'HOBBIES_1_084_CA_1_validation',\n",
       "       'HOBBIES_1_085_CA_1_validation', 'HOBBIES_1_086_CA_1_validation',\n",
       "       'HOBBIES_1_087_CA_1_validation', 'HOBBIES_1_088_CA_1_validation',\n",
       "       'HOBBIES_1_089_CA_1_validation', 'HOBBIES_1_090_CA_1_validation',\n",
       "       'HOBBIES_1_091_CA_1_validation', 'HOBBIES_1_092_CA_1_validation',\n",
       "       'HOBBIES_1_093_CA_1_validation', 'HOBBIES_1_094_CA_1_validation',\n",
       "       'HOBBIES_1_095_CA_1_validation', 'HOBBIES_1_097_CA_1_validation',\n",
       "       'HOBBIES_1_098_CA_1_validation', 'HOBBIES_1_099_CA_1_validation',\n",
       "       'HOBBIES_1_100_CA_1_validation', 'HOBBIES_1_102_CA_1_validation',\n",
       "       'HOBBIES_1_103_CA_1_validation', 'HOBBIES_1_104_CA_1_validation',\n",
       "       'HOBBIES_1_105_CA_1_validation', 'HOBBIES_1_106_CA_1_validation',\n",
       "       'HOBBIES_1_107_CA_1_validation', 'HOBBIES_1_108_CA_1_validation',\n",
       "       'HOBBIES_1_109_CA_1_validation', 'HOBBIES_1_110_CA_1_validation',\n",
       "       'HOBBIES_1_111_CA_1_validation', 'HOBBIES_1_112_CA_1_validation',\n",
       "       'HOBBIES_1_113_CA_1_validation', 'HOBBIES_1_114_CA_1_validation',\n",
       "       'HOBBIES_1_115_CA_1_validation', 'HOBBIES_1_116_CA_1_validation',\n",
       "       'HOBBIES_1_117_CA_1_validation', 'HOBBIES_1_118_CA_1_validation',\n",
       "       'HOBBIES_1_119_CA_1_validation', 'HOBBIES_1_120_CA_1_validation',\n",
       "       'HOBBIES_1_121_CA_1_validation', 'HOBBIES_1_122_CA_1_validation',\n",
       "       'HOBBIES_1_123_CA_1_validation', 'HOBBIES_1_124_CA_1_validation',\n",
       "       'HOBBIES_1_125_CA_1_validation', 'HOBBIES_1_126_CA_1_validation',\n",
       "       'HOBBIES_1_127_CA_1_validation', 'HOBBIES_1_128_CA_1_validation',\n",
       "       'HOBBIES_1_129_CA_1_validation', 'HOBBIES_1_130_CA_1_validation',\n",
       "       'HOBBIES_1_131_CA_1_validation', 'HOBBIES_1_132_CA_1_validation',\n",
       "       'HOBBIES_1_133_CA_1_validation', 'HOBBIES_1_134_CA_1_validation',\n",
       "       'HOBBIES_1_135_CA_1_validation', 'HOBBIES_1_136_CA_1_validation',\n",
       "       'HOBBIES_1_137_CA_1_validation', 'HOBBIES_1_138_CA_1_validation',\n",
       "       'HOBBIES_1_139_CA_1_validation', 'HOBBIES_1_140_CA_1_validation',\n",
       "       'HOBBIES_1_141_CA_1_validation', 'HOBBIES_1_142_CA_1_validation',\n",
       "       'HOBBIES_1_143_CA_1_validation', 'HOBBIES_1_144_CA_1_validation',\n",
       "       'HOBBIES_1_145_CA_1_validation', 'HOBBIES_1_146_CA_1_validation',\n",
       "       'HOBBIES_1_147_CA_1_validation', 'HOBBIES_1_148_CA_1_validation',\n",
       "       'HOBBIES_1_149_CA_1_validation', 'HOBBIES_1_150_CA_1_validation',\n",
       "       'HOBBIES_1_151_CA_1_validation', 'HOBBIES_1_152_CA_1_validation',\n",
       "       'HOBBIES_1_153_CA_1_validation', 'HOBBIES_1_154_CA_1_validation',\n",
       "       'HOBBIES_1_155_CA_1_validation', 'HOBBIES_1_156_CA_1_validation',\n",
       "       'HOBBIES_1_157_CA_1_validation', 'HOBBIES_1_158_CA_1_validation',\n",
       "       'HOBBIES_1_159_CA_1_validation', 'HOBBIES_1_160_CA_1_validation',\n",
       "       'HOBBIES_1_161_CA_1_validation', 'HOBBIES_1_162_CA_1_validation',\n",
       "       'HOBBIES_1_163_CA_1_validation', 'HOBBIES_1_164_CA_1_validation',\n",
       "       'HOBBIES_1_165_CA_1_validation', 'HOBBIES_1_166_CA_1_validation',\n",
       "       'HOBBIES_1_167_CA_1_validation', 'HOBBIES_1_168_CA_1_validation',\n",
       "       'HOBBIES_1_169_CA_1_validation', 'HOBBIES_1_170_CA_1_validation',\n",
       "       'HOBBIES_1_171_CA_1_validation', 'HOBBIES_1_172_CA_1_validation',\n",
       "       'HOBBIES_1_173_CA_1_validation', 'HOBBIES_1_174_CA_1_validation',\n",
       "       'HOBBIES_1_175_CA_1_validation', 'HOBBIES_1_176_CA_1_validation',\n",
       "       'HOBBIES_1_177_CA_1_validation', 'HOBBIES_1_178_CA_1_validation',\n",
       "       'HOBBIES_1_179_CA_1_validation', 'HOBBIES_1_180_CA_1_validation',\n",
       "       'HOBBIES_1_181_CA_1_validation', 'HOBBIES_1_183_CA_1_validation',\n",
       "       'HOBBIES_1_184_CA_1_validation', 'HOBBIES_1_185_CA_1_validation',\n",
       "       'HOBBIES_1_186_CA_1_validation', 'HOBBIES_1_187_CA_1_validation',\n",
       "       'HOBBIES_1_188_CA_1_validation', 'HOBBIES_1_189_CA_1_validation',\n",
       "       'HOBBIES_1_190_CA_1_validation', 'HOBBIES_1_191_CA_1_validation',\n",
       "       'HOBBIES_1_192_CA_1_validation', 'HOBBIES_1_193_CA_1_validation',\n",
       "       'HOBBIES_1_194_CA_1_validation', 'HOBBIES_1_195_CA_1_validation',\n",
       "       'HOBBIES_1_197_CA_1_validation', 'HOBBIES_1_198_CA_1_validation',\n",
       "       'HOBBIES_1_199_CA_1_validation', 'HOBBIES_1_200_CA_1_validation',\n",
       "       'HOBBIES_1_201_CA_1_validation', 'HOBBIES_1_202_CA_1_validation',\n",
       "       'HOBBIES_1_203_CA_1_validation', 'HOBBIES_1_204_CA_1_validation',\n",
       "       'HOBBIES_1_205_CA_1_validation', 'HOBBIES_1_206_CA_1_validation'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df_scaled[\"id\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "199ce411-79fa-4af8-a0cb-0bb69d6e4d5e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_df_scaled['sales'].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2c90d7-181f-4c34-b9db-73d5632292e5",
   "metadata": {},
   "source": [
    "# 1. Defining Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "44c2e333-1676-4450-a8ac-b1d64ce1fee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_prophet(data_train, data_test):\n",
    "\n",
    "    temp_rmsse = []\n",
    "\n",
    "    data_train.reset_index(inplace=True,names=\"date\")\n",
    "    data_test.reset_index(inplace=True,names=\"date\")\n",
    "    \n",
    "    prophet_data_train = data_train[[\"id\",\"date\",\"sales\"]]\n",
    "    prophet_data_test = data_test[[\"id\",\"date\",\"sales\"]]\n",
    "    prophet_data_train.columns = [\"id\",\"ds\",\"y\"]\n",
    "    prophet_data_test.columns = [\"id\",\"ds\",\"y\"]\n",
    "    prophet_data_train['ds'] = pd.to_datetime(prophet_data_train['ds'])\n",
    "    prophet_data_test['ds'] = pd.to_datetime(prophet_data_test['ds'])\n",
    "    \n",
    "    X_train = prophet_data_train[\"ds\"]\n",
    "    y_train = prophet_data_train[\"y\"]\n",
    "    X_test = prophet_data_test[\"ds\"]\n",
    "    y_test = prophet_data_test[\"y\"]\n",
    "    \n",
    "    fbp = Prophet()\n",
    "\n",
    "    model = fbp.fit(prophet_data_train)\n",
    "    \n",
    "    predict_placeholder = fbp.make_future_dataframe(28,freq=\"D\")\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = fbp.predict(predict_placeholder[-28:])\n",
    "    \n",
    "\n",
    "    # Calculate and return the error metric for the current fold\n",
    "    rmsse = calc_rmsse(y_train, y_test, y_pred[\"yhat\"])\n",
    "    \n",
    "    return model, rmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40e117ca-6a4c-42d0-9a8d-ae22cec86ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform Random Forest modeling and calculate MAE\n",
    "def perform_random_forest(data_train, data_test):\n",
    "        \n",
    "    X_train = data_train.drop(columns=\"sales\")\n",
    "    y_train = data_train[\"sales\"]\n",
    "    X_test = data_test.drop(columns=\"sales\")\n",
    "    y_test = data_test[\"sales\"]\n",
    "\n",
    "    # Fit Random Forest model on the training data\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    predictions = model.predict(X_test)\n",
    "\n",
    "    # Calculate wRMSSE\n",
    "    rmsse = calc_rmsse(y_train,y_test, predictions)\n",
    "    \n",
    "    return model, rmsse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "151bd3d5-e70c-41ef-8a3e-2da51e4fe900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_auto_arima(data_train,data_test):\n",
    "    \n",
    "\n",
    "    y_train = data_train[\"sales\"]\n",
    "    y_test = data_test[\"sales\"]\n",
    "\n",
    "    # Fit ARIMA model on the training data using auto_arima to find the best (p, d, q)\n",
    "    model = auto_arima(y_train, start_p=0, start_q=0, max_p=5, max_q=5, d=1,\n",
    "                       seasonal=True, trace=False, error_action='ignore', \n",
    "                       suppress_warnings=True, stepwise=True)\n",
    "    \n",
    "    # Predict on the test data\n",
    "    predictions = model.predict(n_periods=len(y_test))\n",
    "\n",
    "    # Calculate and return the error metric for the current fold\n",
    "    rmsse = calc_rmsse(y_train,y_test,predictions)\n",
    "    \n",
    "    return model, rmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bc9e1224-e66f-4e66-bad4-dd867e626f54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def objective_optuna(trial, y_train, y_test):\n",
    "    \n",
    "    trend = trial.suggest_categorical('trend', ['add'])\n",
    "    seasonal = trial.suggest_categorical('seasonal', [None, 'add'])\n",
    "    seasonal_periods = trial.suggest_categorical('seasonal_periods', [None, 4, 7, 12])\n",
    "    \n",
    "    product_results = []\n",
    "\n",
    "    # Fit Holt-Winters model on the training data\n",
    "    model = ExponentialSmoothing(y_train, trend=trend, seasonal=seasonal, seasonal_periods=seasonal_periods,freq='D')\n",
    "    fitted_model = model.fit(optimized=True)\n",
    "\n",
    "    # Predict on the test data\n",
    "    predictions = fitted_model.forecast(steps=len(y_test))\n",
    "\n",
    "    # Calculate and store the error metric\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    \n",
    "\n",
    "    # Calculate and return the error metric for the current fold\n",
    "    rmsse = calc_rmsse(y_train,y_test, predictions)\n",
    "    product_results.append(rmsse)\n",
    "\n",
    "    # Average MAE for this product\n",
    "    average_rmsse = np.mean(product_results)\n",
    "    return average_rmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "102b594c-39b7-4817-bb4b-f3d5706c0d1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_exp_smoothing(data_train, data_test):\n",
    "    y_train = data_train[\"sales\"]\n",
    "    y_test = data_test[\"sales\"]\n",
    "    # Create a study object\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    \n",
    "    print(f\"Optimizing hyperparameters for product: {id}\")\n",
    "    \n",
    "    \n",
    "    # Run the optimization process for the current product\n",
    "    study.optimize(lambda trial: objective_optuna(trial, y_train, y_test), n_trials=10, n_jobs=-1)\n",
    "\n",
    "    # Get the best hyperparameters and the corresponding best MAE\n",
    "    best_params = study.best_params\n",
    "    best_rmsse = study.best_value\n",
    "\n",
    "    # Create the best model with the obtained hyperparameters\n",
    "    best_model = ExponentialSmoothing(y_train, **best_params).fit()\n",
    "    \n",
    "    return best_model, best_rmsse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1e96217a-a286-4ecd-b8d3-9d23a0fc3811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def perform_lightgbm(data_train, data_val, data_test):\n",
    "    \n",
    "    X_train = data_train.drop(columns=\"sales\")\n",
    "    y_train = data_train[\"sales\"]\n",
    "    X_val = data_val.drop(columns=\"sales\")\n",
    "    y_val = data_val[\"sales\"]\n",
    "    X_test = data_test.drop(columns=\"sales\")\n",
    "    y_test = data_test[\"sales\"]\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Define LightGBM parameters\n",
    "    params = {\n",
    "        \"n_estimators\": 1000,\n",
    "        'objective': 'regression',\n",
    "        'metric': 'rmse',\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"max_depth\": -1,\n",
    "        'learning_rate': 0.01,\n",
    "        'feature_fraction': 0.4,\n",
    "        \"lambda_l1\": 1,\n",
    "        \"lambda_l2\": 1,\n",
    "        \"seed\": 46,\n",
    "        \"verbose\": -1\n",
    "    }\n",
    "\n",
    "    model = lgb.LGBMRegressor(**params)\n",
    "    \n",
    "    # Create dataset for LightGBM\n",
    "    lgb_train = lgb.Dataset(X_train, y_train)\n",
    "    lgb_eval = lgb.Dataset(X_val, y_val, reference=lgb_train)\n",
    "    \n",
    "    # Train the model\n",
    "    num_round = 1000\n",
    "\n",
    "    bst = lgb.train(params, lgb_train, num_round, valid_sets=lgb_eval, callbacks=[lgb.early_stopping(stopping_rounds=50)])\n",
    "     \n",
    "    # Make predictions for the next 28 days\n",
    "    predictions = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "\n",
    "    # Calculate and return the error metric for the current fold\n",
    "    rmsse = calc_rmsse(y_train,y_test, predictions)\n",
    "    \n",
    "    return bst, rmsse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49177452-a384-4cbe-9513-2d5e6d341606",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(product_data):\n",
    "    target = product_data[['sales']]\n",
    "    past_cov = product_data.drop(columns=['sales', 'id']) # ,'item_id','dept_id','state_id','event_name_2'\n",
    "    future_cov = product_data.drop(columns=['sales','id']) # ,'item_id','dept_id','state_id','event_name_2'\n",
    "\n",
    "    y_train = target.loc[:'2016-01-01']\n",
    "    past_cov_train = past_cov.loc[:'2016-01-01']\n",
    "    future_cov_train = future_cov.loc[:'2016-01-29']\n",
    "\n",
    "    y_val = target.loc['2016-01-02':'2016-04-24']\n",
    "    past_cov_val = past_cov.loc['2016-01-02':'2016-04-24']\n",
    "    future_cov_val = future_cov.loc['2016-01-02':'2016-05-22']\n",
    "\n",
    "    return (y_train, past_cov_train, future_cov_train,\n",
    "            y_val, past_cov_val, future_cov_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a38bf15-f7de-422c-b174-b00f28c44c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tft_model(y_train_series, past_cov_train_series, future_cov_train_series,\n",
    "                    y_val_series, past_cov_val_series, future_cov_val_series):\n",
    "    input_chunk_length = 28*2\n",
    "    output_chunk_length = 28\n",
    "\n",
    "    from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "    from pytorch_lightning.loggers import CSVLogger\n",
    "    csv_logger = CSVLogger(\"logs\", name=\"tft_logs\")\n",
    "    patience = 5\n",
    "    my_stopper = EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=patience,\n",
    "        min_delta=0.001,\n",
    "        mode='min',\n",
    "    )\n",
    "\n",
    "    pl_trainer_kwargs={\"callbacks\": [my_stopper],\n",
    "                       \"accelerator\": \"cpu\",\n",
    "                      \"logger\": csv_logger}\n",
    "\n",
    "    tft = TFTModel(input_chunk_length=input_chunk_length,\n",
    "                   output_chunk_length=output_chunk_length,\n",
    "                   pl_trainer_kwargs=pl_trainer_kwargs,\n",
    "                   lstm_layers=2,\n",
    "                   num_attention_heads=4,\n",
    "                   dropout=0.2,\n",
    "                   batch_size=16,\n",
    "                   hidden_size=16,\n",
    "                   torch_metrics=MeanAbsoluteError(),\n",
    "                   n_epochs=50)\n",
    "\n",
    "    tft.fit(series=y_train_series,\n",
    "            past_covariates=past_cov_train_series,\n",
    "            future_covariates=future_cov_train_series,\n",
    "            val_series=y_val_series,\n",
    "            val_past_covariates=past_cov_val_series,\n",
    "            val_future_covariates=future_cov_val_series)\n",
    "\n",
    "    metrics_df = pd.read_csv(\"logs/tft_logs/version_0/metrics.csv\")\n",
    "    validation_loss = metrics_df.val_MeanAbsoluteError.dropna().iloc[-(patience+1)]\n",
    "    \n",
    "    shutil.rmtree(\"logs/tft_logs/version_0\")\n",
    "    \n",
    "    return tft, validation_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b772c5e4-6892-4cf3-93c2-4e4c7cf4bead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c4313bdf-003b-428f-addb-07ad6b7058b8",
   "metadata": {},
   "source": [
    "# 2.Running all models in a loop to find for each product with lowest score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e09341ed-7407-4421-87d4-b97c92ef1fc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_list = [\"ARIMA\",\"ExponentialSmoothing\", \"Prophet\", \"LightGBM\", \"RandomForest\", \"DartsTFT\"] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "25636b8c-848f-4188-bc73-1546fdeb687b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/tmp/ipykernel_448295/714081597.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  last_28_days_df['revenues'] = last_28_days_df['sales'] * last_28_days_df['sell_price']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing product: HOBBIES_1_079_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:02:54,705] A new study created in memory with name: no-name-b6c7ea5f-1722-4fec-9fa0-7b291cb76e21\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: HOBBIES_1_079_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:02:56,175] Trial 1 finished with value: 0.39858175055532846 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 1 with value: 0.39858175055532846.\n",
      "[I 2024-05-15 11:02:56,185] Trial 0 finished with value: 0.39858175055532846 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 1 with value: 0.39858175055532846.\n",
      "[I 2024-05-15 11:02:56,591] Trial 8 finished with value: 0.39858175055532846 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 1 with value: 0.39858175055532846.\n",
      "[I 2024-05-15 11:02:56,627] Trial 2 finished with value: 0.39716155486433224 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 2 with value: 0.39716155486433224.\n",
      "[I 2024-05-15 11:02:56,630] Trial 9 finished with value: 0.39858175055532846 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 2 with value: 0.39716155486433224.\n",
      "[I 2024-05-15 11:02:56,663] Trial 7 finished with value: 0.39858175055532846 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 2 with value: 0.39716155486433224.\n",
      "[I 2024-05-15 11:02:56,665] Trial 5 finished with value: 0.39858175055532846 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 2 with value: 0.39716155486433224.\n",
      "[I 2024-05-15 11:02:56,789] Trial 3 finished with value: 0.38155418180178885 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 3 with value: 0.38155418180178885.\n",
      "[I 2024-05-15 11:02:56,846] Trial 4 finished with value: 0.4001958360177356 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 3 with value: 0.38155418180178885.\n",
      "[I 2024-05-15 11:02:56,912] Trial 6 finished with value: 0.38155418180178885 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 3 with value: 0.38155418180178885.\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "[I 2024-05-15 11:02:57,259] A new study created in memory with name: no-name-1981290b-68ce-4851-9192-12865df2321e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: HOBBIES_1_079_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:02:58,537] Trial 9 finished with value: 0.4873802900566628 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 9 with value: 0.4873802900566628.\n",
      "[I 2024-05-15 11:02:58,556] Trial 6 finished with value: 0.4873802900566628 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 9 with value: 0.4873802900566628.\n",
      "[I 2024-05-15 11:02:59,075] Trial 7 finished with value: 0.4873802900566628 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 9 with value: 0.4873802900566628.\n",
      "[I 2024-05-15 11:02:59,424] Trial 5 finished with value: 0.4873802900566628 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 9 with value: 0.4873802900566628.\n",
      "[I 2024-05-15 11:02:59,663] Trial 1 finished with value: 0.4938717215554582 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 9 with value: 0.4873802900566628.\n",
      "[I 2024-05-15 11:02:59,795] Trial 8 finished with value: 0.4938717215554582 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 9 with value: 0.4873802900566628.\n",
      "[I 2024-05-15 11:02:59,892] Trial 0 finished with value: 0.4910832561018107 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 9 with value: 0.4873802900566628.\n",
      "[I 2024-05-15 11:03:00,010] Trial 3 finished with value: 0.4910832561018107 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 9 with value: 0.4873802900566628.\n",
      "[I 2024-05-15 11:03:00,086] Trial 2 finished with value: 0.4910832561018107 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 9 with value: 0.4873802900566628.\n",
      "[I 2024-05-15 11:03:00,132] Trial 4 finished with value: 0.4910832561018107 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 9 with value: 0.4873802900566628.\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "[I 2024-05-15 11:03:00,201] A new study created in memory with name: no-name-806cba4b-d06c-469a-9b38-c18e12d43d61\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: HOBBIES_1_079_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:03:01,442] Trial 4 finished with value: 0.4881800625433293 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 4 with value: 0.4881800625433293.\n",
      "[I 2024-05-15 11:03:01,706] Trial 7 finished with value: 0.4881800625433293 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 4 with value: 0.4881800625433293.\n",
      "[I 2024-05-15 11:03:02,096] Trial 5 finished with value: 0.4881800625433293 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 4 with value: 0.4881800625433293.\n",
      "[I 2024-05-15 11:03:02,866] Trial 2 finished with value: 0.48456793477313603 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 2 with value: 0.48456793477313603.\n",
      "[I 2024-05-15 11:03:02,980] Trial 0 finished with value: 0.48456793477313603 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 2 with value: 0.48456793477313603.\n",
      "[I 2024-05-15 11:03:03,042] Trial 6 finished with value: 0.48456793477313603 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 2 with value: 0.48456793477313603.\n",
      "[I 2024-05-15 11:03:03,080] Trial 9 finished with value: 0.48456793477313603 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 2 with value: 0.48456793477313603.\n",
      "[I 2024-05-15 11:03:03,130] Trial 1 finished with value: 0.49149118838378575 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 2 with value: 0.48456793477313603.\n",
      "[I 2024-05-15 11:03:03,180] Trial 8 finished with value: 0.49149118838378575 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 2 with value: 0.48456793477313603.\n",
      "[I 2024-05-15 11:03:03,253] Trial 3 finished with value: 0.4960887183859357 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 2 with value: 0.48456793477313603.\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "[I 2024-05-15 11:03:03,589] A new study created in memory with name: no-name-54d3a900-49f1-4ea8-9d6c-5a7bb3f6a906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: HOBBIES_1_079_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:03:04,073] Trial 0 finished with value: 0.5122907473499104 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 0 with value: 0.5122907473499104.\n",
      "[I 2024-05-15 11:03:05,553] Trial 4 finished with value: 0.5122907473499104 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 0 with value: 0.5122907473499104.\n",
      "[I 2024-05-15 11:03:05,652] Trial 8 finished with value: 0.5122907473499104 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 0 with value: 0.5122907473499104.\n",
      "[I 2024-05-15 11:03:06,257] Trial 3 finished with value: 0.5182383526823281 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 0 with value: 0.5122907473499104.\n",
      "[I 2024-05-15 11:03:06,275] Trial 5 finished with value: 0.53719957953137 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 7}. Best is trial 0 with value: 0.5122907473499104.\n",
      "[I 2024-05-15 11:03:06,306] Trial 6 finished with value: 0.5182383526823281 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 0 with value: 0.5122907473499104.\n",
      "[I 2024-05-15 11:03:06,379] Trial 2 finished with value: 0.5221152357312842 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 0 with value: 0.5122907473499104.\n",
      "[I 2024-05-15 11:03:06,474] Trial 9 finished with value: 0.5221152357312842 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 0 with value: 0.5122907473499104.\n",
      "[I 2024-05-15 11:03:06,561] Trial 1 finished with value: 0.53719957953137 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 0 with value: 0.5122907473499104.\n",
      "[I 2024-05-15 11:03:06,587] Trial 7 finished with value: 0.53719957953137 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 7}. Best is trial 0 with value: 0.5122907473499104.\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "[I 2024-05-15 11:03:06,651] A new study created in memory with name: no-name-99851b6d-ec3d-4f33-9eea-f052c6e4d73a\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: HOBBIES_1_079_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:03:07,365] Trial 0 finished with value: 0.2948876350631427 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 0 with value: 0.2948876350631427.\n",
      "[I 2024-05-15 11:03:08,165] Trial 7 finished with value: 0.2948876350631427 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 0 with value: 0.2948876350631427.\n",
      "[I 2024-05-15 11:03:08,221] Trial 4 finished with value: 0.2948876350631427 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 0 with value: 0.2948876350631427.\n",
      "[I 2024-05-15 11:03:08,395] Trial 6 finished with value: 0.2948876350631427 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 0 with value: 0.2948876350631427.\n",
      "[I 2024-05-15 11:03:08,573] Trial 8 finished with value: 0.2948876350631427 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 0 with value: 0.2948876350631427.\n",
      "[I 2024-05-15 11:03:08,730] Trial 1 finished with value: 0.3105472278085587 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 7}. Best is trial 0 with value: 0.2948876350631427.\n",
      "[I 2024-05-15 11:03:08,848] Trial 3 finished with value: 0.2906994126871484 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 3 with value: 0.2906994126871484.\n",
      "[I 2024-05-15 11:03:08,901] Trial 2 finished with value: 0.3105472278085587 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 3 with value: 0.2906994126871484.\n",
      "[I 2024-05-15 11:03:09,025] Trial 9 finished with value: 0.2906994126871484 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 3 with value: 0.2906994126871484.\n",
      "[I 2024-05-15 11:03:09,066] Trial 5 finished with value: 0.3105472278085587 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 7}. Best is trial 3 with value: 0.2906994126871484.\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/var/tmp/ipykernel_448295/1261245201.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_train['ds'] = pd.to_datetime(prophet_data_train['ds'])\n",
      "/var/tmp/ipykernel_448295/1261245201.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_test['ds'] = pd.to_datetime(prophet_data_test['ds'])\n",
      "11:03:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:03:09 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/var/tmp/ipykernel_448295/1261245201.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_train['ds'] = pd.to_datetime(prophet_data_train['ds'])\n",
      "/var/tmp/ipykernel_448295/1261245201.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_test['ds'] = pd.to_datetime(prophet_data_test['ds'])\n",
      "11:03:09 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:03:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/var/tmp/ipykernel_448295/1261245201.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_train['ds'] = pd.to_datetime(prophet_data_train['ds'])\n",
      "/var/tmp/ipykernel_448295/1261245201.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_test['ds'] = pd.to_datetime(prophet_data_test['ds'])\n",
      "11:03:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:03:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/var/tmp/ipykernel_448295/1261245201.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_train['ds'] = pd.to_datetime(prophet_data_train['ds'])\n",
      "/var/tmp/ipykernel_448295/1261245201.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_test['ds'] = pd.to_datetime(prophet_data_test['ds'])\n",
      "11:03:10 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:03:10 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/var/tmp/ipykernel_448295/1261245201.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_train['ds'] = pd.to_datetime(prophet_data_train['ds'])\n",
      "/var/tmp/ipykernel_448295/1261245201.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_test['ds'] = pd.to_datetime(prophet_data_test['ds'])\n",
      "11:03:11 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:03:11 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[998]\tvalid_0's rmse: 0.602269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 0.600948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 0.739497\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 0.461678\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[998]\tvalid_0's rmse: 0.628156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: logs/tft_logs\n",
      "\n",
      "   | Name                              | Type                             | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | train_metrics                     | MetricCollection                 | 0     \n",
      "1  | val_metrics                       | MetricCollection                 | 0     \n",
      "2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
      "3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
      "4  | encoder_vsn                       | _VariableSelectionNetwork        | 80.5 K\n",
      "5  | decoder_vsn                       | _VariableSelectionNetwork        | 40.2 K\n",
      "6  | static_context_grn                | _GatedResidualNetwork            | 1.1 K \n",
      "7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_enrichment         | _GatedResidualNetwork            | 1.1 K \n",
      "10 | lstm_encoder                      | LSTM                             | 4.4 K \n",
      "11 | lstm_decoder                      | LSTM                             | 4.4 K \n",
      "12 | post_lstm_gan                     | _GateAddNorm                     | 576   \n",
      "13 | static_enrichment_grn             | _GatedResidualNetwork            | 1.4 K \n",
      "14 | multihead_attn                    | _InterpretableMultiHeadAttention | 676   \n",
      "15 | post_attn_gan                     | _GateAddNorm                     | 576   \n",
      "16 | feed_forward_block                | _GatedResidualNetwork            | 1.1 K \n",
      "17 | pre_output_gan                    | _GateAddNorm                     | 576   \n",
      "18 | output_layer                      | Linear                           | 289   \n",
      "----------------------------------------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.552     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64609b8fd6224f15bc301805d1616c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ARIMA': {'rmsse': 0.4346728954288251, 'model': ARIMA(order=(0, 1, 1), scoring_args={}, suppress_warnings=True,\n",
      "      with_intercept=False)}, 'ExponentialSmoothing': {'rmsse': 0.43129851333372937, 'model': <statsmodels.tsa.holtwinters.results.HoltWintersResultsWrapper object at 0x7f1a32986ec0>}, 'Prophet': {'rmsse': 0.44200709481367345, 'model': <prophet.forecaster.Prophet object at 0x7f1a329d73a0>}, 'LightGBM': {'rmsse': 1.055946474626952, 'model': <lightgbm.basic.Booster object at 0x7f1a327a7550>}, 'RandomForest': {'rmsse': 0.6344300917622752, 'model': RandomForestRegressor(random_state=42)}, 'DartsTFT': {'rmsse': 0.9442148804664612, 'model': TFTModel(output_chunk_shift=0, hidden_size=16, lstm_layers=2, num_attention_heads=4, full_attention=False, feed_forward=GatedResidualNetwork, dropout=0.2, hidden_continuous_size=8, categorical_embedding_sizes=None, add_relative_index=False, loss_fn=None, likelihood=None, norm_type=LayerNorm, use_static_covariates=True, input_chunk_length=56, output_chunk_length=28, pl_trainer_kwargs={'callbacks': [<pytorch_lightning.callbacks.early_stopping.EarlyStopping object at 0x7f1a329ea2f0>], 'accelerator': 'cpu', 'logger': <pytorch_lightning.loggers.csv_logs.CSVLogger object at 0x7f1a329ea080>}, batch_size=16, torch_metrics=MeanAbsoluteError(), n_epochs=50)}}\n",
      "Model results for HOBBIES_1_079_CA_1_validation\n",
      "Best model: ExponentialSmoothing\n",
      "Best score: 0.43129851333372937\n",
      "Analyzing product: HOBBIES_1_080_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:09:35,618] A new study created in memory with name: no-name-09cd3cd4-734e-4e85-9aef-1ab3815808db\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: HOBBIES_1_080_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:09:36,892] Trial 0 finished with value: 0.28782987648336433 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 0 with value: 0.28782987648336433.\n",
      "[I 2024-05-15 11:09:36,985] Trial 2 finished with value: 0.28782987648336433 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 0 with value: 0.28782987648336433.\n",
      "[I 2024-05-15 11:09:37,201] Trial 3 finished with value: 0.28782987648336433 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 0 with value: 0.28782987648336433.\n",
      "[I 2024-05-15 11:09:37,416] Trial 8 finished with value: 0.28782987648336433 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 0 with value: 0.28782987648336433.\n",
      "[I 2024-05-15 11:09:37,440] Trial 9 finished with value: 0.28782987648336433 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 0 with value: 0.28782987648336433.\n",
      "[I 2024-05-15 11:09:37,457] Trial 7 finished with value: 0.28782987648336433 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 0 with value: 0.28782987648336433.\n",
      "[I 2024-05-15 11:09:37,634] Trial 5 finished with value: 0.3226581169195913 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 7}. Best is trial 0 with value: 0.28782987648336433.\n",
      "[I 2024-05-15 11:09:37,672] Trial 1 finished with value: 0.3226581169195913 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 0 with value: 0.28782987648336433.\n",
      "[I 2024-05-15 11:09:37,737] Trial 4 finished with value: 0.285492470598082 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 4 with value: 0.285492470598082.\n",
      "[I 2024-05-15 11:09:37,783] Trial 6 finished with value: 0.285492470598082 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 4 with value: 0.285492470598082.\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "[I 2024-05-15 11:09:38,151] A new study created in memory with name: no-name-91c3e6aa-ecda-4eb8-9d30-d47171ea57b8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: HOBBIES_1_080_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:09:38,971] Trial 0 finished with value: 0.18484263811631552 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 0 with value: 0.18484263811631552.\n",
      "[I 2024-05-15 11:09:39,137] Trial 2 finished with value: 0.18484263811631552 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 0 with value: 0.18484263811631552.\n",
      "[I 2024-05-15 11:09:39,335] Trial 6 finished with value: 0.18484263811631552 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 0 with value: 0.18484263811631552.\n",
      "[I 2024-05-15 11:09:39,361] Trial 5 finished with value: 0.18484263811631552 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 0 with value: 0.18484263811631552.\n",
      "[I 2024-05-15 11:09:39,607] Trial 8 finished with value: 0.18484263811631552 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 0 with value: 0.18484263811631552.\n",
      "[I 2024-05-15 11:09:39,713] Trial 9 finished with value: 0.18484263811631552 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 0 with value: 0.18484263811631552.\n",
      "[I 2024-05-15 11:09:39,723] Trial 7 finished with value: 0.18484263811631552 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 0 with value: 0.18484263811631552.\n",
      "[I 2024-05-15 11:09:39,818] Trial 1 finished with value: 0.18997979859043884 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 7}. Best is trial 0 with value: 0.18484263811631552.\n",
      "[I 2024-05-15 11:09:39,924] Trial 4 finished with value: 0.18672449378002517 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 0 with value: 0.18484263811631552.\n",
      "[I 2024-05-15 11:09:39,928] Trial 3 finished with value: 0.1900911609192232 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 0 with value: 0.18484263811631552.\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "[I 2024-05-15 11:09:39,994] A new study created in memory with name: no-name-78a9329a-8275-4155-952f-a05594a892f9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: HOBBIES_1_080_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:09:40,842] Trial 0 finished with value: 0.3029363992233814 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 0 with value: 0.3029363992233814.\n",
      "[I 2024-05-15 11:09:41,483] Trial 3 finished with value: 0.3029363992233814 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 0 with value: 0.3029363992233814.\n",
      "[I 2024-05-15 11:09:41,586] Trial 6 finished with value: 0.3029363992233814 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 0 with value: 0.3029363992233814.\n",
      "[I 2024-05-15 11:09:41,599] Trial 8 finished with value: 0.3029363992233814 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 0 with value: 0.3029363992233814.\n",
      "[I 2024-05-15 11:09:41,743] Trial 7 finished with value: 0.3029363992233814 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 0 with value: 0.3029363992233814.\n",
      "[I 2024-05-15 11:09:42,130] Trial 5 finished with value: 0.310149067515475 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 0 with value: 0.3029363992233814.\n",
      "[I 2024-05-15 11:09:42,189] Trial 9 finished with value: 0.3019479410287516 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 9 with value: 0.3019479410287516.\n",
      "[I 2024-05-15 11:09:42,245] Trial 2 finished with value: 0.310149067515475 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 9 with value: 0.3019479410287516.\n",
      "[I 2024-05-15 11:09:42,292] Trial 1 finished with value: 0.3019479410287516 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 9 with value: 0.3019479410287516.\n",
      "[I 2024-05-15 11:09:42,340] Trial 4 finished with value: 0.30918945450102253 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 9 with value: 0.3019479410287516.\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "[I 2024-05-15 11:09:42,682] A new study created in memory with name: no-name-966b76fe-6914-48d2-847e-8773d29419b7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: HOBBIES_1_080_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:09:43,515] Trial 1 finished with value: 0.2678161661293074 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 1 with value: 0.2678161661293074.\n",
      "[I 2024-05-15 11:09:43,953] Trial 5 finished with value: 0.2678161661293074 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 1 with value: 0.2678161661293074.\n",
      "[I 2024-05-15 11:09:44,063] Trial 7 finished with value: 0.2678161661293074 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 1 with value: 0.2678161661293074.\n",
      "[I 2024-05-15 11:09:44,125] Trial 8 finished with value: 0.2678161661293074 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 1 with value: 0.2678161661293074.\n",
      "[I 2024-05-15 11:09:44,183] Trial 9 finished with value: 0.2678161661293074 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 1 with value: 0.2678161661293074.\n",
      "[I 2024-05-15 11:09:44,379] Trial 6 finished with value: 0.2678161661293074 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 1 with value: 0.2678161661293074.\n",
      "[I 2024-05-15 11:09:44,575] Trial 2 finished with value: 0.266990473730553 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 2 with value: 0.266990473730553.\n",
      "[I 2024-05-15 11:09:44,712] Trial 3 finished with value: 0.27109313975556404 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 2 with value: 0.266990473730553.\n",
      "[I 2024-05-15 11:09:44,732] Trial 0 finished with value: 0.28096211958543343 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 2 with value: 0.266990473730553.\n",
      "[I 2024-05-15 11:09:44,736] Trial 4 finished with value: 0.27109313975556404 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 2 with value: 0.266990473730553.\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "[I 2024-05-15 11:09:45,063] A new study created in memory with name: no-name-98e0e508-3b0b-4b45-97aa-de401f662ed9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: HOBBIES_1_080_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:09:45,854] Trial 2 finished with value: 0.0021225037909506483 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 2 with value: 0.0021225037909506483.\n",
      "[I 2024-05-15 11:09:45,917] Trial 3 finished with value: 0.0021225037909506483 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 2 with value: 0.0021225037909506483.\n",
      "[I 2024-05-15 11:09:46,542] Trial 7 finished with value: 0.0021225037909506483 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 2 with value: 0.0021225037909506483.\n",
      "[I 2024-05-15 11:09:46,634] Trial 9 finished with value: 0.0021225037909506483 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 2 with value: 0.0021225037909506483.\n",
      "[I 2024-05-15 11:09:46,661] Trial 5 finished with value: 0.0021225037909506483 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 2 with value: 0.0021225037909506483.\n",
      "[I 2024-05-15 11:09:47,070] Trial 1 finished with value: 0.09406634281522575 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 7}. Best is trial 2 with value: 0.0021225037909506483.\n",
      "[I 2024-05-15 11:09:47,138] Trial 0 finished with value: 0.09406634281522575 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 7}. Best is trial 2 with value: 0.0021225037909506483.\n",
      "[I 2024-05-15 11:09:47,238] Trial 4 finished with value: 0.018302809676433034 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 2 with value: 0.0021225037909506483.\n",
      "[I 2024-05-15 11:09:47,243] Trial 6 finished with value: 0.018302809676433034 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 2 with value: 0.0021225037909506483.\n",
      "[I 2024-05-15 11:09:47,291] Trial 8 finished with value: 0.09406634281522575 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 2 with value: 0.0021225037909506483.\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/var/tmp/ipykernel_448295/1261245201.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_train['ds'] = pd.to_datetime(prophet_data_train['ds'])\n",
      "/var/tmp/ipykernel_448295/1261245201.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_test['ds'] = pd.to_datetime(prophet_data_test['ds'])\n",
      "11:09:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:09:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/var/tmp/ipykernel_448295/1261245201.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_train['ds'] = pd.to_datetime(prophet_data_train['ds'])\n",
      "/var/tmp/ipykernel_448295/1261245201.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_test['ds'] = pd.to_datetime(prophet_data_test['ds'])\n",
      "11:09:47 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:09:47 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/var/tmp/ipykernel_448295/1261245201.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_train['ds'] = pd.to_datetime(prophet_data_train['ds'])\n",
      "/var/tmp/ipykernel_448295/1261245201.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_test['ds'] = pd.to_datetime(prophet_data_test['ds'])\n",
      "11:09:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:09:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/var/tmp/ipykernel_448295/1261245201.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_train['ds'] = pd.to_datetime(prophet_data_train['ds'])\n",
      "/var/tmp/ipykernel_448295/1261245201.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_test['ds'] = pd.to_datetime(prophet_data_test['ds'])\n",
      "11:09:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:09:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/var/tmp/ipykernel_448295/1261245201.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_train['ds'] = pd.to_datetime(prophet_data_train['ds'])\n",
      "/var/tmp/ipykernel_448295/1261245201.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_test['ds'] = pd.to_datetime(prophet_data_test['ds'])\n",
      "11:09:48 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:09:48 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[998]\tvalid_0's rmse: 1.31733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 2.10283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[313]\tvalid_0's rmse: 1.61332\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[852]\tvalid_0's rmse: 0.0370325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[982]\tvalid_0's rmse: 0.023095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: logs/tft_logs\n",
      "\n",
      "   | Name                              | Type                             | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | train_metrics                     | MetricCollection                 | 0     \n",
      "1  | val_metrics                       | MetricCollection                 | 0     \n",
      "2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
      "3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
      "4  | encoder_vsn                       | _VariableSelectionNetwork        | 80.5 K\n",
      "5  | decoder_vsn                       | _VariableSelectionNetwork        | 40.2 K\n",
      "6  | static_context_grn                | _GatedResidualNetwork            | 1.1 K \n",
      "7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_enrichment         | _GatedResidualNetwork            | 1.1 K \n",
      "10 | lstm_encoder                      | LSTM                             | 4.4 K \n",
      "11 | lstm_decoder                      | LSTM                             | 4.4 K \n",
      "12 | post_lstm_gan                     | _GateAddNorm                     | 576   \n",
      "13 | static_enrichment_grn             | _GatedResidualNetwork            | 1.4 K \n",
      "14 | multihead_attn                    | _InterpretableMultiHeadAttention | 676   \n",
      "15 | post_attn_gan                     | _GateAddNorm                     | 576   \n",
      "16 | feed_forward_block                | _GatedResidualNetwork            | 1.1 K \n",
      "17 | pre_output_gan                    | _GateAddNorm                     | 576   \n",
      "18 | output_layer                      | Linear                           | 289   \n",
      "----------------------------------------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.552     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637f32ad89124999a95ac95f7a43a9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ARIMA': {'rmsse': 0.20943450595834817, 'model': ARIMA(order=(1, 1, 2), scoring_args={}, suppress_warnings=True,\n",
      "      with_intercept=False)}, 'ExponentialSmoothing': {'rmsse': 0.2082792054529306, 'model': <statsmodels.tsa.holtwinters.results.HoltWintersResultsWrapper object at 0x7f19e62ed8d0>}, 'Prophet': {'rmsse': 0.26334487891378755, 'model': <prophet.forecaster.Prophet object at 0x7f19e62ec190>}, 'LightGBM': {'rmsse': 0.40169608974321785, 'model': <lightgbm.basic.Booster object at 0x7f19e632c910>}, 'RandomForest': {'rmsse': 0.20811015950273926, 'model': RandomForestRegressor(random_state=42)}, 'DartsTFT': {'rmsse': 1.290215253829956, 'model': TFTModel(output_chunk_shift=0, hidden_size=16, lstm_layers=2, num_attention_heads=4, full_attention=False, feed_forward=GatedResidualNetwork, dropout=0.2, hidden_continuous_size=8, categorical_embedding_sizes=None, add_relative_index=False, loss_fn=None, likelihood=None, norm_type=LayerNorm, use_static_covariates=True, input_chunk_length=56, output_chunk_length=28, pl_trainer_kwargs={'callbacks': [<pytorch_lightning.callbacks.early_stopping.EarlyStopping object at 0x7f19e632f400>], 'accelerator': 'cpu', 'logger': <pytorch_lightning.loggers.csv_logs.CSVLogger object at 0x7f19e632c7c0>}, batch_size=16, torch_metrics=MeanAbsoluteError(), n_epochs=50)}}\n",
      "Model results for HOBBIES_1_080_CA_1_validation\n",
      "Best model: RandomForest\n",
      "Best score: 0.20811015950273926\n",
      "Analyzing product: HOBBIES_1_081_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:16:33,413] A new study created in memory with name: no-name-15811470-0b40-4227-8b10-de3c715df071\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: HOBBIES_1_081_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:16:33,921] Trial 1 finished with value: 0.6841974002736215 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 1 with value: 0.6841974002736215.\n",
      "[I 2024-05-15 11:16:34,224] Trial 3 finished with value: 0.6841974002736215 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 1 with value: 0.6841974002736215.\n",
      "[I 2024-05-15 11:16:34,345] Trial 4 finished with value: 0.6841974002736215 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 1 with value: 0.6841974002736215.\n",
      "[I 2024-05-15 11:16:34,719] Trial 6 finished with value: 0.6841974002736215 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 1 with value: 0.6841974002736215.\n",
      "[I 2024-05-15 11:16:34,871] Trial 5 finished with value: 0.6841974002736215 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 1 with value: 0.6841974002736215.\n",
      "[I 2024-05-15 11:16:34,998] Trial 9 finished with value: 0.6841974002736215 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 1 with value: 0.6841974002736215.\n",
      "[I 2024-05-15 11:16:35,239] Trial 2 finished with value: 0.6563811048515293 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 7}. Best is trial 2 with value: 0.6563811048515293.\n",
      "[I 2024-05-15 11:16:35,295] Trial 0 finished with value: 0.6563811048515293 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 7}. Best is trial 2 with value: 0.6563811048515293.\n",
      "[I 2024-05-15 11:16:35,354] Trial 7 finished with value: 0.6845052937843483 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 2 with value: 0.6563811048515293.\n",
      "[I 2024-05-15 11:16:35,395] Trial 8 finished with value: 0.6563811048515293 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 7}. Best is trial 2 with value: 0.6563811048515293.\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "[I 2024-05-15 11:16:35,740] A new study created in memory with name: no-name-d5690853-003c-481c-8353-a36cf5a40dda\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: HOBBIES_1_081_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:16:36,509] Trial 2 finished with value: 0.7851755647556365 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 2 with value: 0.7851755647556365.\n",
      "[I 2024-05-15 11:16:36,690] Trial 4 finished with value: 0.7851755647556365 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 2 with value: 0.7851755647556365.\n",
      "[I 2024-05-15 11:16:36,696] Trial 0 finished with value: 0.7851755647556365 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 2 with value: 0.7851755647556365.\n",
      "[I 2024-05-15 11:16:36,908] Trial 9 finished with value: 0.7851755647556365 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 2 with value: 0.7851755647556365.\n",
      "[I 2024-05-15 11:16:37,020] Trial 7 finished with value: 0.7851755647556365 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 2 with value: 0.7851755647556365.\n",
      "[I 2024-05-15 11:16:37,366] Trial 8 finished with value: 0.7851755647556365 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 2 with value: 0.7851755647556365.\n",
      "[I 2024-05-15 11:16:37,581] Trial 1 finished with value: 0.8079608977349046 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 7}. Best is trial 2 with value: 0.7851755647556365.\n",
      "[I 2024-05-15 11:16:37,693] Trial 5 finished with value: 0.8079608977349046 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 2 with value: 0.7851755647556365.\n",
      "[I 2024-05-15 11:16:37,698] Trial 3 finished with value: 0.7805640444997041 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 3 with value: 0.7805640444997041.\n",
      "[I 2024-05-15 11:16:37,744] Trial 6 finished with value: 0.7805640444997041 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 3 with value: 0.7805640444997041.\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "[I 2024-05-15 11:16:38,101] A new study created in memory with name: no-name-09890908-35db-4ac9-8e5b-ab1f6c872de6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: HOBBIES_1_081_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:16:38,625] Trial 0 finished with value: 1.2186207114545744 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 0 with value: 1.2186207114545744.\n",
      "[I 2024-05-15 11:16:38,767] Trial 3 finished with value: 1.2186207114545744 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 0 with value: 1.2186207114545744.\n",
      "[I 2024-05-15 11:16:39,090] Trial 5 finished with value: 1.2186207114545744 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 0 with value: 1.2186207114545744.\n",
      "[I 2024-05-15 11:16:39,282] Trial 9 finished with value: 1.2186207114545744 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 0 with value: 1.2186207114545744.\n",
      "[I 2024-05-15 11:16:39,309] Trial 8 finished with value: 1.2186207114545744 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 0 with value: 1.2186207114545744.\n",
      "[I 2024-05-15 11:16:39,309] Trial 6 finished with value: 1.2186207114545744 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': None}. Best is trial 0 with value: 1.2186207114545744.\n",
      "[I 2024-05-15 11:16:39,400] Trial 7 finished with value: 1.2186207114545744 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 0 with value: 1.2186207114545744.\n",
      "[I 2024-05-15 11:16:39,678] Trial 2 finished with value: 1.2181730830505966 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 2 with value: 1.2181730830505966.\n",
      "[I 2024-05-15 11:16:39,738] Trial 1 finished with value: 1.2107249719966302 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 1 with value: 1.2107249719966302.\n",
      "[I 2024-05-15 11:16:39,792] Trial 4 finished with value: 1.2107249719966302 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 1 with value: 1.2107249719966302.\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "[I 2024-05-15 11:16:40,143] A new study created in memory with name: no-name-e15d797a-08bf-4b6c-afbe-ab232f12b6cb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: HOBBIES_1_081_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:16:40,380] Trial 0 finished with value: 0.9389989313830116 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 0 with value: 0.9389989313830116.\n",
      "[I 2024-05-15 11:16:40,393] Trial 1 finished with value: 0.9389989313830116 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 0 with value: 0.9389989313830116.\n",
      "[I 2024-05-15 11:16:41,316] Trial 9 finished with value: 0.9389989313830116 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 0 with value: 0.9389989313830116.\n",
      "[I 2024-05-15 11:16:41,561] Trial 7 finished with value: 0.9389989313830116 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 0 with value: 0.9389989313830116.\n",
      "[I 2024-05-15 11:16:42,405] Trial 2 finished with value: 0.933728518751242 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 4}. Best is trial 2 with value: 0.933728518751242.\n",
      "[I 2024-05-15 11:16:42,438] Trial 6 finished with value: 0.8979372256920832 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 7}. Best is trial 6 with value: 0.8979372256920832.\n",
      "[I 2024-05-15 11:16:42,499] Trial 8 finished with value: 0.8979372256920832 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 6 with value: 0.8979372256920832.\n",
      "[I 2024-05-15 11:16:42,554] Trial 3 finished with value: 0.8979372256920832 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 7}. Best is trial 6 with value: 0.8979372256920832.\n",
      "[I 2024-05-15 11:16:42,670] Trial 4 finished with value: 0.9367913267903417 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 6 with value: 0.8979372256920832.\n",
      "[I 2024-05-15 11:16:42,685] Trial 5 finished with value: 0.9367913267903417 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': 12}. Best is trial 6 with value: 0.8979372256920832.\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "[I 2024-05-15 11:16:43,013] A new study created in memory with name: no-name-ca1cf837-b91d-48b2-919c-6b19c89b22f4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters for product: HOBBIES_1_081_CA_1_validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-05-15 11:16:43,445] Trial 2 finished with value: 0.6922438717481055 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 2 with value: 0.6922438717481055.\n",
      "[I 2024-05-15 11:16:43,446] Trial 1 finished with value: 0.6922438717481055 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 2 with value: 0.6922438717481055.\n",
      "[I 2024-05-15 11:16:43,472] Trial 5 finished with value: 0.6922438717481055 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 2 with value: 0.6922438717481055.\n",
      "[I 2024-05-15 11:16:43,496] Trial 0 finished with value: 0.6922438717481055 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 2 with value: 0.6922438717481055.\n",
      "[I 2024-05-15 11:16:43,528] Trial 4 finished with value: 0.6922438717481055 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 7}. Best is trial 2 with value: 0.6922438717481055.\n",
      "[I 2024-05-15 11:16:43,642] Trial 3 finished with value: 0.6922438717481055 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 2 with value: 0.6922438717481055.\n",
      "[I 2024-05-15 11:16:43,795] Trial 7 finished with value: 0.6922438717481055 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 4}. Best is trial 2 with value: 0.6922438717481055.\n",
      "[I 2024-05-15 11:16:44,013] Trial 8 finished with value: 0.6922438717481055 and parameters: {'trend': 'add', 'seasonal': None, 'seasonal_periods': 12}. Best is trial 2 with value: 0.6922438717481055.\n",
      "[I 2024-05-15 11:16:44,341] Trial 9 finished with value: 0.6546486610522382 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 9 with value: 0.6546486610522382.\n",
      "[I 2024-05-15 11:16:44,353] Trial 6 finished with value: 0.6546486610522382 and parameters: {'trend': 'add', 'seasonal': 'add', 'seasonal_periods': None}. Best is trial 9 with value: 0.6546486610522382.\n",
      "/opt/conda/lib/python3.10/site-packages/statsmodels/tsa/base/tsa_model.py:473: ValueWarning: No frequency information was provided, so inferred frequency D will be used.\n",
      "  self._init_dates(dates, freq)\n",
      "/var/tmp/ipykernel_448295/1261245201.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_train['ds'] = pd.to_datetime(prophet_data_train['ds'])\n",
      "/var/tmp/ipykernel_448295/1261245201.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_test['ds'] = pd.to_datetime(prophet_data_test['ds'])\n",
      "11:16:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:16:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/var/tmp/ipykernel_448295/1261245201.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_train['ds'] = pd.to_datetime(prophet_data_train['ds'])\n",
      "/var/tmp/ipykernel_448295/1261245201.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_test['ds'] = pd.to_datetime(prophet_data_test['ds'])\n",
      "11:16:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:16:45 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/var/tmp/ipykernel_448295/1261245201.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_train['ds'] = pd.to_datetime(prophet_data_train['ds'])\n",
      "/var/tmp/ipykernel_448295/1261245201.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_test['ds'] = pd.to_datetime(prophet_data_test['ds'])\n",
      "11:16:45 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:16:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/var/tmp/ipykernel_448295/1261245201.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_train['ds'] = pd.to_datetime(prophet_data_train['ds'])\n",
      "/var/tmp/ipykernel_448295/1261245201.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_test['ds'] = pd.to_datetime(prophet_data_test['ds'])\n",
      "11:16:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:16:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/var/tmp/ipykernel_448295/1261245201.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_train['ds'] = pd.to_datetime(prophet_data_train['ds'])\n",
      "/var/tmp/ipykernel_448295/1261245201.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  prophet_data_test['ds'] = pd.to_datetime(prophet_data_test['ds'])\n",
      "11:16:46 - cmdstanpy - INFO - Chain [1] start processing\n",
      "11:16:46 - cmdstanpy - INFO - Chain [1] done processing\n",
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[145]\tvalid_0's rmse: 0.637101\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's rmse: 0.992762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 0.654427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1000]\tvalid_0's rmse: 0.605531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/lightgbm/engine.py:172: UserWarning: Found `n_estimators` in params. Will use it instead of argument\n",
      "  _log_warning(f\"Found `{alias}` in params. Will use it instead of argument\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 50 rounds\n",
      "Early stopping, best iteration is:\n",
      "[262]\tvalid_0's rmse: 0.498727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: logs/tft_logs\n",
      "\n",
      "   | Name                              | Type                             | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | train_metrics                     | MetricCollection                 | 0     \n",
      "1  | val_metrics                       | MetricCollection                 | 0     \n",
      "2  | input_embeddings                  | _MultiEmbedding                  | 0     \n",
      "3  | static_covariates_vsn             | _VariableSelectionNetwork        | 0     \n",
      "4  | encoder_vsn                       | _VariableSelectionNetwork        | 80.5 K\n",
      "5  | decoder_vsn                       | _VariableSelectionNetwork        | 40.2 K\n",
      "6  | static_context_grn                | _GatedResidualNetwork            | 1.1 K \n",
      "7  | static_context_hidden_encoder_grn | _GatedResidualNetwork            | 1.1 K \n",
      "8  | static_context_cell_encoder_grn   | _GatedResidualNetwork            | 1.1 K \n",
      "9  | static_context_enrichment         | _GatedResidualNetwork            | 1.1 K \n",
      "10 | lstm_encoder                      | LSTM                             | 4.4 K \n",
      "11 | lstm_decoder                      | LSTM                             | 4.4 K \n",
      "12 | post_lstm_gan                     | _GateAddNorm                     | 576   \n",
      "13 | static_enrichment_grn             | _GatedResidualNetwork            | 1.4 K \n",
      "14 | multihead_attn                    | _InterpretableMultiHeadAttention | 676   \n",
      "15 | post_attn_gan                     | _GateAddNorm                     | 576   \n",
      "16 | feed_forward_block                | _GatedResidualNetwork            | 1.1 K \n",
      "17 | pre_output_gan                    | _GateAddNorm                     | 576   \n",
      "18 | output_layer                      | Linear                           | 289   \n",
      "----------------------------------------------------------------------------------------\n",
      "138 K     Trainable params\n",
      "0         Non-trainable params\n",
      "138 K     Total params\n",
      "0.552     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c49b24c77429400f824b14484d3477f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/pytorch_lightning/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'val_MeanAbsoluteError'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/tmp/ipykernel_448295/3099035450.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0my_val_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeSeries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_missing_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0mpast_cov_val_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeSeries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpast_cov_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_missing_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0mfuture_cov_val_series\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTimeSeries\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfuture_cov_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_missing_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             trained_model, validation_loss = train_tft_model(y_train_series, past_cov_train_series, future_cov_train_series,\n\u001b[0m\u001b[1;32m    124\u001b[0m                                             y_val_series, past_cov_val_series, future_cov_val_series)\n\u001b[1;32m    125\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/tmp/ipykernel_448295/461817704.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(y_train_series, past_cov_train_series, future_cov_train_series, y_val_series, past_cov_val_series, future_cov_val_series)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0mval_past_covariates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_cov_val_series\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             val_future_covariates=future_cov_val_series)\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0mmetrics_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logs/tft_logs/version_0/metrics.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m     \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetrics_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_MeanAbsoluteError\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrmtree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"logs/tft_logs/version_0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5985\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5986\u001b[0m             \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5987\u001b[0m         ):\n\u001b[1;32m   5988\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'val_MeanAbsoluteError'"
     ]
    }
   ],
   "source": [
    "from pmdarima import auto_arima\n",
    "\n",
    "# Dictionary to store MAE results for each unique time-series identified by id\n",
    "product_results = {}\n",
    "average_rmsse = []\n",
    "\n",
    "ids_filtered=merge_df_scaled['id'].unique()[76:100] # download the files for 200 ids.\n",
    "filtered_df = merge_df_scaled[merge_df_scaled['id'].isin(ids_filtered)]\n",
    "weights=calculate_last_28_days_weights(filtered_df)   \n",
    "\n",
    "# Iterate over each unique product series identified by id\n",
    "for id in ids_filtered:\n",
    "    print(f\"Analyzing product: {id}\")\n",
    "    product_data = merge_df_scaled[merge_df_scaled['id'] == id].drop(columns=\"id\")\n",
    "    product_data_with_id = merge_df_scaled[merge_df_scaled['id'] == id]\n",
    "\n",
    "    # Results list for the current product time-series\n",
    "    results = {}\n",
    "    best_score = 999.99\n",
    "    best_model_name = \"\"\n",
    "\n",
    "\n",
    "    product_weight=weights.loc[id].weights\n",
    "\n",
    "\n",
    "\n",
    "    #Looping all models\n",
    "    for model_name in models_list:\n",
    "\n",
    "        if model_name == \"ARIMA\":\n",
    "            rmsse_list = []\n",
    "            # Fit ARIMA model on the training data using auto_arima to find the best (p, d, q)\n",
    "            for splits in custom_ttsplit(product_data):\n",
    "                data_train, data_test = splits[0], splits[1]\n",
    "                model, rmsse = perform_auto_arima(data_train,data_test)\n",
    "                rmsse_list.append(rmsse)\n",
    "\n",
    "            rmsse = np.mean(rmsse_list)\n",
    "            results[model_name] = {\"rmsse\": rmsse, \"model\": model}\n",
    "            if rmsse < best_score:\n",
    "                best_score = rmsse\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "\n",
    "        elif model_name == \"ExponentialSmoothing\":\n",
    "            rmsse_list = []\n",
    "            for splits in custom_ttsplit(product_data):\n",
    "                data_train, data_test = splits[0], splits[1]\n",
    "                model, rmsse = perform_exp_smoothing(data_train, data_test)\n",
    "                rmsse_list.append(rmsse)\n",
    "\n",
    "            rmsse = np.mean(rmsse_list)\n",
    "            results[model_name] = {\"rmsse\": rmsse, \"model\": model}\n",
    "            if rmsse < best_score:\n",
    "                best_score = rmsse\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "\n",
    "        elif model_name == \"Prophet\":\n",
    "            rmsse_list = []\n",
    "            for splits in custom_ttsplit(product_data_with_id):\n",
    "                data_train, data_test = splits[0], splits[1]\n",
    "                model, rmsse = perform_prophet(data_train,data_test)\n",
    "                rmsse_list.append(rmsse)\n",
    "\n",
    "            rmsse = np.mean(rmsse_list)\n",
    "            results[model_name] = {\"rmsse\": rmsse, \"model\": model}\n",
    "            if rmsse < best_score:\n",
    "                best_score = rmsse\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "\n",
    "\n",
    "        elif model_name == \"LightGBM\":\n",
    "            rmsse_list = []\n",
    "\n",
    "            for splits in custom_ttsplit_inclval(product_data):\n",
    "                data_train, data_val, data_test = splits[0], splits[1], splits[2]\n",
    "\n",
    "                model, rmsse = perform_lightgbm(data_train, data_val, data_test)\n",
    "                rmsse_list.append(rmsse)\n",
    "\n",
    "            rmsse = np.mean(rmsse_list)\n",
    "            results[model_name] = {\"rmsse\": rmsse, \"model\": model}\n",
    "            if rmsse < best_score:\n",
    "                best_score = rmsse\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "\n",
    "\n",
    "        elif model_name == \"RandomForest\":\n",
    "            rmsse_list = []\n",
    "\n",
    "            for splits in custom_ttsplit_onlyfeatures(product_data):\n",
    "                data_train, data_test = splits[0], splits[1]\n",
    "\n",
    "                model, rmsse = perform_random_forest(data_train, data_test)\n",
    "                rmsse_list.append(rmsse)\n",
    "\n",
    "            rmsse = np.mean(rmsse_list)\n",
    "            results[model_name] = {\"rmsse\": rmsse, \"model\": model}\n",
    "            if rmsse < best_score:\n",
    "                best_score = rmsse\n",
    "                best_model = model\n",
    "                best_model_name = model_name\n",
    "                \n",
    "                \n",
    "        elif model_name == \"DartsTFT\":\n",
    "            shutil.rmtree(\"logs/tft_logs\")\n",
    "            # Prepare data for TFT model\n",
    "            (y_train, past_cov_train, future_cov_train,\n",
    "             y_val, past_cov_val, future_cov_val) = prepare_data(product_data_with_id)\n",
    "\n",
    "            # Example code assuming daily frequency ('D')\n",
    "            y_train_series = TimeSeries.from_dataframe(y_train, fill_missing_dates=True, freq='D')\n",
    "            past_cov_train_series = TimeSeries.from_dataframe(past_cov_train, fill_missing_dates=True, freq='D')\n",
    "            future_cov_train_series = TimeSeries.from_dataframe(future_cov_train, fill_missing_dates=True, freq='D')\n",
    "\n",
    "            y_val_series = TimeSeries.from_dataframe(y_val, fill_missing_dates=True, freq='D')\n",
    "            past_cov_val_series = TimeSeries.from_dataframe(past_cov_val, fill_missing_dates=True, freq='D')\n",
    "            future_cov_val_series = TimeSeries.from_dataframe(future_cov_val, fill_missing_dates=True, freq='D')\n",
    "\n",
    "            trained_model, validation_loss = train_tft_model(y_train_series, past_cov_train_series, future_cov_train_series,\n",
    "                                            y_val_series, past_cov_val_series, future_cov_val_series)\n",
    "            \n",
    "\n",
    "            # Store the trained TFT model in the results dictionary\n",
    "            results[model_name] = {\"rmsse\": validation_loss, \"model\": trained_model}\n",
    "            if validation_loss < best_score:\n",
    "                best_score = validation_loss\n",
    "                best_model = trained_model\n",
    "                best_model_name = model_name\n",
    "\n",
    "\n",
    "    #Printing results for this product\n",
    "    print(results)\n",
    "    print(f\"Model results for {id}\")\n",
    "    print(f\"Best model: {best_model_name}\")\n",
    "    print(f\"Best score: {best_score}\")\n",
    "\n",
    "    average_rmsse.append(best_score*product_weight)\n",
    "\n",
    "    # Store the average MAE for the current product time-series\n",
    "    product_results[id] = {\"best_score\": best_score, \"best_model\": best_model_name, \"model\": best_model}\n",
    "\n",
    "    #Store the best model in a pkl file\n",
    "    if best_model_name == \"DartsTFT\":\n",
    "        filename = f'../models/{id}_model.pt'\n",
    "        best_model.save(filename)\n",
    "    # elif best_model_name == \"LSTM\":\n",
    "    #     filename = f'../models/{id}_model.h5'\n",
    "    #     best_model.save(filename)\n",
    "    else:\n",
    "        filename = f'../models/{id}_model.pkl'\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(best_model, f)\n",
    "\n",
    "# Create a DataFrame to store the results\n",
    "results_df_arima = pd.DataFrame(product_results.items(), columns=['id', 'RMSSE'])\n",
    "\n",
    "# Set the 'id' column as the index\n",
    "results_df_arima.set_index('id', inplace=True)\n",
    "\n",
    "average_rmsse_score = np.sum(average_rmsse)\n",
    "\n",
    "print(f\"Total average wRMSSE: {average_rmsse_score}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6752189-3c59-43db-bdca-1538095d58d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c2e1d-77c1-45b2-b5e1-97ded9501914",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "517d4c26-7684-462d-bcff-9686788292e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-cpu.2-11.m120",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/tf2-cpu.2-11:m120"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
